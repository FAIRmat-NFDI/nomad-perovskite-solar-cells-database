{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79759a0e",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"\n",
    "    background-color: #f7f7f7;\n",
    "    background-image: url('data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiIHN0YW5kYWxvbmU9Im5vIj8+CjxzdmcKICAgd2lkdGg9IjcyIgogICBoZWlnaHQ9IjczIgogICB2aWV3Qm94PSIwIDAgNzIgNzMiCiAgIGZpbGw9Im5vbmUiCiAgIHZlcnNpb249IjEuMSIKICAgaWQ9InN2ZzEzMTkiCiAgIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIKICAgeG1sbnM6c3ZnPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CiAgPGRlZnMKICAgICBpZD0iZGVmczEzMjMiIC8+CiAgPHBhdGgKICAgICBkPSJNIC0wLjQ5OTk4NSwxNDUgQyAzOS41MzMsMTQ1IDcyLDExMi41MzIgNzIsNzIuNSA3MiwzMi40Njc4IDM5LjUzMywwIC0wLjQ5OTk4NSwwIC00MC41MzI5LDAgLTczLDMyLjQ2NzggLTczLDcyLjUgYyAwLDQwLjAzMiAzMi40NjcxLDcyLjUgNzIuNTAwMDE1LDcyLjUgeiIKICAgICBmaWxsPSIjMDA4YTY3IgogICAgIGZpbGwtb3BhY2l0eT0iMC4yNSIKICAgICBpZD0icGF0aDEzMTciIC8+Cjwvc3ZnPgo='), url('data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiIHN0YW5kYWxvbmU9Im5vIj8+CjxzdmcKICAgd2lkdGg9IjIxNyIKICAgaGVpZ2h0PSIyMjMiCiAgIHZpZXdCb3g9IjAgMCAyMTcgMjIzIgogICBmaWxsPSJub25lIgogICB2ZXJzaW9uPSIxLjEiCiAgIGlkPSJzdmcxMTA3IgogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciCiAgIHhtbG5zOnN2Zz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogIDxkZWZzCiAgICAgaWQ9ImRlZnMxMTExIiAvPgogIDxwYXRoCiAgICAgZD0ibSAyMi4wNDIsNDUuMDEwOSBjIDIxLjM2MjUsMjEuMjc1NyA1NS45NzYsMjEuMjc1NyA3Ny41MTkyLDAgQyAxMTkuNTU4LDI1LjA4IDE1MS41MDIsMjMuNzM1MiAxNzIuODY0LDQxLjM3OCBjIDEuMzQ1LDEuNTI1NCAyLjY5LDMuMjUxNiA0LjIzNiw0Ljc5NzEgMjEuMzYzLDIxLjI3NTYgMjEuMzYzLDU1Ljc5ODkgMCw3Ny4yNTQ5IC0yMS4zNjIsMjEuMjc2IC0yMS4zNjIsNTUuNzk4IDAsNzcuMjU1IDIxLjM2MywyMS40NTYgNTUuOTc2LDIxLjI3NSA3Ny41MiwwIDIxLjU0MywtMjEuMjc2IDIxLjM2MiwtNTUuNzk5IDAsLTc3LjI1NSAtMjEuMzYzLC0yMS4yNzYgLTIxLjM2MywtNTUuNzk4NiAwLC03Ny4yNTQ5IDEyLjY4OSwtMTIuNjQ1IDE3Ljg4OSwtMzAuMTA3MSAxNS4zOTksLTQ2LjU4NTc2IC0xLjU0NiwtMTEuNTAwOTQgLTYuNzI2LC0yMi44MjExNCAtMTUuNTgsLTMxLjYzMjU0IC0yMS4zNjMsLTIxLjI3NTYgLTU1Ljk3NiwtMjEuMjc1NiAtNzcuNTE5LDAgLTIxLjM2MywyMS4yNzU3IC01NS45NzYsMjEuMjc1NyAtNzcuNTE5NCwwIC0yMS4zNjI1LC0yMS4yNzU2IC01NS45NzYxLC0yMS4yNzU2IC03Ny41MTkyLDAgQyAwLjY3OTU2NSwtMTAuNzg3NiAwLjY3OTU5NiwyMy43MzUyIDIyLjA0Miw0NS4wMTA5IFoiCiAgICAgZmlsbD0iIzJhNGNkZiIKICAgICBzdHJva2U9IiMyYTRjZGYiCiAgICAgc3Ryb2tlLXdpZHRoPSIxMiIKICAgICBzdHJva2UtbWl0ZXJsaW1pdD0iMTAiCiAgICAgaWQ9InBhdGgxMTA1IiAvPgogIDxwYXRoCiAgICAgZD0ibSA1MS45OTUyMTIsMjIyLjczMDEzIGMgMjguMzU5MSwwIDUxLjM1ODM5OCwtMjIuOTk5OSA1MS4zNTgzOTgsLTUxLjM1ODQgMCwtMjguMzU4NiAtMjIuOTk5Mjk4LC01MS4zNTg1OSAtNTEuMzU4Mzk4LC01MS4zNTg1OSAtMjguMzU5MSwwIC01MS4zNTg2MDIsMjIuOTk5OTkgLTUxLjM1ODYwMiw1MS4zNTg1OSAwLDI4LjM1ODUgMjIuOTk5NTAyLDUxLjM1ODQgNTEuMzU4NjAyLDUxLjM1ODQgeiIKICAgICBmaWxsPSIjMTkyZTg2IgogICAgIGZpbGwtb3BhY2l0eT0iMC4zNSIKICAgICBpZD0icGF0aDE5MzciIC8+Cjwvc3ZnPgo=') ;\n",
    "    background-position: left bottom, right top;\n",
    "    background-repeat: no-repeat,  no-repeat;\n",
    "    background-size: auto 60px, auto 160px;\n",
    "    border-radius: 5px;\n",
    "    box-shadow: 0px 3px 1px -2px rgba(0, 0, 0, 0.2), 0px 2px 2px 0px rgba(0, 0, 0, 0.14), 0px 1px 5px 0px rgba(0,0,0,.12);\">\n",
    "\n",
    "<h1 style=\"\n",
    "    color: #2a4cdf;\n",
    "    font-style: normal;\n",
    "    font-size: 2.25rem;\n",
    "    line-height: 1.4em;\n",
    "    font-weight: 600;\n",
    "    padding: 30px 200px 0px 30px;\"> \n",
    "        Perovscribe Evals</h1>\n",
    "\n",
    "<p style=\"\n",
    "    line-height: 1.4em;\n",
    "    padding: 30px 200px 0px 30px;\">\n",
    "    This notebook runs through the analysis of the Perovscribe extraction pipeline to compute extraction performance metrics\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928b5d2",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook evaluates the performance of the Perovscribe extraction pipeline by comparing extracted data against a ground truth dataset. The evaluation covers multiple Large Language Models (LLMs) and compares their extraction performance across different data fields.\n",
    "\n",
    "\n",
    "### Evaluation Methodology\n",
    "\n",
    "The evaluation uses a **confusion matrix** approach:\n",
    "- **True Positives (TP)**: Fields correctly extracted and matching ground truth\n",
    "- **False Positives (FP)**: Fields extracted but not present in ground truth\n",
    "- **False Negatives (FN)**: Fields in ground truth but not extracted\n",
    "\n",
    "**Metrics calculated**:\n",
    "- **Precision** = TP / (TP + FP) - Measures extraction accuracy\n",
    "- **Recall** = TP / (TP + FN) - Measures extraction completeness\n",
    "- **F1 Score** = 2 × (Precision × Recall) / (Precision + Recall) - Harmonic mean of precision and recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d3a4b",
   "metadata": {},
   "source": [
    "## Setup and Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f088a593",
   "metadata": {},
   "source": [
    "The evaluation is done by comparing the extracted data to a ground truth dataset.\n",
    "Sometimes, the scoring will use an LLM to score the extracted data.\n",
    "\n",
    "For this reason, we need API keys for the LLMs we are using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b7a1b",
   "metadata": {},
   "source": [
    "### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45abc1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-compiling numba functions for DABEST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling numba functions: 100%|██████████| 11/11 [00:00<00:00, 61.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba compilation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[92m11:51:19 - LiteLLM:ERROR\u001b[0m: redis_cache.py:178 - Error connecting to Sync Redis client\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:51:19 - LiteLLM:ERROR\u001b[0m: redis_cache.py:1081 - LiteLLM Redis Cache PING: - Got exception from REDIS : Error 61 connecting to 127.0.0.1:6379. Connect call failed ('127.0.0.1', 6379).\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import json\n",
    "import os\n",
    "from importlib.resources import files\n",
    "from math import pi\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import dabest\n",
    "import litellm\n",
    "from litellm.caching.caching import Cache\n",
    "litellm.cache = Cache(type=\"disk\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Third-party libraries\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Internal modules\n",
    "# Ensure 'perovscribe' is accessible in the repo structure\n",
    "from perovscribe.pipeline import ExtractionPipeline\n",
    "from plotly_theme import register_template, set_defaults, MODEL_COLORS\n",
    "\n",
    "# --- Configuration & Theme ---\n",
    "load_dotenv()  # Loads .env if present\n",
    "register_template()\n",
    "set_defaults()\n",
    "\n",
    "# Define Paths (Use relative paths for reproducibility!)\n",
    "DATA_DIR = files(\"perovscribe\").joinpath(\"data\")\n",
    "EXTRACTIONS_DIR = DATA_DIR / \"extractions\" \n",
    "GROUND_TRUTH_DIR = DATA_DIR / \"ground_truth\" / \"test\"\n",
    "EXPERTS_DIR = EXTRACTIONS_DIR / \"humans\" / \"Consensus\"\n",
    "# MODEL_COLORS is imported from plotly_theme (single source of truth for colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22fca42",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731419e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model metadata: Display Names, Colors, and Token Costs (per 1M tokens)\n",
    "# Prices are examples; verify current API pricing.\n",
    "# Colors are obtained from MODEL_COLORS (imported from plotly_theme)\n",
    "MODEL_CONFIG = {\n",
    "    \"gpt-5-2025-08-07\": {\n",
    "        \"name\": \"GPT-5\",\n",
    "        \"color\": MODEL_COLORS[\"GPT-5\"],\n",
    "    },\n",
    "    \"gpt-5-mini-2025-08-07\": {\n",
    "        \"name\": \"GPT-5 Mini\",\n",
    "        \"color\": MODEL_COLORS[\"GPT-5 Mini\"],\n",
    "    },\n",
    "    \"claude-opus-4-20250514\": {\n",
    "        \"name\": \"Claude Opus 4\",\n",
    "        \"color\": MODEL_COLORS[\"Claude Opus 4\"],\n",
    "    },\n",
    "    \"claude-sonnet-4-20250514\": {\n",
    "        \"name\": \"Claude Sonnet 4\",\n",
    "        \"color\": MODEL_COLORS[\"Claude Sonnet 4\"],\n",
    "    },\n",
    "    \"claude-opus-4-1-20250805\": {\n",
    "        \"name\": \"Claude Opus 4.1\",\n",
    "        \"color\": MODEL_COLORS[\"Claude Opus 4.1\"],\n",
    "    },\n",
    "    \"gpt-4.1-2025-04-14\": {\n",
    "        \"name\": \"GPT-4.1\",\n",
    "        \"color\": MODEL_COLORS[\"GPT-4.1\"],\n",
    "    },\n",
    "    \"gpt-4o-2024-08-06\": {\n",
    "        \"name\": \"GPT-4o\",\n",
    "        \"color\": MODEL_COLORS[\"GPT-4o\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df8ac6",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6d3f7",
   "metadata": {},
   "source": [
    "##### Evals Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356dd4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING AND MODEL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "all_metrics = {}  # model_name -> paper_doi -> {field: score}\n",
    "all_precs_and_recalls = {}\n",
    "\n",
    "# Evaluate all models\n",
    "for model_dir in tqdm(EXTRACTIONS_DIR.iterdir()):\n",
    "    if not model_dir.is_dir() or model_dir == \"humans\":\n",
    "        continue\n",
    "    \n",
    "    model_name = model_dir.name\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    \n",
    "    pipeline = ExtractionPipeline(\n",
    "        model_name=model_name, \n",
    "        preprocessor=\"pymupdf\", \n",
    "        postprocessor=\"NONE\", \n",
    "        cache_dir=\"\", \n",
    "        use_cache=True\n",
    "    )\n",
    "    model_metrics, avg_recalls, avg_precisions = pipeline._evaluate_multiple(\n",
    "        model_dir, GROUND_TRUTH_DIR\n",
    "    )\n",
    "    \n",
    "    all_precs_and_recalls[model_name] = {\n",
    "        \"precision\": avg_precisions, \n",
    "        \"recall\": avg_recalls\n",
    "    }\n",
    "    all_metrics[model_name] = model_metrics\n",
    "\n",
    "# Rename models to readable names\n",
    "model_name_map = {\n",
    "    \"claude-opus-4-1-20250805\": \"Claude Opus 4.1\",\n",
    "    \"claude-opus-4-20250514\": \"Claude Opus 4\",\n",
    "    \"claude-sonnet-4-20250514\": \"Claude Sonnet 4\",\n",
    "    \"gpt-4.1-2025-04-14\": \"GPT-4.1\",\n",
    "    \"gpt-4o-2024-08-06\": \"GPT-4o\",\n",
    "    \"gpt-5-2025-08-07\": \"GPT-5\",\n",
    "    \"gpt-5-mini-2025-08-07\": \"GPT-5 Mini\"\n",
    "}\n",
    "\n",
    "all_metrics = {\n",
    "    model_name_map.get(k, k): v for k, v in all_metrics.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf5d2d",
   "metadata": {},
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS (DATAFRAME VERSION)\n",
    "# ============================================================================\n",
    "\n",
    "def metrics_to_dataframe(metrics_dict):\n",
    "    \"\"\"\n",
    "    Convert nested metrics dictionary to a flat DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: model, paper, field, TP, FP, FN\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for model, papers in metrics_dict.items():\n",
    "        for paper, fields in papers.items():\n",
    "            for field, values in fields.items():\n",
    "                if isinstance(values, dict):\n",
    "                    rows.append({\n",
    "                        'model': model,\n",
    "                        'paper': paper,\n",
    "                        'field': field,\n",
    "                        'TP': values.get('TP', 0.0),\n",
    "                        'FP': values.get('FP', 0.0),\n",
    "                        'FN': values.get('FN', 0.0)\n",
    "                    })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def add_field_categories(df):\n",
    "    \"\"\"Add aggregation category for each field.\"\"\"\n",
    "    def categorize(field):  # noqa: PLR0911\n",
    "        if field.endswith(\":unit\"):\n",
    "            return \"units\"\n",
    "        field_lower = field.lower()\n",
    "        if \"composition\" in field_lower:\n",
    "            return \"composition\"\n",
    "        if \"stability\" in field_lower:\n",
    "            return \"stability\"\n",
    "        if \"deposition\" in field_lower:\n",
    "            return \"deposition\"\n",
    "        if \"layers\" in field_lower:\n",
    "            return \"layers\"\n",
    "        if \"light\" in field_lower:\n",
    "            return \"light\"\n",
    "        # Clean up individual fields\n",
    "        if any(x in field for x in [\"averaged_quantities\", \"number_devices\", \"encapsulated\"]):\n",
    "            return None\n",
    "        return field.replace(\"_\", \" \").split(\":value\")[0]\n",
    "    \n",
    "    df['category'] = df['field'].apply(categorize)\n",
    "    return df[df['category'].notna()]\n",
    "\n",
    "def calculate_metrics(df, metric_type='recall'):\n",
    "    \"\"\"\n",
    "    Calculate precision or recall for each row.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with TP, FP, FN columns\n",
    "        metric_type: 'recall' or 'precision'\n",
    "    \"\"\"\n",
    "    if metric_type == 'recall':\n",
    "        df['score'] = df.apply(\n",
    "            lambda row: row['TP'] / (row['TP'] + row['FN']) \n",
    "            if (row['TP'] + row['FN']) > 0 else np.nan, \n",
    "            axis=1\n",
    "        )\n",
    "    else:  # precision\n",
    "        df['score'] = df.apply(\n",
    "            lambda row: row['TP'] / (row['TP'] + row['FP']) \n",
    "            if (row['TP'] + row['FP']) > 0 else np.nan, \n",
    "            axis=1\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8982890",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99343f6a",
   "metadata": {},
   "source": [
    "#### Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0089f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BAR CHART: OVERALL MODEL PERFORMANCE (DATAFRAME VERSION)\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate overall metrics per model\n",
    "df = metrics_to_dataframe(all_metrics)\n",
    "\n",
    "df_doi = df.groupby(['model', 'paper']).agg({'TP':'sum', 'FP':'sum', 'FN':'sum'}).reset_index()\n",
    "\n",
    "overall = df_doi.groupby('model').sum().reset_index()\n",
    "overall['precision'] = overall['TP'] / (overall['TP'] + overall['FP'])\n",
    "overall['recall']    = overall['TP'] / (overall['TP'] + overall['FN'])\n",
    "\n",
    "\n",
    "# Plot\n",
    "x = np.arange(len(overall))\n",
    "width = 0.35\n",
    "\n",
    "overall_performance_fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, overall['precision'], width, label='Precision')\n",
    "rects2 = ax.bar(x + width/2, overall['recall'], width, label='Recall')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performances')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(overall['model'], rotation=45)\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.4))\n",
    "ax.set_yticklabels([f\"{y:.1f}\" for y in np.arange(0, 1.1, 0.4)])\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=2, frameon=False)\n",
    "\n",
    "# Add value labels\n",
    "for rects, values in [(rects1, overall['precision']), (rects2, overall['recall'])]:\n",
    "    for rect, val in zip(rects, values):\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., height + 0.02,\n",
    "                f'{val:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20134901",
   "metadata": {},
   "source": [
    "#### Radar Plot: Recalls per field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81cbb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RADAR PLOT: MODEL RECALLS PER FIELD (DATAFRAME VERSION)\n",
    "# ============================================================================\n",
    "\n",
    "# Convert to DataFrame and calculate recalls\n",
    "df = metrics_to_dataframe(all_metrics)\n",
    "df = add_field_categories(df)\n",
    "df = calculate_metrics(df, metric_type='recall')\n",
    "\n",
    "# Aggregate by model and category\n",
    "aggregated = df.groupby(['model', 'category'])['score'].mean().reset_index()\n",
    "\n",
    "# Pivot for radar plot\n",
    "pivot_df = aggregated.pivot(index='model', columns='category', values='score').fillna(0)\n",
    "\n",
    "# Create radar plot\n",
    "fields = sorted(pivot_df.columns)\n",
    "num_fields = len(fields)\n",
    "angles = [n / float(num_fields) * 2 * pi for n in range(num_fields)]\n",
    "angles += angles[:1]\n",
    "\n",
    "radar_recall_fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Plot each model\n",
    "for model_name in pivot_df.index:\n",
    "    scores = pivot_df.loc[model_name, fields].tolist()\n",
    "    values = scores + [scores[0]]\n",
    "\n",
    "    color = MODEL_COLORS.get(model_name, \"#333333\")  # fallback if missing\n",
    "\n",
    "    ax.plot(\n",
    "        angles,\n",
    "        values,\n",
    "        label=model_name,\n",
    "        linewidth=2,\n",
    "        color=color,\n",
    "    )\n",
    "    ax.fill(\n",
    "        angles,\n",
    "        values,\n",
    "        color=color,\n",
    "        alpha=0.03,\n",
    "    )\n",
    "\n",
    "\n",
    "# [Rest of plotting code remains the same...]\n",
    "ax.set_ylim(0.0, 1)\n",
    "angle_degrees = [a * 180 / np.pi for a in angles[:-1]]\n",
    "ax.set_thetagrids(angle_degrees, labels=fields)\n",
    "ax.tick_params(axis='x', pad=25)\n",
    "\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_fontsize(24)\n",
    "    label.set_color(\"dimgray\")\n",
    "    label.set_rotation(45)\n",
    "    label.set_horizontalalignment(\"center\")\n",
    "\n",
    "ax.set_yticks(np.linspace(0.0, 1, 3))\n",
    "ax.set_yticklabels([f\"{y:.1f}\" for y in np.linspace(0.0, 1, 3)], \n",
    "                    fontsize=24, color=\"dimgray\")\n",
    "\n",
    "ax.set_theta_offset(np.deg2rad(17))\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "plt.title(\"Model Recalls per Field\", size=40, color=\"dimgray\")\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), \n",
    "           fontsize=24, ncol=2, frameon=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a745172",
   "metadata": {},
   "source": [
    "#### Radar Plot: Precisions per field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RADAR PLOT: MODEL PRECISIONS PER FIELD (DATAFRAME VERSION)\n",
    "# ============================================================================\n",
    "\n",
    "# Convert to DataFrame and calculate precisions\n",
    "df = metrics_to_dataframe(all_metrics)\n",
    "df = add_field_categories(df)\n",
    "df = calculate_metrics(df, metric_type='precision')  # Changed to precision\n",
    "\n",
    "# Aggregate by model and category\n",
    "aggregated = df.groupby(['model', 'category'])['score'].mean().reset_index()\n",
    "\n",
    "# Pivot for radar plot\n",
    "pivot_df = aggregated.pivot(index='model', columns='category', values='score').fillna(0)\n",
    "\n",
    "# Create radar plot\n",
    "fields = sorted(pivot_df.columns)\n",
    "num_fields = len(fields)\n",
    "angles = [n / float(num_fields) * 2 * pi for n in range(num_fields)]\n",
    "angles += angles[:1]\n",
    "\n",
    "radar_precision_fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Plot each model\n",
    "for model_name in pivot_df.index:\n",
    "    scores = pivot_df.loc[model_name, fields].tolist()\n",
    "    values = scores + [scores[0]]\n",
    "\n",
    "    color = MODEL_COLORS.get(model_name, \"#333333\")  # fallback if missing\n",
    "\n",
    "    ax.plot(\n",
    "        angles,\n",
    "        values,\n",
    "        label=model_name,\n",
    "        linewidth=2,\n",
    "        color=color,\n",
    "    )\n",
    "    ax.fill(\n",
    "        angles,\n",
    "        values,\n",
    "        color=color,\n",
    "        alpha=0.03,\n",
    "    )\n",
    "\n",
    "\n",
    "# Customize plot\n",
    "ax.set_ylim(0.3, 1)  # Different y-limit for precisions\n",
    "angle_degrees = [a * 180 / np.pi for a in angles[:-1]]\n",
    "ax.set_thetagrids(angle_degrees, labels=fields)\n",
    "ax.tick_params(axis='x', pad=25)\n",
    "\n",
    "# Style field labels\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_fontsize(24)\n",
    "    label.set_color(\"dimgray\")\n",
    "    label.set_rotation(45)\n",
    "    label.set_horizontalalignment(\"center\")\n",
    "\n",
    "# Style radial ticks\n",
    "ax.set_yticks(np.linspace(0.3, 1, 3))\n",
    "ax.set_yticklabels([f\"{y:.1f}\" for y in np.linspace(0.3, 1, 3)], \n",
    "                    fontsize=24, color=\"dimgray\")\n",
    "\n",
    "# Rotate plot\n",
    "ax.set_theta_offset(np.deg2rad(17))\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "# Title and legend\n",
    "plt.title(\"Model Precisions per Field\", size=40, color=\"dimgray\")\n",
    "plt.legend(\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, -0.1),\n",
    "    fontsize=24,\n",
    "    ncol=2,\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"plots/model_precisions_spider.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7dd2da",
   "metadata": {},
   "source": [
    "### Comparison with Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b54c6",
   "metadata": {},
   "source": [
    "#### Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d27c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ExtractionPipeline(model_name=\"Consensus\", preprocessor=\"pymupdf\", postprocessor=\"NONE\", cache_dir=\"\", use_cache=True)\n",
    "authors_metrics, authors_recalls, authors_precisions = pipeline._evaluate_multiple(EXPERTS_DIR, GROUND_TRUTH_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65758080",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics[\"Consensus\"] = authors_metrics\n",
    "experts_included_df = metrics_to_dataframe(all_metrics)\n",
    "\n",
    "# 1. Get the set of papers that appear with model == \"Consensus\"\n",
    "expert_papers = set(experts_included_df.loc[\n",
    "    experts_included_df[\"model\"] == \"Consensus\", \n",
    "    \"paper\"\n",
    "])\n",
    "\n",
    "# 2. Filter the DataFrame\n",
    "filtered_df = experts_included_df[\n",
    "    (experts_included_df[\"model\"] == \"Consensus\") |\n",
    "    ((experts_included_df[\"model\"] != \"Consensus\") &\n",
    "     (experts_included_df[\"paper\"].isin(expert_papers)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by paper and model, sum TP and FP\n",
    "micro_precision_df = (\n",
    "    filtered_df\n",
    "    .groupby(['paper', 'model'])\n",
    "    [['TP', 'FP']]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Compute micro-precision\n",
    "micro_precision_df['precision'] = micro_precision_df['TP'] / (micro_precision_df['TP'] + micro_precision_df['FP'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select the papers where both LLMs and experts exist\n",
    "papers_with_both = micro_precision_df['paper'].value_counts()\n",
    "papers_with_both = papers_with_both[papers_with_both > 1].index\n",
    "df_plot = micro_precision_df[micro_precision_df['paper'].isin(papers_with_both)]\n",
    "\n",
    "# Pivot data so each row is a DOI and each column is a model\n",
    "df_pivot = df_plot.pivot(index='paper', columns='model', values='precision').reset_index()\n",
    "\n",
    "# Melt data for dabest\n",
    "df_melt = df_pivot.melt(id_vars='paper', var_name='model', value_name='precision')\n",
    "\n",
    "# Create a dabest object using authors as the control\n",
    "dabest_data = dabest.load(\n",
    "    data=df_melt,\n",
    "    x='model',\n",
    "    y='precision',\n",
    "    idx=(\"Consensus\", \"GPT-4.1\", \"Claude Opus 4\", \"GPT-4o\", \"GPT-5\",\n",
    "         \"Claude Sonnet 4\", \"Claude Opus 4.1\", \"GPT-5 Mini\")\n",
    ")\n",
    "\n",
    "# Plot mean difference against authors\n",
    "plt.figure()\n",
    "mean_fig = dabest_data.mean_diff.plot(\n",
    "    raw_marker_size=4,\n",
    "    custom_palette=MODEL_COLORS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb11127",
   "metadata": {},
   "source": [
    "#### Overlapping extractions spider plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5198f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING AND MODEL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "human_metrics = {}  # model_name -> paper_doi -> {field: score}\n",
    "human_precs_and_recalls = {}\n",
    "\n",
    "HUMANS_DIR = EXTRACTIONS_DIR / \"humans\"\n",
    "DEV_DIR = DATA_DIR / \"ground_truth\" / \"dev\"\n",
    "# Evaluate all models\n",
    "for model_dir in HUMANS_DIR.iterdir():\n",
    "    if not model_dir.is_dir():\n",
    "        continue\n",
    "    \n",
    "    model_name = model_dir.name\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    \n",
    "    pipeline = ExtractionPipeline(\n",
    "        model_name=model_name, \n",
    "        preprocessor=\"pymupdf\", \n",
    "        postprocessor=\"NONE\", \n",
    "        cache_dir=\"\", \n",
    "        use_cache=True\n",
    "    )\n",
    "    model_metrics, avg_recalls, avg_precisions = pipeline._evaluate_multiple(\n",
    "        model_dir, DEV_DIR\n",
    "    )\n",
    "    \n",
    "    human_precs_and_recalls[model_name] = {\n",
    "        \"precision\": avg_precisions, \n",
    "        \"recall\": avg_recalls\n",
    "    }\n",
    "    human_metrics[model_name] = model_metrics\n",
    "\n",
    "sonnet_4_metrics, s_rec, s_prec = pipeline._evaluate_multiple(\n",
    "    EXTRACTIONS_DIR / \"claude-sonnet-4-20250514/\",\n",
    "    DEV_DIR\n",
    ")\n",
    "human_metrics[\"Claude Sonnet 4\"] = sonnet_4_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4025ee1",
   "metadata": {},
   "source": [
    "##### What dois match across our extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "doi_to_groups = defaultdict(list)\n",
    "\n",
    "for group, dois in human_metrics.items():\n",
    "    for doi in dois:\n",
    "        doi_to_groups[doi].append(group)\n",
    "\n",
    "print(\"DOI matches across groups:\\n\")\n",
    "for doi, groups in doi_to_groups.items():\n",
    "    if len(groups) > 1:\n",
    "        print(f\"{doi} -> {', '.join(groups)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeebdc3",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RADAR PLOT: MODEL PRECISIONS PER FIELD (DATAFRAME VERSION)\n",
    "# ============================================================================\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Convert to DataFrame and calculate precisions\n",
    "df = metrics_to_dataframe(human_metrics)\n",
    "df = add_field_categories(df)\n",
    "df = calculate_metrics(df, metric_type='precision')  # Changed to precision\n",
    "\n",
    "# Aggregate by model and category\n",
    "aggregated = df.groupby(['model', 'category'])['score'].mean().reset_index()\n",
    "\n",
    "# Pivot for radar plot\n",
    "pivot_df = aggregated.pivot(index='model', columns='category', values='score').fillna(0)\n",
    "\n",
    "# Create radar plot\n",
    "fields = sorted(pivot_df.columns)\n",
    "num_fields = len(fields)\n",
    "angles = [n / float(num_fields) * 2 * pi for n in range(num_fields)]\n",
    "angles += angles[:1]\n",
    "\n",
    "human_radar_precision_fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Plot each model\n",
    "for model_name in pivot_df.index:\n",
    "    scores = pivot_df.loc[model_name, fields].tolist()\n",
    "    values = scores + [scores[0]]\n",
    "    \n",
    "    # 2. Set distinct color for the line\n",
    "    line_color = MODEL_COLORS.get(model_name, \"#333333\")\n",
    "    \n",
    "    # 3. Control visibility and emphasis\n",
    "    if model_name in ('Consensus', 'Claude Sonnet 4'):\n",
    "        # Highlight: thicker line, higher alpha fill\n",
    "        line_alpha = 1.0\n",
    "        fill_alpha = 0.2\n",
    "        line_width = 4\n",
    "    else:\n",
    "        # Dimmer: lower opacity for line and fill, normal line width\n",
    "        line_alpha = 0.3\n",
    "        fill_alpha = 0.02\n",
    "        line_width = 2\n",
    "\n",
    "    line, = ax.plot(\n",
    "        angles, \n",
    "        values, \n",
    "        label=model_name, \n",
    "        linewidth=line_width,\n",
    "        color=line_color,\n",
    "        alpha=line_alpha\n",
    "    )\n",
    "    \n",
    "    ax.fill(\n",
    "        angles, \n",
    "        values, \n",
    "        color=line_color,\n",
    "        alpha=fill_alpha\n",
    "    )\n",
    "\n",
    "# Customize plot (Rest of the customization remains the same)\n",
    "x.set_ylim(0.3, 1)\n",
    "angle_degrees = [a * 180 / np.pi for a in angles[:-1]]\n",
    "ax.set_thetagrids(angle_degrees, labels=fields)\n",
    "\n",
    "ax.set_yticks(np.linspace(0.3, 1, 3))\n",
    "ax.set_yticklabels([f\"{y:.1f}\" for y in np.linspace(0.3, 1, 3)])\n",
    "\n",
    "# Rotate plot\n",
    "ax.set_theta_offset(np.deg2rad(17))\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "# Title and legend\n",
    "plt.title(\"Model Precisions per Field\")\n",
    "humans_legend_handle = Line2D(\n",
    "    [0], [0],\n",
    "    color=\"gray\",\n",
    "    linewidth=4,\n",
    "    alpha=1.0,\n",
    "    label=\"Humans\"\n",
    ")\n",
    "# Get existing handles (Consensus & Sonnet 4 only)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "allowed = {\"Consensus\", \"Claude Sonnet 4\"}\n",
    "filtered = [\n",
    "    (allowed_handle, allowed_label)\n",
    "    for allowed_handle, allowed_label in zip(handles, labels)\n",
    "    if allowed_label in allowed\n",
    "]\n",
    "if filtered:\n",
    "    handles, labels = zip(*filtered)\n",
    "    handles = list(handles)\n",
    "    labels = list(labels)\n",
    "else:\n",
    "    handles, labels = [], []\n",
    "\n",
    "# Add Humans as legend-only entry\n",
    "handles.append(humans_legend_handle)\n",
    "labels.append(\"Humans\")\n",
    "\n",
    "plt.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, -0.1),\n",
    "    ncol=3,\n",
    "    frameon=False\n",
    ")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perov-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
