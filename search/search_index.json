{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#welcome-to-the-nomad-perovskite-solar-cells-database-documentation","title":"Welcome to the <code>nomad-perovskite-solar-cells-database</code> documentation","text":""},{"location":"index.html#nomad-perovskite-solar-cells-database","title":"NOMAD Perovskite Solar Cells Database","text":""},{"location":"index.html#introduction","title":"Introduction","text":"<p>Welcome to the NOMAD plugin for the Perovskite Solar Cell Database. This project aims to provide a FAIR and open-access interface for the perovskite solar cells database in NOMAD and related data sources. Additionally, it has data models and schemas for defining ions and halide hybrid-perovskite compositions. The data can be accessed via the NOMAD API and explored in the NOMAD Solar Cell Search App.</p> <p></p> <p>Information about the original database is available at perovskitedatabase.com.</p>"},{"location":"index.html#search-applications","title":"Search applications","text":"<p>Search applications provide an efficient way to explore the databases. They are powered by metadata definitions that define the structure of the data and allow for filtering and visualization.</p> \ud83d\udd0d Search Application \ud83d\udcdd Description \ud83d\udcbb Code \ud83d\udcc4 Metadata Definitions The Perovskite Database Project Explore the data from the perovskite database. App's code Metadata Halide Perovskite Ions Database Search ions used in halide perovskites compounds App's code Metadata Solar Cells An application to explore solar cells App's code Metadata"},{"location":"index.html#tutorial","title":"Tutorial","text":"<ul> <li>Sharing a perovskite composition</li> </ul>"},{"location":"index.html#how-to-guides","title":"How-to guides","text":"<p>How-to guides provide step-by-step instructions for a wide range of tasks:</p> <ul> <li>Install this plugin</li> <li>Explore the databases</li> <li>Create a perovskite composition</li> <li>Add a new perovskite ion to the database</li> <li>Export structure files</li> <li>Download data</li> </ul>"},{"location":"index.html#example-notebooks","title":"Example Notebooks","text":"<p>Explore our collection of interactive Jupyter notebooks that demonstrate how to query, analyze, and work with perovskite solar cell data:</p> <p>Getting Started: - Query the Perovskite Database - Retrieve data from NOMAD - Query the Ion Database - Access perovskite ions - Build Perovskite Structures - Construct crystal structures</p> <p>Data Analysis: - Architecture Evolution - Device architecture trends over time - Bandgap Evolution - Bandgap distribution analysis - Performance Evolution - Track efficiency improvements</p> <p>Machine Learning: - CrabNet Bandgap Prediction - ML-based property prediction - ML Distribution Shift Case Study - Understanding data distribution challenges</p> <p>View all notebooks \u2192</p>"},{"location":"index.html#related-resources","title":"Related Resources","text":"<ul> <li>Original Paper on Nature Energy</li> <li>NOMAD Documentation</li> <li>Information about the original database is available at perovskitedatabase.com.</li> </ul>"},{"location":"index.html#acknowledgments","title":"Acknowledgments","text":"<p>Special thanks to Jinzhao Li and all contributors who have made this project possible. This project is supported by the FAIRmat NFDI initiative and also by by the European Union as part of the SolMates project (Project Nr. 101122288).</p> <p> </p>"},{"location":"main.html","title":"Main","text":"In\u00a0[\u00a0]: Copied! <pre>from nomad_docs import define_env\n</pre> from nomad_docs import define_env"},{"location":"how_to/add_a_new_ion.html","title":"Add a new ion","text":""},{"location":"how_to/add_a_new_ion.html#how-to-add-a-new-ion-to-the-database","title":"How to add a new ion to the database","text":"<p>The Halide Perovskite Ions Database, compiles a set of more than 300 entries of ions used in halide perovskites compounds. This one can be used to explore potential ions for perovskites, visualize those ions, or download its structured data. This dataset can be extended for new ions by uploading new data to the NOMAD repository following the steps below.</p>"},{"location":"how_to/create_a_perovskite_composition.html","title":"Create a perovskite compostion","text":""},{"location":"how_to/create_a_perovskite_composition.html#how-to-create-a-perovskite-composition","title":"How to create a perovskite composition","text":"<p>In NOMAD it is possible to utilize the perovskite ions database for creating perovskite composition entries.</p> <p>The documentation below contains instructions for the following tasks:</p> <ul> <li>Create a new perovskite composition in NOMAD</li> <li>Create a new perovskite composition from an existing JSON file</li> <li>Download a JSON file with a perovskite composition from your own upload</li> <li>Upload and publish a Perovskite composition to the NOMAD Database</li> </ul>"},{"location":"how_to/create_a_perovskite_composition.html#create-a-new-perovskite-composition-in-nomad","title":"Create a new perovskite composition in NOMAD","text":""},{"location":"how_to/create_a_perovskite_composition.html#upload-a-perovskite-composition-created-elsewhere-to-the-nomad-database","title":"Upload a Perovskite composition created elsewhere to the NOMAD Database","text":""},{"location":"how_to/create_a_perovskite_composition.html#download-a-json-file-with-a-perovskite-composition-from-your-own-upload","title":"Download a JSON file with a perovskite composition from your own upload","text":""},{"location":"how_to/create_a_perovskite_composition.html#publish-a-perovskite-composition-in-nomad","title":"Publish a Perovskite composition in NOMAD","text":""},{"location":"how_to/download_data.html","title":"Download data","text":""},{"location":"how_to/download_data.html#downloading-data","title":"Downloading data","text":""},{"location":"how_to/download_data.html#download-a-json-file-from-gui","title":"Download a JSON file from GUI","text":"<p>To download a JSON file with the data of the ion you have uploaded, follow the steps below:</p>"},{"location":"how_to/explore_the_databases.html","title":"Explore the databases","text":""},{"location":"how_to/explore_the_databases.html#explore-the-databases","title":"Explore the Databases","text":"<p>The most efficient way to explore the databases is to use the search applications.</p>"},{"location":"how_to/explore_the_databases.html#search-perovskite-solar-cells","title":"Search Perovskite Solar Cells","text":"<p>Using the Perovskite Database Project app, you can combine different filters to find exactly what you need. For example, you can search for solar cells that contain Sn in the perovskite composition and where C60 has been used as an electron transport layer.</p> <p></p> <p>Once the results appear, you can navigate to individual entries by clicking on a row in the results table or directly on points of interest in the scatter plot.</p>"},{"location":"how_to/explore_the_databases.html#search-ions","title":"Search Ions","text":"<p>Similarly, you can explore the Ion Database app to find ions used in halide perovskites. Filters are available for elements, and you can use two-dimensional filters in the scatter plot to locate bulky ions with a large number of atoms and high molecular weight.</p> <p></p>"},{"location":"how_to/explore_the_databases.html#copy-the-search-query-for-the-api","title":"Copy the Search Query for the API","text":"<p>If you need to work with larger datasets programmatically, the GUI provides tools to build API calls. Use the \"Copy API call\" <code>&lt; &gt;</code> button to copy the API call to your clipboard, and then paste it into your code.</p> <p></p> <p>For more information on using the API, check the NOMAD API documentation.</p>"},{"location":"how_to/export_structures.html","title":"Export a structure","text":""},{"location":"how_to/export_structures.html#exporting-structure-files","title":"Exporting structure files","text":"<p>For the Halide Perovskite Ions Database we have calculated the conformers of the molecules using the RDKit Software. The structure files of these conformers can be downloaded directly from the overview page of the ion entry or Programmatically using the NOMAD API.</p>"},{"location":"how_to/export_structures.html#downloading-structure-files-from-the-overview-page","title":"Downloading structure files from the overview page","text":""},{"location":"how_to/export_structures.html#programmatically-using-the-nomad-api","title":"Programmatically using the NOMAD API","text":"<p>Example for retrieving the structure of Pyrene-O-ethylammonium:</p> <pre><code>import requests\n\n# Define the base URL\nbase_url = 'https://nomad-lab.eu/prod/v1/develop/api/v1/systems'\n\n# Specify the entry ID\nentry_id = 'ccun2zHQ49i6bA-RoTdK3U6mqBZM'\n\n# Define the query parameters\nparams = {\n    'path': 'results/material/topology/0',\n    'format': 'xyz',\n    'wrap_mode': 'original'\n}\n\n# Construct the full URL by appending the entry ID\nfull_url = f'{base_url}/{entry_id}'\n\n# Make the GET request with the specified parameters\nresponse = requests.get(full_url, params=params)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Process the response content\n    data = response.content\n    # For example, save the content to a file\n    with open('output.xyz', 'wb') as file:\n        file.write(data)\nelse:\n    print(f'Error: {response.status_code}')\n</code></pre> <p>With this, you could write a script to download the structure files of the conformers of all the ions in the database.</p>"},{"location":"how_to/install_this_plugin.html","title":"Install this plugin","text":""},{"location":"how_to/install_this_plugin.html#install-this-plugin","title":"Install This Plugin","text":"<p>If you want to run this plugin locally on your Oasis to use the defined schemas, you  need to add the plugin to your Oasis image. The recommended way of doing this is to add it to the plugins table in the  <code>pyproject.toml</code> file of your  NOMAD distribution repository.</p> <p>Currently the plugin is not published to PyPI and you will need to specify a git   source. For this you also need to specify a version tag, branch, or commit.   For example, to use the v0.1.1 release you should add the following the to the   <code>pyproject.toml</code>:</p> <pre><code>[project.optional-dependencies]\nplugins = [\n  \"perovskite-solar-cell-database @ git+https://github.com/FAIRmat-NFDI/nomad-perovskite-solar-cells-database.git@v0.1.1\"\n]\n</code></pre> <p>For more detailed installation instructions, visit our docs for NOMAD plugins.</p>"},{"location":"notebooks/ions_database/build-perovskite-structure-from-ion-entry.html","title":"Build Perovskite Structures from Ions","text":"Building Perovskite Structures from their Ions Components with pyrovskite. <p>         This notebook demonstrates how to use the python package pyrovskite to build hypothetical structures using the ions components.          We will create a hypothetical Dion-Jacobson perovskite structure using the cations          extracted from the Halide Perovskite Ions database.  </p><p> </p> <p></p> <p>Let's start by installing the package and importing the necessary libraries.</p> In\u00a0[1]: Copied! <pre>! pip install pyrovskite\n</pre> ! pip install pyrovskite <pre>Requirement already satisfied: pyrovskite in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (1.0.0)\nRequirement already satisfied: pymatgen in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pyrovskite) (2024.5.1)\nRequirement already satisfied: ase in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pyrovskite) (3.22.1)\nRequirement already satisfied: matplotlib&gt;=3.1.0 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from ase-&gt;pyrovskite) (3.10.0)\nRequirement already satisfied: numpy&gt;=1.15.0 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from ase-&gt;pyrovskite) (1.26.4)\nRequirement already satisfied: scipy&gt;=1.1.0 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from ase-&gt;pyrovskite) (1.14.1)\nRequirement already satisfied: monty&gt;=2024.2.2 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (2024.12.10)\nRequirement already satisfied: networkx&gt;=2.2 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (3.4.2)\nRequirement already satisfied: palettable&gt;=3.1.1 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (3.3.3)\nRequirement already satisfied: pandas in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (2.2.3)\nRequirement already satisfied: plotly&gt;=4.5.0 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (5.24.1)\nRequirement already satisfied: pybtex in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (0.24.0)\nRequirement already satisfied: requests in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (2.31.0)\nRequirement already satisfied: ruamel.yaml&gt;=0.17.0 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (0.18.6)\nRequirement already satisfied: spglib&gt;=2.0.2 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (2.5.0)\nRequirement already satisfied: sympy in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (1.13.3)\nRequirement already satisfied: tabulate in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (0.8.9)\nRequirement already satisfied: tqdm in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (4.67.1)\nRequirement already satisfied: uncertainties&gt;=3.1.4 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (3.2.2)\nRequirement already satisfied: joblib in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pymatgen-&gt;pyrovskite) (1.4.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from matplotlib&gt;=3.1.0-&gt;ase-&gt;pyrovskite) (1.3.1)\nRequirement already satisfied: cycler&gt;=0.10 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from matplotlib&gt;=3.1.0-&gt;ase-&gt;pyrovskite) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from matplotlib&gt;=3.1.0-&gt;ase-&gt;pyrovskite) (4.55.3)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from matplotlib&gt;=3.1.0-&gt;ase-&gt;pyrovskite) (1.4.7)\nRequirement already satisfied: packaging&gt;=20.0 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from matplotlib&gt;=3.1.0-&gt;ase-&gt;pyrovskite) (24.2)\nRequirement already satisfied: pillow&gt;=8 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from matplotlib&gt;=3.1.0-&gt;ase-&gt;pyrovskite) (10.0.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from matplotlib&gt;=3.1.0-&gt;ase-&gt;pyrovskite) (3.2.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from matplotlib&gt;=3.1.0-&gt;ase-&gt;pyrovskite) (2.9.0.post0)\nRequirement already satisfied: tenacity&gt;=6.2.0 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from plotly&gt;=4.5.0-&gt;pymatgen-&gt;pyrovskite) (9.0.0)\nRequirement already satisfied: ruamel.yaml.clib&gt;=0.2.7 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from ruamel.yaml&gt;=0.17.0-&gt;pymatgen-&gt;pyrovskite) (0.2.12)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pandas-&gt;pymatgen-&gt;pyrovskite) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pandas-&gt;pymatgen-&gt;pyrovskite) (2024.2)\nRequirement already satisfied: PyYAML&gt;=3.01 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pybtex-&gt;pymatgen-&gt;pyrovskite) (6.0.2)\nRequirement already satisfied: latexcodec&gt;=1.0.4 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pybtex-&gt;pymatgen-&gt;pyrovskite) (3.0.0)\nRequirement already satisfied: six in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from pybtex-&gt;pymatgen-&gt;pyrovskite) (1.17.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from requests-&gt;pymatgen-&gt;pyrovskite) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from requests-&gt;pymatgen-&gt;pyrovskite) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from requests-&gt;pymatgen-&gt;pyrovskite) (1.26.20)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from requests-&gt;pymatgen-&gt;pyrovskite) (2024.12.14)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/pepe_marquez/NOMAD/nomad/.pyenv/lib/python3.11/site-packages (from sympy-&gt;pymatgen-&gt;pyrovskite) (1.3.0)\n\n[notice] A new release of pip is available: 24.0 -&gt; 24.3.1\n[notice] To update, run: pip install --upgrade pip\n</pre> In\u00a0[2]: Copied! <pre>import os\n\nimport ase.io\n</pre> import os  import ase.io <p>In this notebook we will be using already downloaded data from the Halide Perovskite Ions database. We will use a Methilammonium (MA) cation, 2-(2-azaniumylethyldisulfanyl)ethylazanium (AESE) as a separator, a Bromide (Br) anion, and a Tin (Sn) cation to build a Dion-Jacobson perovskite structure.</p> <p>You can also retrieve further structures using the NOMAD API.</p> In\u00a0[9]: Copied! <pre>if not os.path.isdir('builder_outputs'):\n    os.mkdir('builder_outputs')\n\nfrom pyrovskite.builder import *\n\nA = ase.io.read('CNH6.xyz')\nAp = ase.io.read('C2NH7S.xyz')\n</pre> if not os.path.isdir('builder_outputs'):     os.mkdir('builder_outputs')  from pyrovskite.builder import *  A = ase.io.read('CNH6.xyz') Ap = ase.io.read('C2NH7S.xyz') In\u00a0[\u00a0]: Copied! <pre>from ase.visualize import view\nfrom pyrovskite.builder import make_dj\n\ndion_jacobson = make_dj(\n    Ap=Ap,\n    A=A,\n    B='Sn',\n    X='Br',\n    n=2,\n    BX_dist=2.8,\n    Ap_Rx=0,\n    Ap_Ry=90,\n    Ap_Rz=0,\n    penet=-0.2,\n    Bp='Sn',\n)\n</pre> from ase.visualize import view from pyrovskite.builder import make_dj  dion_jacobson = make_dj(     Ap=Ap,     A=A,     B='Sn',     X='Br',     n=2,     BX_dist=2.8,     Ap_Rx=0,     Ap_Ry=90,     Ap_Rz=0,     penet=-0.2,     Bp='Sn', ) <p>Let's increase the system size and visualize the structure.</p> In\u00a0[8]: Copied! <pre>large_dion_jacobson = dion_jacobson.repeat((4, 2, 2))\nview(large_dion_jacobson, viewer='x3d')\n</pre> large_dion_jacobson = dion_jacobson.repeat((4, 2, 2)) view(large_dion_jacobson, viewer='x3d') Out[8]: ASE atomic visualization <p>Great! Save your structure and feel free to bring it to the next step to relax the structure with your favorite DFT code or your or machine learning model. Explore the pyrovskite package and build your own structures using the ions components from the Halide Perovskite Ions database.</p> In\u00a0[\u00a0]: Copied! <pre>dion_jacobson.write('builder_outputs/dion_jacobson.cif')\n</pre> dion_jacobson.write('builder_outputs/dion_jacobson.cif')"},{"location":"notebooks/ions_database/query-ion-database-api.html","title":"Query the Ion Database","text":"Query the perovskite ion database in NOMAD <p>         This notebook demonstrates how query NOMAD for all, as well as for specific perovskite ions. The notebook also shows how to validate the ions against the JSON Schema </p><p> </p> <p></p> In\u00a0[1]: Copied! <pre>import json\n\nimport requests\n\nbase_url = 'http://nomad-lab.eu/prod/v1/api/v1'\n</pre> import json  import requests  base_url = 'http://nomad-lab.eu/prod/v1/api/v1' In\u00a0[2]: Copied! <pre>response = requests.post(\n    f'{base_url}/entries/archive/query',\n    json={\n        'owner': 'visible',\n        'query': {\n            'data.abbreviation#perovskite_solar_cell_database.composition.PerovskiteAIon:any': [\n                'MA'\n            ]\n        },\n        'pagination': {'page_size': 1},\n    },\n)\nresponse_json = response.json()\nion_data = response_json['data'][0]['archive']['data']\nprint(json.dumps(ion_data, indent=2))\n</pre> response = requests.post(     f'{base_url}/entries/archive/query',     json={         'owner': 'visible',         'query': {             'data.abbreviation#perovskite_solar_cell_database.composition.PerovskiteAIon:any': [                 'MA'             ]         },         'pagination': {'page_size': 1},     }, ) response_json = response.json() ion_data = response_json['data'][0]['archive']['data'] print(json.dumps(ion_data, indent=2)) <pre>{\n  \"m_def\": \"perovskite_solar_cell_database.composition.PerovskiteAIon\",\n  \"name\": \"MA perovskite ion\",\n  \"datetime\": \"2024-12-18T14:46:39.211148+00:00\",\n  \"lab_id\": \"perovskite_ion_MA\",\n  \"common_name\": \"Methylammonium\",\n  \"molecular_formula\": \"CH6N+\",\n  \"smiles\": \"C[NH3+]\",\n  \"iupac_name\": \"methylazanium\",\n  \"cas_number\": \"17000-00-9\",\n  \"abbreviation\": \"MA\",\n  \"source_compound_molecular_formula\": \"CH5N\",\n  \"source_compound_smiles\": \"CN\",\n  \"source_compound_iupac_name\": \"methanamine\",\n  \"source_compound_cas_number\": \"74-89-5\",\n  \"elemental_composition\": [\n    {\n      \"element\": \"C\",\n      \"atomic_fraction\": 0.125,\n      \"mass_fraction\": 0.3745730552651735\n    },\n    {\n      \"element\": \"H\",\n      \"atomic_fraction\": 0.75,\n      \"mass_fraction\": 0.1886054095051807\n    },\n    {\n      \"element\": \"N\",\n      \"atomic_fraction\": 0.125,\n      \"mass_fraction\": 0.4368215352296457\n    }\n  ],\n  \"pure_substance\": {\n    \"name\": \"Methylammonium\",\n    \"iupac_name\": \"methylazanium\",\n    \"molecular_formula\": \"CH6N\",\n    \"molecular_mass\": 32.050024196,\n    \"molar_mass\": 32.065,\n    \"monoisotopic_mass\": 32.050024196,\n    \"inchi\": \"InChI=1S/CH5N/c1-2/h2H2,1H3/p+1\",\n    \"inchi_key\": \"BAVYZALUXZFZLV-UHFFFAOYSA-O\",\n    \"smile\": \"C[NH3+]\",\n    \"canonical_smile\": \"C[NH3+]\",\n    \"cas_number\": \"17000-00-9\",\n    \"pub_chem_cid\": 644041,\n    \"pub_chem_link\": \"https://pubchem.ncbi.nlm.nih.gov/compound/644041\"\n  },\n  \"source_compound\": {\n    \"name\": \"Methylamine\",\n    \"iupac_name\": \"methanamine\",\n    \"molecular_formula\": \"CH5N\",\n    \"molecular_mass\": 31.042199164,\n    \"molar_mass\": 31.057,\n    \"monoisotopic_mass\": 31.042199164,\n    \"inchi\": \"InChI=1S/CH5N/c1-2/h2H2,1H3\",\n    \"inchi_key\": \"BAVYZALUXZFZLV-UHFFFAOYSA-N\",\n    \"smile\": \"CN\",\n    \"canonical_smile\": \"CN\",\n    \"cas_number\": \"74-89-5\",\n    \"pub_chem_cid\": 6329,\n    \"pub_chem_link\": \"https://pubchem.ncbi.nlm.nih.gov/compound/6329\"\n  }\n}\n</pre> In\u00a0[3]: Copied! <pre>from jsonschema import ValidationError, validate\n\nschema_id = (\n    'https://raw.githubusercontent.com/Jesperkemist/'\n    'Perovskite_composition/v1.0.0/ion_schema.json'\n)\n\n# Load the JSON schema from the URL\nschema_response = requests.get(schema_id)\nschema = schema_response.json()\n\n# Validate ion_data against the schema\ntry:\n    validate(instance=ion_data, schema=schema)\n    print(f'ion_data is valid against the {schema_id} schema.')\nexcept ValidationError as e:\n    print('Validation error:', e.message)\n</pre> from jsonschema import ValidationError, validate  schema_id = (     'https://raw.githubusercontent.com/Jesperkemist/'     'Perovskite_composition/v1.0.0/ion_schema.json' )  # Load the JSON schema from the URL schema_response = requests.get(schema_id) schema = schema_response.json()  # Validate ion_data against the schema try:     validate(instance=ion_data, schema=schema)     print(f'ion_data is valid against the {schema_id} schema.') except ValidationError as e:     print('Validation error:', e.message) <pre>ion_data is valid against the https://raw.githubusercontent.com/Jesperkemist/Perovskite_composition/v1.0.0/ion_schema.json schema.\n</pre> In\u00a0[4]: Copied! <pre>json_body = {\n    'owner': 'visible',\n    'query': {\n        'results.eln.sections:any': [\n            'PerovskiteAIon',\n            'PerovskiteBIon',\n            'PerovskiteXIon',\n        ],\n    },\n    'pagination': {\n        'page_size': 10,\n    },\n}\n\nall_ion_data = {}\n\nwhile len(all_ion_data) &lt; 500:\n    response = requests.post(f'{base_url}/entries/archive/query', json=json_body)\n    response_json = response.json()\n\n    for entry in response_json['data']:\n        abbreviation = entry['archive']['data']['abbreviation']\n        if abbreviation in all_ion_data:\n            print(f'Duplicate entry found for abbreviation: {abbreviation}')\n        all_ion_data[abbreviation] = entry['archive']['data']\n\n    next_value = response_json['pagination'].get('next_page_after_value')\n    if not next_value:\n        break\n    json_body['pagination']['page_after_value'] = next_value\n\nprint(f'Retrieved {len(all_ion_data)} ion entries.')\n</pre> json_body = {     'owner': 'visible',     'query': {         'results.eln.sections:any': [             'PerovskiteAIon',             'PerovskiteBIon',             'PerovskiteXIon',         ],     },     'pagination': {         'page_size': 10,     }, }  all_ion_data = {}  while len(all_ion_data) &lt; 500:     response = requests.post(f'{base_url}/entries/archive/query', json=json_body)     response_json = response.json()      for entry in response_json['data']:         abbreviation = entry['archive']['data']['abbreviation']         if abbreviation in all_ion_data:             print(f'Duplicate entry found for abbreviation: {abbreviation}')         all_ion_data[abbreviation] = entry['archive']['data']      next_value = response_json['pagination'].get('next_page_after_value')     if not next_value:         break     json_body['pagination']['page_after_value'] = next_value  print(f'Retrieved {len(all_ion_data)} ion entries.') <pre>Retrieved 332 ion entries.\n</pre> In\u00a0[5]: Copied! <pre>print(json.dumps(all_ion_data['MA'], indent=2))\n</pre> print(json.dumps(all_ion_data['MA'], indent=2)) <pre>{\n  \"m_def\": \"perovskite_solar_cell_database.composition.PerovskiteAIon\",\n  \"name\": \"MA perovskite ion\",\n  \"datetime\": \"2024-12-18T14:46:39.211148+00:00\",\n  \"lab_id\": \"perovskite_ion_MA\",\n  \"common_name\": \"Methylammonium\",\n  \"molecular_formula\": \"CH6N+\",\n  \"smiles\": \"C[NH3+]\",\n  \"iupac_name\": \"methylazanium\",\n  \"cas_number\": \"17000-00-9\",\n  \"abbreviation\": \"MA\",\n  \"source_compound_molecular_formula\": \"CH5N\",\n  \"source_compound_smiles\": \"CN\",\n  \"source_compound_iupac_name\": \"methanamine\",\n  \"source_compound_cas_number\": \"74-89-5\",\n  \"elemental_composition\": [\n    {\n      \"element\": \"C\",\n      \"atomic_fraction\": 0.125,\n      \"mass_fraction\": 0.3745730552651735\n    },\n    {\n      \"element\": \"H\",\n      \"atomic_fraction\": 0.75,\n      \"mass_fraction\": 0.1886054095051807\n    },\n    {\n      \"element\": \"N\",\n      \"atomic_fraction\": 0.125,\n      \"mass_fraction\": 0.4368215352296457\n    }\n  ],\n  \"pure_substance\": {\n    \"name\": \"Methylammonium\",\n    \"iupac_name\": \"methylazanium\",\n    \"molecular_formula\": \"CH6N\",\n    \"molecular_mass\": 32.050024196,\n    \"molar_mass\": 32.065,\n    \"monoisotopic_mass\": 32.050024196,\n    \"inchi\": \"InChI=1S/CH5N/c1-2/h2H2,1H3/p+1\",\n    \"inchi_key\": \"BAVYZALUXZFZLV-UHFFFAOYSA-O\",\n    \"smile\": \"C[NH3+]\",\n    \"canonical_smile\": \"C[NH3+]\",\n    \"cas_number\": \"17000-00-9\",\n    \"pub_chem_cid\": 644041,\n    \"pub_chem_link\": \"https://pubchem.ncbi.nlm.nih.gov/compound/644041\"\n  },\n  \"source_compound\": {\n    \"name\": \"Methylamine\",\n    \"iupac_name\": \"methanamine\",\n    \"molecular_formula\": \"CH5N\",\n    \"molecular_mass\": 31.042199164,\n    \"molar_mass\": 31.057,\n    \"monoisotopic_mass\": 31.042199164,\n    \"inchi\": \"InChI=1S/CH5N/c1-2/h2H2,1H3\",\n    \"inchi_key\": \"BAVYZALUXZFZLV-UHFFFAOYSA-N\",\n    \"smile\": \"CN\",\n    \"canonical_smile\": \"CN\",\n    \"cas_number\": \"74-89-5\",\n    \"pub_chem_cid\": 6329,\n    \"pub_chem_link\": \"https://pubchem.ncbi.nlm.nih.gov/compound/6329\"\n  }\n}\n</pre>"},{"location":"notebooks/ions_database/query-ion-database-api.html#basic-imports","title":"Basic imports\u00b6","text":""},{"location":"notebooks/ions_database/query-ion-database-api.html#query-for-an-a-site-ion-with-abbreviation-ma","title":"Query for an A site ion with abbreviation \"MA\"\u00b6","text":""},{"location":"notebooks/ions_database/query-ion-database-api.html#validate-response-against-json-schema","title":"Validate response against JSON Schema\u00b6","text":""},{"location":"notebooks/ions_database/query-ion-database-api.html#query-all-ions-in-the-database","title":"Query all ions in the database\u00b6","text":""},{"location":"notebooks/ions_database/query-perovskite-composition.html","title":"Query Perovskite Compositions","text":"Query NOMAD for perovskite compositions <p>         This notebook demonstrates how query NOMAD for perovskite compositions. The notebook also shows how to validate the ions against the JSON Schema </p><p> </p> <p></p> In\u00a0[1]: Copied! <pre>import json\n\nimport requests\n\nbase_url = 'http://nomad-lab.eu/prod/v1/api/v1'\n</pre> import json  import requests  base_url = 'http://nomad-lab.eu/prod/v1/api/v1' In\u00a0[2]: Copied! <pre>response = requests.post(\n    f'{base_url}/entries/archive/query',\n    json={\n        'owner': 'visible',\n        'query': {\n            'results.eln.sections:any': ['PerovskiteComposition'],\n            'results.material.elements:all': ['Pb'],\n            'results.properties.electronic.band_gap.value': {\n                'lte': 2.6435914460999996e-19\n            },\n        },\n        'pagination': {'page_size': 1},\n    },\n)\nresponse_json = response.json()\ncomposition_data = response_json['data'][0]['archive']['data']\nprint(json.dumps(composition_data, indent=2))\n</pre> response = requests.post(     f'{base_url}/entries/archive/query',     json={         'owner': 'visible',         'query': {             'results.eln.sections:any': ['PerovskiteComposition'],             'results.material.elements:all': ['Pb'],             'results.properties.electronic.band_gap.value': {                 'lte': 2.6435914460999996e-19             },         },         'pagination': {'page_size': 1},     }, ) response_json = response.json() composition_data = response_json['data'][0]['archive']['data'] print(json.dumps(composition_data, indent=2)) <pre>{\n  \"m_def\": \"perovskite_solar_cell_database.composition.PerovskiteComposition\",\n  \"short_form\": \"MAPbI\",\n  \"long_form\": \"MAPbI3\",\n  \"composition_estimate\": \"Estimated from precursor solutions\",\n  \"sample_type\": \"Polycrystalline film\",\n  \"dimensionality\": \"3D\",\n  \"band_gap\": 1.63,\n  \"name\": \"MAPI\",\n  \"datetime\": \"2025-03-31T09:09:48.394259+00:00\",\n  \"ions_a_site\": [\n    {\n      \"m_def\": \"perovskite_solar_cell_database.composition.PerovskiteAIonComponent\",\n      \"system\": \"../uploads/ztDkTFJETdiMgrnRQSgsEA/archive/ZHsLIxapQ6idJZjA25Zohc8FRZzr#/data\",\n      \"common_name\": \"Methylammonium\",\n      \"molecular_formula\": \"CH6N+\",\n      \"smiles\": \"C[NH3+]\",\n      \"iupac_name\": \"methylazanium\",\n      \"cas_number\": \"17000-00-9\",\n      \"abbreviation\": \"MA\",\n      \"source_compound_molecular_formula\": \"CH5N\",\n      \"source_compound_smiles\": \"CN\",\n      \"source_compound_iupac_name\": \"methanamine\",\n      \"source_compound_cas_number\": \"74-89-5\",\n      \"coefficient\": \"1\"\n    }\n  ],\n  \"ions_b_site\": [\n    {\n      \"m_def\": \"perovskite_solar_cell_database.composition.PerovskiteBIonComponent\",\n      \"system\": \"../uploads/ztDkTFJETdiMgrnRQSgsEA/archive/CIsbAqf80HCIft4qH3pjCdKuCOUr#/data\",\n      \"common_name\": \"Lead\",\n      \"molecular_formula\": \"Pb2+\",\n      \"smiles\": \"[Pb+2]\",\n      \"iupac_name\": \"lead(2+)\",\n      \"cas_number\": \"14280-50-3\",\n      \"abbreviation\": \"Pb\",\n      \"source_compound_molecular_formula\": \"Pb\",\n      \"source_compound_smiles\": \"[Pb]\",\n      \"source_compound_iupac_name\": \"Lead\",\n      \"source_compound_cas_number\": \"7439-92-1\",\n      \"coefficient\": \"1\"\n    }\n  ],\n  \"ions_x_site\": [\n    {\n      \"m_def\": \"perovskite_solar_cell_database.composition.PerovskiteXIonComponent\",\n      \"system\": \"../uploads/ztDkTFJETdiMgrnRQSgsEA/archive/EtZIjsyxaPFuo4Hod84m-EcffFNe#/data\",\n      \"common_name\": \"Iodide\",\n      \"molecular_formula\": \"I-\",\n      \"smiles\": \"[I-]\",\n      \"iupac_name\": \"Iodide\",\n      \"cas_number\": \"20461-54-5\",\n      \"abbreviation\": \"I\",\n      \"source_compound_molecular_formula\": \"I2\",\n      \"source_compound_smiles\": \"II\",\n      \"source_compound_iupac_name\": \"molecular iodine\",\n      \"source_compound_cas_number\": \"7553-56-2\",\n      \"coefficient\": \"3\"\n    }\n  ],\n  \"elemental_composition\": [\n    {\n      \"element\": \"Pb\",\n      \"atomic_fraction\": 0.08333333333333333,\n      \"mass_fraction\": 0.3342051647117734\n    },\n    {\n      \"element\": \"C\",\n      \"atomic_fraction\": 0.08333333333333333,\n      \"mass_fraction\": 0.019372770134187728\n    },\n    {\n      \"element\": \"N\",\n      \"atomic_fraction\": 0.08333333333333333,\n      \"mass_fraction\": 0.0225922368753301\n    },\n    {\n      \"element\": \"H\",\n      \"atomic_fraction\": 0.5,\n      \"mass_fraction\": 0.009754597115431996\n    },\n    {\n      \"element\": \"I\",\n      \"atomic_fraction\": 0.25,\n      \"mass_fraction\": 0.6140752311632768\n    }\n  ],\n  \"components\": [\n    {\n      \"m_def\": \"perovskite_solar_cell_database.composition.PerovskiteAIonComponent\",\n      \"system\": \"../uploads/ztDkTFJETdiMgrnRQSgsEA/archive/ZHsLIxapQ6idJZjA25Zohc8FRZzr#/data\",\n      \"common_name\": \"Methylammonium\",\n      \"molecular_formula\": \"CH6N+\",\n      \"smiles\": \"C[NH3+]\",\n      \"iupac_name\": \"methylazanium\",\n      \"cas_number\": \"17000-00-9\",\n      \"abbreviation\": \"MA\",\n      \"source_compound_molecular_formula\": \"CH5N\",\n      \"source_compound_smiles\": \"CN\",\n      \"source_compound_iupac_name\": \"methanamine\",\n      \"source_compound_cas_number\": \"74-89-5\",\n      \"coefficient\": \"1\"\n    },\n    {\n      \"m_def\": \"perovskite_solar_cell_database.composition.PerovskiteBIonComponent\",\n      \"system\": \"../uploads/ztDkTFJETdiMgrnRQSgsEA/archive/CIsbAqf80HCIft4qH3pjCdKuCOUr#/data\",\n      \"common_name\": \"Lead\",\n      \"molecular_formula\": \"Pb2+\",\n      \"smiles\": \"[Pb+2]\",\n      \"iupac_name\": \"lead(2+)\",\n      \"cas_number\": \"14280-50-3\",\n      \"abbreviation\": \"Pb\",\n      \"source_compound_molecular_formula\": \"Pb\",\n      \"source_compound_smiles\": \"[Pb]\",\n      \"source_compound_iupac_name\": \"Lead\",\n      \"source_compound_cas_number\": \"7439-92-1\",\n      \"coefficient\": \"1\"\n    },\n    {\n      \"m_def\": \"perovskite_solar_cell_database.composition.PerovskiteXIonComponent\",\n      \"system\": \"../uploads/ztDkTFJETdiMgrnRQSgsEA/archive/EtZIjsyxaPFuo4Hod84m-EcffFNe#/data\",\n      \"common_name\": \"Iodide\",\n      \"molecular_formula\": \"I-\",\n      \"smiles\": \"[I-]\",\n      \"iupac_name\": \"Iodide\",\n      \"cas_number\": \"20461-54-5\",\n      \"abbreviation\": \"I\",\n      \"source_compound_molecular_formula\": \"I2\",\n      \"source_compound_smiles\": \"II\",\n      \"source_compound_iupac_name\": \"molecular iodine\",\n      \"source_compound_cas_number\": \"7553-56-2\",\n      \"coefficient\": \"3\"\n    }\n  ]\n}\n</pre> In\u00a0[3]: Copied! <pre>from jsonschema import ValidationError, validate\n\nschema_id = (\n    'https://raw.githubusercontent.com/Jesperkemist/'\n    'Perovskite_composition/v1.0.0/perovskite_composition_schema.json'\n)\n\n# Load the JSON schema from the URL\nschema_response = requests.get(schema_id)\nschema = schema_response.json()\n\n# Validate ion_data against the schema\ntry:\n    validate(instance=composition_data, schema=schema)\n    print(f'composition_data is valid against the {schema_id} schema.')\nexcept ValidationError as e:\n    print('Validation error:', e.message)\n</pre> from jsonschema import ValidationError, validate  schema_id = (     'https://raw.githubusercontent.com/Jesperkemist/'     'Perovskite_composition/v1.0.0/perovskite_composition_schema.json' )  # Load the JSON schema from the URL schema_response = requests.get(schema_id) schema = schema_response.json()  # Validate ion_data against the schema try:     validate(instance=composition_data, schema=schema)     print(f'composition_data is valid against the {schema_id} schema.') except ValidationError as e:     print('Validation error:', e.message) <pre>composition_data is valid against the https://raw.githubusercontent.com/Jesperkemist/Perovskite_composition/v1.0.0/perovskite_composition_schema.json schema.\n</pre>"},{"location":"notebooks/ions_database/query-perovskite-composition.html#basic-imports","title":"Basic imports\u00b6","text":""},{"location":"notebooks/ions_database/query-perovskite-composition.html#query-for-a-lead-perovskite-with-band-gap-less-than-165-ev","title":"Query for a lead perovskite with band gap less than 1.65 eV\u00b6","text":""},{"location":"notebooks/ions_database/query-perovskite-composition.html#validate-response-against-json-schema","title":"Validate response against JSON Schema\u00b6","text":""},{"location":"notebooks/perla_notebooks/index.html","title":"PERLA Notebooks","text":""},{"location":"notebooks/perla_notebooks/index.html#perla-notebooks","title":"PERLA Notebooks","text":"<p>This collection of Jupyter notebooks provides comprehensive analysis tools for perovskite solar cell data from PERLA (Perovskite Living Archive) in NOMAD.</p>"},{"location":"notebooks/perla_notebooks/index.html#overview","title":"Overview","text":"<p>PERLA is a continuously updated database of perovskite solar cell performance data, combining both legacy human-curated entries and modern LLM-extracted entries from scientific literature.</p>"},{"location":"notebooks/perla_notebooks/index.html#getting-started","title":"Getting Started","text":""},{"location":"notebooks/perla_notebooks/index.html#prerequisites","title":"Prerequisites","text":"<ol> <li>Query the database first: Start with the query notebook to generate the required data file</li> <li>Install dependencies: Ensure you have the necessary Python packages installed from the souce repositroy. uv is recommended to be used.</li> </ol>"},{"location":"notebooks/perla_notebooks/index.html#workflow","title":"Workflow","text":"<ol> <li>Generate the dataset (required first step):</li> <li>Run query-perovskite-database.ipynb to download data from NOMAD and create <code>perovskite_solar_cell_database.parquet</code></li> <li> <p>\u26a0\ufe0f Note: This query may take up to 1 hour due to API rate limits</p> </li> <li> <p>Analyze the data: Once you have the parquet file, you can run any of the analysis notebooks</p> </li> </ol>"},{"location":"notebooks/perla_notebooks/index.html#notebooks","title":"Notebooks","text":""},{"location":"notebooks/perla_notebooks/index.html#data-retrieval","title":"Data Retrieval","text":"<ul> <li>query-perovskite-database.ipynb - Download perovskite solar cell data from NOMAD and save as parquet file (run this first)</li> </ul>"},{"location":"notebooks/perla_notebooks/index.html#performance-and-evolution-analysis","title":"Performance and Evolution Analysis","text":"<ul> <li>performance-evolution.ipynb - Temporal evolution of power conversion efficiency and device performance metrics</li> <li>architecture-evolution.ipynb - Device architecture trends (n-i-p vs. p-i-n) and material layer evolution</li> <li>bandgap-evolution.ipynb - Temporal changes in bandgap values and absorber compositions</li> </ul>"},{"location":"notebooks/perla_notebooks/index.html#diversity-and-discovery","title":"Diversity and Discovery","text":"<ul> <li>diversity-analysis.ipynb - Material diversity evolution using entropy metrics and Heap's law analysis</li> </ul>"},{"location":"notebooks/perla_notebooks/index.html#machine-learning-applications","title":"Machine Learning Applications","text":"<ul> <li>crabnet-perovskite-bandgap-prediction.ipynb - Composition-based bandgap prediction using CrabNet neural network</li> <li>ml-distribution-shift-case-study.ipynb - Analysis of distribution shift challenges in ML models for perovskite property prediction</li> </ul>"},{"location":"notebooks/perla_notebooks/index.html#data-quality-and-validation","title":"Data Quality and Validation","text":"<ul> <li>physics_filter.ipynb - Physics consistency validation comparing legacy database vs. PERLA pipeline</li> <li>perla-evals-analysis.ipynb - Evaluation metrics for PERLA LLM extraction pipeline performance</li> </ul>"},{"location":"notebooks/perla_notebooks/index.html#automation-and-tools","title":"Automation and Tools","text":"<ul> <li>perovskite-paperbot-plot.ipynb - Visualization of automated literature extraction pipeline filtering steps</li> </ul>"},{"location":"notebooks/perla_notebooks/index.html#data-structure","title":"Data Structure","text":"<p>The <code>perovskite_solar_cell_database.parquet</code> file contains structured data including: - Device performance metrics (PCE, Voc, Jsc, FF) - Material compositions (absorber, ETL, HTL) - Device architecture information - Publication metadata - Extraction source (Manual Entry vs. LLM Extracted)</p>"},{"location":"notebooks/perla_notebooks/__init__.html","title":"init","text":""},{"location":"notebooks/perla_notebooks/architecture-evolution.html","title":"Architecture Evolution Analysis","text":"In\u00a0[1]: \"hide-cell\" Copied! <pre># ruff: noqa: E402, F601, E741\n</pre> # ruff: noqa: E402, F601, E741           Device Architecture Evolution in Perovskite Solar Cells <p>     This notebook analyzes the temporal evolution of device architectures and functional layer materials in perovskite solar cells using data from the Perovskite Database in NOMAD.   </p> <p> </p> In\u00a0[2]: Copied! <pre>from plotly_theme import register_template, set_defaults\n\nregister_template()\nset_defaults()\n</pre> from plotly_theme import register_template, set_defaults  register_template() set_defaults() In\u00a0[\u00a0]: Copied! <pre># Load the data from the parquet file into a DataFrame\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.patches import PathPatch\nfrom matplotlib.path import Path\nfrom scipy import stats\n\n# Configure matplotlib to match plotly theme\nplt.rcParams.update(\n    {\n        'font.family': 'Arial',\n        'font.size': 10,\n        'axes.labelsize': 11,\n        'axes.titlesize': 12,\n        'xtick.labelsize': 10,\n        'ytick.labelsize': 10,\n        'legend.fontsize': 10,\n        'figure.titlesize': 12,\n        'axes.linewidth': 1,\n        'axes.edgecolor': 'black',\n        'axes.facecolor': 'white',\n        'figure.facecolor': 'white',\n        'grid.color': 'lightgray',\n        'grid.linewidth': 0.5,\n        'axes.grid': False,\n        'axes.spines.top': True,\n        'axes.spines.right': True,\n        'savefig.dpi': 300,\n        'savefig.bbox': 'tight',\n        'savefig.facecolor': 'white',\n    }\n)\n\ndf = pd.read_parquet('perovskite_solar_cell_database.parquet')\n</pre> # Load the data from the parquet file into a DataFrame import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib.patches import PathPatch from matplotlib.path import Path from scipy import stats  # Configure matplotlib to match plotly theme plt.rcParams.update(     {         'font.family': 'Arial',         'font.size': 10,         'axes.labelsize': 11,         'axes.titlesize': 12,         'xtick.labelsize': 10,         'ytick.labelsize': 10,         'legend.fontsize': 10,         'figure.titlesize': 12,         'axes.linewidth': 1,         'axes.edgecolor': 'black',         'axes.facecolor': 'white',         'figure.facecolor': 'white',         'grid.color': 'lightgray',         'grid.linewidth': 0.5,         'axes.grid': False,         'axes.spines.top': True,         'axes.spines.right': True,         'savefig.dpi': 300,         'savefig.bbox': 'tight',         'savefig.facecolor': 'white',     } )  df = pd.read_parquet('perovskite_solar_cell_database.parquet') In\u00a0[\u00a0]: Copied! <pre># Set the source_database column based on who entered the data\ndf['source_database'] = df['data.ref.name_of_person_entering_the_data'].apply(\n    lambda x: 'LLM Extracted' if x == 'LLM Extraction' else 'Manual Entry'\n)\n</pre> # Set the source_database column based on who entered the data df['source_database'] = df['data.ref.name_of_person_entering_the_data'].apply(     lambda x: 'LLM Extracted' if x == 'LLM Extraction' else 'Manual Entry' ) In\u00a0[5]: Copied! <pre>df['pub_date'] = pd.to_datetime(df['data.ref.publication_date'], errors='coerce')\ndf['pub_year'] = df['pub_date'].dt.year\ndf['ETL'] = df['results.properties.optoelectronic.solar_cell.electron_transport_layer']\ndf['HTL'] = df['results.properties.optoelectronic.solar_cell.hole_transport_layer']\ndf['absorber'] = df['results.properties.optoelectronic.solar_cell.absorber']\ndf['PCE'] = df['results.properties.optoelectronic.solar_cell.efficiency']\ndf['architecture'] = df['data.cell.architecture']\n\ndf_early = df[(df['pub_year'] &lt; 2022)]\ndf_late = df[(df['pub_year'] &gt;= 2022)]\n</pre> df['pub_date'] = pd.to_datetime(df['data.ref.publication_date'], errors='coerce') df['pub_year'] = df['pub_date'].dt.year df['ETL'] = df['results.properties.optoelectronic.solar_cell.electron_transport_layer'] df['HTL'] = df['results.properties.optoelectronic.solar_cell.hole_transport_layer'] df['absorber'] = df['results.properties.optoelectronic.solar_cell.absorber'] df['PCE'] = df['results.properties.optoelectronic.solar_cell.efficiency'] df['architecture'] = df['data.cell.architecture']  df_early = df[(df['pub_year'] &lt; 2022)] df_late = df[(df['pub_year'] &gt;= 2022)] In\u00a0[6]: Copied! <pre>print(f'Early period entries: {len(df_early)}')\nprint(f'Late period entries: {len(df_late)}')\n</pre> print(f'Early period entries: {len(df_early)}') print(f'Late period entries: {len(df_late)}') <pre>Early period entries: 43701\nLate period entries: 4672\n</pre> In\u00a0[7]: Copied! <pre>arch_data = (\n    df[df['architecture'].isin(['nip', 'pin'])]\n    .groupby(['pub_year', 'architecture'])\n    .size()\n    .unstack(fill_value=0)\n)\n</pre> arch_data = (     df[df['architecture'].isin(['nip', 'pin'])]     .groupby(['pub_year', 'architecture'])     .size()     .unstack(fill_value=0) ) In\u00a0[8]: Copied! <pre>arch_data\n</pre> arch_data Out[8]: architecture nip pin pub_year 2009.0 2 0 2011.0 6 0 2012.0 18 0 2013.0 215 11 2014.0 1112 431 2015.0 2563 968 2016.0 4312 1899 2017.0 5719 2301 2018.0 7284 3312 2019.0 7408 3218 2020.0 1243 625 2021.0 566 226 2022.0 592 445 2023.0 873 394 2024.0 1031 534 2025.0 367 286 In\u00a0[9]: Copied! <pre>arch_pct = arch_data.div(arch_data.sum(axis=1), axis=0) * 100\npin_pct = arch_pct['pin'].to_dict()\n</pre> arch_pct = arch_data.div(arch_data.sum(axis=1), axis=0) * 100 pin_pct = arch_pct['pin'].to_dict() In\u00a0[20]: Copied! <pre>def get_three_layer_flows(df_subset):\n    abs_htl_flows = {}\n    htl_etl_flows = {}\n\n    for _, row in df_subset.iterrows():\n        abs_arr = row['absorber']\n        htl_arr = row['HTL']\n        etl_arr = row['ETL']\n\n        if abs_arr is None or htl_arr is None or etl_arr is None:\n            continue\n        if (\n            not hasattr(abs_arr, '__iter__')\n            or not hasattr(htl_arr, '__iter__')\n            or not hasattr(etl_arr, '__iter__')\n        ):\n            continue\n\n        # Simplify absorber\n        for absorber in abs_arr:\n            if not absorber or absorber == 'Unknown':\n                continue\n            if absorber == 'MAPbI':\n                abs_simple = 'MAPbI'\n            elif 'CsFA' in absorber or 'FAMA' in absorber:\n                abs_simple = 'Mixed'\n            elif absorber == 'FAPbI':\n                abs_simple = 'FAPbI'\n            else:\n                abs_simple = 'Other'\n\n            for htl in htl_arr:\n                if not htl or htl in {'none', 'Unknown'}:\n                    continue\n                if 'Spiro' in htl:\n                    htl_simple = 'Spiro'\n                elif 'PEDOT' in htl:\n                    htl_simple = 'PEDOT:PSS'\n                elif 'NiO' in htl:\n                    htl_simple = 'NiOx'\n                elif 'PTAA' in htl:\n                    htl_simple = 'PTAA'\n                elif 'PACz' in htl or 'SAM' in htl:\n                    htl_simple = 'SAMs'\n                else:\n                    htl_simple = 'Other'\n\n                abs_htl_flows[(abs_simple, htl_simple)] = (\n                    abs_htl_flows.get((abs_simple, htl_simple), 0) + 1\n                )\n\n                for etl in etl_arr:\n                    if not etl or etl in {'none', 'Unknown'}:\n                        continue\n                    if 'TiO2' in etl:\n                        etl_simple = 'TiO2'\n                    elif 'SnO2' in etl:\n                        etl_simple = 'SnO2'\n                    elif 'PCBM' in etl or 'PC61BM' in etl:\n                        etl_simple = 'PCBM'\n                    elif etl == 'C60':\n                        etl_simple = 'C60'\n                    else:\n                        etl_simple = 'Other'\n\n                    htl_etl_flows[(htl_simple, etl_simple)] = (\n                        htl_etl_flows.get((htl_simple, etl_simple), 0) + 1\n                    )\n\n    return abs_htl_flows, htl_etl_flows\n\n\npre_abs_htl_pin, pre_htl_etl_pin = get_three_layer_flows(\n    df_early[df_early['architecture'].isin(['pin'])]\n)\npost_abs_htl_pin, post_htl_etl_pin = get_three_layer_flows(\n    df_late[df_late['architecture'].isin(['pin'])]\n)\n\npre_abs_htl_nip, pre_htl_etl_nip = get_three_layer_flows(\n    df_early[df_early['architecture'].isin(['nip'])]\n)\npost_abs_htl_nip, post_htl_etl_nip = get_three_layer_flows(\n    df_late[df_late['architecture'].isin(['nip'])]\n)\n</pre> def get_three_layer_flows(df_subset):     abs_htl_flows = {}     htl_etl_flows = {}      for _, row in df_subset.iterrows():         abs_arr = row['absorber']         htl_arr = row['HTL']         etl_arr = row['ETL']          if abs_arr is None or htl_arr is None or etl_arr is None:             continue         if (             not hasattr(abs_arr, '__iter__')             or not hasattr(htl_arr, '__iter__')             or not hasattr(etl_arr, '__iter__')         ):             continue          # Simplify absorber         for absorber in abs_arr:             if not absorber or absorber == 'Unknown':                 continue             if absorber == 'MAPbI':                 abs_simple = 'MAPbI'             elif 'CsFA' in absorber or 'FAMA' in absorber:                 abs_simple = 'Mixed'             elif absorber == 'FAPbI':                 abs_simple = 'FAPbI'             else:                 abs_simple = 'Other'              for htl in htl_arr:                 if not htl or htl in {'none', 'Unknown'}:                     continue                 if 'Spiro' in htl:                     htl_simple = 'Spiro'                 elif 'PEDOT' in htl:                     htl_simple = 'PEDOT:PSS'                 elif 'NiO' in htl:                     htl_simple = 'NiOx'                 elif 'PTAA' in htl:                     htl_simple = 'PTAA'                 elif 'PACz' in htl or 'SAM' in htl:                     htl_simple = 'SAMs'                 else:                     htl_simple = 'Other'                  abs_htl_flows[(abs_simple, htl_simple)] = (                     abs_htl_flows.get((abs_simple, htl_simple), 0) + 1                 )                  for etl in etl_arr:                     if not etl or etl in {'none', 'Unknown'}:                         continue                     if 'TiO2' in etl:                         etl_simple = 'TiO2'                     elif 'SnO2' in etl:                         etl_simple = 'SnO2'                     elif 'PCBM' in etl or 'PC61BM' in etl:                         etl_simple = 'PCBM'                     elif etl == 'C60':                         etl_simple = 'C60'                     else:                         etl_simple = 'Other'                      htl_etl_flows[(htl_simple, etl_simple)] = (                         htl_etl_flows.get((htl_simple, etl_simple), 0) + 1                     )      return abs_htl_flows, htl_etl_flows   pre_abs_htl_pin, pre_htl_etl_pin = get_three_layer_flows(     df_early[df_early['architecture'].isin(['pin'])] ) post_abs_htl_pin, post_htl_etl_pin = get_three_layer_flows(     df_late[df_late['architecture'].isin(['pin'])] )  pre_abs_htl_nip, pre_htl_etl_nip = get_three_layer_flows(     df_early[df_early['architecture'].isin(['nip'])] ) post_abs_htl_nip, post_htl_etl_nip = get_three_layer_flows(     df_late[df_late['architecture'].isin(['nip'])] ) In\u00a0[16]: Copied! <pre>def get_global_color_mapping(*flow_pairs):\n    \"\"\"Create consistent color mappings across multiple datasets.\n\n    Args:\n        *flow_pairs: Tuples of (abs_htl_flows, htl_etl_flows) for each dataset\n\n    Returns:\n        Tuple of (abs_colors, htl_colors, etl_colors) dictionaries\n    \"\"\"\n    # Define color palettes matching plotly theme\n    COLOR_PALETTES = {\n        'absorber': ['#1f77b4', '#ff0e5a', '#e9c821', '#ba78d6', '#4cd8a5', '#ff9408'],\n        'htl': ['#ba78d6', '#86d9ea', '#4cd8a5', '#7f7f7f', '#e9c821', '#17becf'],\n        'etl': ['#1f77b4', '#4cd8a5', '#ff9408', '#ff0e5a', '#ba78d6', '#86d9ea'],\n    }\n\n    # Aggregate counts across all datasets\n    abs_counts = {}\n    htl_counts = {}\n    etl_counts = {}\n\n    for abs_htl_flows, htl_etl_flows in flow_pairs:\n        for (abs, htl), count in abs_htl_flows.items():\n            abs_counts[abs] = abs_counts.get(abs, 0) + count\n            htl_counts[htl] = htl_counts.get(htl, 0) + count\n\n        for (htl, etl), count in htl_etl_flows.items():\n            htl_counts[htl] = htl_counts.get(htl, 0) + count\n            etl_counts[etl] = etl_counts.get(etl, 0) + count\n\n    # Sort by total frequency across all datasets\n    abs_labels = sorted(abs_counts.keys(), key=abs_counts.get, reverse=True)\n    htl_labels = sorted(htl_counts.keys(), key=htl_counts.get, reverse=True)\n    etl_labels = sorted(etl_counts.keys(), key=etl_counts.get, reverse=True)\n\n    # Create consistent color mappings\n    abs_colors = {\n        label: COLOR_PALETTES['absorber'][i % len(COLOR_PALETTES['absorber'])]\n        for i, label in enumerate(abs_labels)\n    }\n    htl_colors = {\n        label: COLOR_PALETTES['htl'][i % len(COLOR_PALETTES['htl'])]\n        for i, label in enumerate(htl_labels)\n    }\n    etl_colors = {\n        label: COLOR_PALETTES['etl'][i % len(COLOR_PALETTES['etl'])]\n        for i, label in enumerate(etl_labels)\n    }\n\n    return abs_colors, htl_colors, etl_colors\n</pre> def get_global_color_mapping(*flow_pairs):     \"\"\"Create consistent color mappings across multiple datasets.      Args:         *flow_pairs: Tuples of (abs_htl_flows, htl_etl_flows) for each dataset      Returns:         Tuple of (abs_colors, htl_colors, etl_colors) dictionaries     \"\"\"     # Define color palettes matching plotly theme     COLOR_PALETTES = {         'absorber': ['#1f77b4', '#ff0e5a', '#e9c821', '#ba78d6', '#4cd8a5', '#ff9408'],         'htl': ['#ba78d6', '#86d9ea', '#4cd8a5', '#7f7f7f', '#e9c821', '#17becf'],         'etl': ['#1f77b4', '#4cd8a5', '#ff9408', '#ff0e5a', '#ba78d6', '#86d9ea'],     }      # Aggregate counts across all datasets     abs_counts = {}     htl_counts = {}     etl_counts = {}      for abs_htl_flows, htl_etl_flows in flow_pairs:         for (abs, htl), count in abs_htl_flows.items():             abs_counts[abs] = abs_counts.get(abs, 0) + count             htl_counts[htl] = htl_counts.get(htl, 0) + count          for (htl, etl), count in htl_etl_flows.items():             htl_counts[htl] = htl_counts.get(htl, 0) + count             etl_counts[etl] = etl_counts.get(etl, 0) + count      # Sort by total frequency across all datasets     abs_labels = sorted(abs_counts.keys(), key=abs_counts.get, reverse=True)     htl_labels = sorted(htl_counts.keys(), key=htl_counts.get, reverse=True)     etl_labels = sorted(etl_counts.keys(), key=etl_counts.get, reverse=True)      # Create consistent color mappings     abs_colors = {         label: COLOR_PALETTES['absorber'][i % len(COLOR_PALETTES['absorber'])]         for i, label in enumerate(abs_labels)     }     htl_colors = {         label: COLOR_PALETTES['htl'][i % len(COLOR_PALETTES['htl'])]         for i, label in enumerate(htl_labels)     }     etl_colors = {         label: COLOR_PALETTES['etl'][i % len(COLOR_PALETTES['etl'])]         for i, label in enumerate(etl_labels)     }      return abs_colors, htl_colors, etl_colors In\u00a0[\u00a0]: Copied! <pre>def format_chemical_formula(name):\n    \"\"\"Convert chemical formulas to proper subscript notation using mathtext.\"\"\"\n    formula_map = {\n        'C60': r'C$_{60}$',\n        'SnO2': r'SnO$_2$',\n        'TiO2': r'TiO$_2$',\n        'NiOx': r'NiO$_x$',\n        'PbI2': r'PbI$_2$',\n        'ZnO2': r'ZnO$_2$',\n        'MAPbI': r'MAPbI',  # no change needed\n        'FAPbI': r'FAPbI',\n    }\n    return formula_map.get(name, name)\n\n\ndef draw_three_layer_alluvial(  # noqa: PLR0913\n    ax,\n    abs_htl_flows,\n    htl_etl_flows,\n    abs_colors,\n    htl_colors,\n    etl_colors,\n    title='',\n    panel_label='',\n):\n    \"\"\"Draw a 3-layer alluvial diagram: Absorber \u2192 HTL \u2192 ETL\n\n    Uses provided color mappings for consistency across multiple plots.\n    \"\"\"\n    # Extract unique materials from flows and sort by frequency\n    abs_counts = {}\n    htl_counts = {}\n    etl_counts = {}\n\n    for (abs, htl), count in abs_htl_flows.items():\n        abs_counts[abs] = abs_counts.get(abs, 0) + count\n        htl_counts[htl] = htl_counts.get(htl, 0) + count\n\n    for (htl, etl), count in htl_etl_flows.items():\n        htl_counts[htl] = htl_counts.get(htl, 0) + count\n        etl_counts[etl] = etl_counts.get(etl, 0) + count\n\n    # Sort materials by count (most common first) within this dataset\n    abs_labels = sorted(abs_counts.keys(), key=abs_counts.get, reverse=True)\n    htl_labels = sorted(htl_counts.keys(), key=htl_counts.get, reverse=True)\n    etl_labels = sorted(etl_counts.keys(), key=etl_counts.get, reverse=True)\n\n    # Calculate totals for normalization\n    total_abs_htl = sum(abs_htl_flows.values())\n    total_htl_etl = sum(htl_etl_flows.values())\n\n    # Calculate heights for each layer\n    abs_heights = {label: abs_counts[label] / total_abs_htl for label in abs_labels}\n\n    htl_heights_left = {}\n    htl_heights_right = {}\n    for label in htl_labels:\n        htl_heights_left[label] = (\n            sum(v for (a, h), v in abs_htl_flows.items() if h == label) / total_abs_htl\n        )\n        htl_heights_right[label] = (\n            sum(v for (h, e), v in htl_etl_flows.items() if h == label) / total_htl_etl\n        )\n\n    etl_heights = {label: etl_counts[label] / total_htl_etl for label in etl_labels}\n\n    # Positions: x = 0 (Absorber), 0.4 (HTL), 0.8 (ETL)\n    x_abs, x_htl, x_etl = 0, 0.4, 0.8\n    bar_width = 0.06\n\n    # Draw Absorber nodes\n    y_pos = 0\n    abs_positions = {}\n    for label in abs_labels:\n        height = abs_heights[label]\n        if height &gt; 0.01:\n            rect = plt.Rectangle(\n                (x_abs, y_pos),\n                bar_width,\n                height,\n                facecolor=abs_colors[label],\n                edgecolor='white',\n                linewidth=1,\n            )\n            ax.add_patch(rect)\n            abs_positions[label] = (y_pos, y_pos + height)\n            if height &gt; 0.03:\n                ax.text(\n                    x_abs - 0.02,\n                    y_pos + height / 2,\n                    format_chemical_formula(label),\n                    ha='right',\n                    va='center',\n                    fontsize=7,\n                    fontweight='normal',\n                )\n            y_pos += height + 0.008\n\n    # Draw HTL nodes (use average of left and right heights)\n    y_pos = 0\n    htl_positions = {}\n    for label in htl_labels:\n        height = (htl_heights_left.get(label, 0) + htl_heights_right.get(label, 0)) / 2\n        if height &gt; 0.01:\n            rect = plt.Rectangle(\n                (x_htl, y_pos),\n                bar_width,\n                height,\n                facecolor=htl_colors[label],\n                edgecolor='white',\n                linewidth=1,\n            )\n            ax.add_patch(rect)\n            htl_positions[label] = (y_pos, y_pos + height)\n            if height &gt; 0.04:\n                ax.text(\n                    x_htl + bar_width / 2,\n                    y_pos + height / 2,\n                    format_chemical_formula(label),\n                    ha='center',\n                    va='center',\n                    fontsize=7,\n                    fontweight='normal',\n                    rotation=90,\n                )\n            y_pos += height + 0.008\n\n    # Draw ETL nodes\n    y_pos = 0\n    etl_positions = {}\n    for label in etl_labels:\n        height = etl_heights[label]\n        if height &gt; 0.01:\n            rect = plt.Rectangle(\n                (x_etl, y_pos),\n                bar_width,\n                height,\n                facecolor=etl_colors[label],\n                edgecolor='white',\n                linewidth=1,\n            )\n            ax.add_patch(rect)\n            etl_positions[label] = (y_pos, y_pos + height)\n            if height &gt; 0.03:\n                ax.text(\n                    x_etl + bar_width + 0.02,\n                    y_pos + height / 2,\n                    format_chemical_formula(label),\n                    ha='left',\n                    va='center',\n                    fontsize=7,\n                    fontweight='normal',\n                )\n            y_pos += height + 0.008\n\n    # Draw flows: Absorber \u2192 HTL\n    abs_cursors = {l: abs_positions[l][0] for l in abs_positions}\n    htl_cursors_left = {l: htl_positions[l][0] for l in htl_positions}\n\n    for (abs_label, htl_label), value in sorted(\n        abs_htl_flows.items(), key=lambda x: -x[1]\n    ):\n        if abs_label not in abs_positions or htl_label not in htl_positions:\n            continue\n\n        height = value / total_abs_htl\n        if height &lt; 0.005:\n            continue\n\n        y_abs = abs_cursors[abs_label]\n        y_htl = htl_cursors_left[htl_label]\n\n        verts = [\n            (x_abs + bar_width, y_abs),\n            (x_abs + bar_width + 0.1, y_abs),\n            (x_htl - 0.1, y_htl),\n            (x_htl, y_htl),\n            (x_htl, y_htl + height),\n            (x_htl - 0.1, y_htl + height),\n            (x_abs + bar_width + 0.1, y_abs + height),\n            (x_abs + bar_width, y_abs + height),\n            (x_abs + bar_width, y_abs),\n        ]\n\n        codes = [\n            Path.MOVETO,\n            Path.CURVE4,\n            Path.CURVE4,\n            Path.CURVE4,\n            Path.LINETO,\n            Path.CURVE4,\n            Path.CURVE4,\n            Path.CURVE4,\n            Path.CLOSEPOLY,\n        ]\n\n        path = Path(verts, codes)\n        patch = PathPatch(\n            path, facecolor=abs_colors[abs_label], alpha=0.4, edgecolor='none'\n        )\n        ax.add_patch(patch)\n\n        abs_cursors[abs_label] += height\n        htl_cursors_left[htl_label] += height\n\n    # Draw flows: HTL \u2192 ETL\n    htl_cursors_right = {l: htl_positions[l][0] for l in htl_positions}\n    etl_cursors = {l: etl_positions[l][0] for l in etl_positions}\n\n    for (htl_label, etl_label), value in sorted(\n        htl_etl_flows.items(), key=lambda x: -x[1]\n    ):\n        if htl_label not in htl_positions or etl_label not in etl_positions:\n            continue\n\n        height = value / total_htl_etl\n        if height &lt; 0.005:\n            continue\n\n        y_htl = htl_cursors_right[htl_label]\n        y_etl = etl_cursors[etl_label]\n\n        verts = [\n            (x_htl + bar_width, y_htl),\n            (x_htl + bar_width + 0.1, y_htl),\n            (x_etl - 0.1, y_etl),\n            (x_etl, y_etl),\n            (x_etl, y_etl + height),\n            (x_etl - 0.1, y_etl + height),\n            (x_htl + bar_width + 0.1, y_htl + height),\n            (x_htl + bar_width, y_htl + height),\n            (x_htl + bar_width, y_htl),\n        ]\n\n        codes = [\n            Path.MOVETO,\n            Path.CURVE4,\n            Path.CURVE4,\n            Path.CURVE4,\n            Path.LINETO,\n            Path.CURVE4,\n            Path.CURVE4,\n            Path.CURVE4,\n            Path.CLOSEPOLY,\n        ]\n\n        path = Path(verts, codes)\n        patch = PathPatch(\n            path, facecolor=htl_colors[htl_label], alpha=0.4, edgecolor='none'\n        )\n        ax.add_patch(patch)\n\n        htl_cursors_right[htl_label] += height\n        etl_cursors[etl_label] += height\n\n    # Add layer labels\n    ax.text(x_abs + bar_width / 2, -0.02, 'Absorber', ha='center', va='top', fontsize=8)\n    ax.text(x_htl + bar_width / 2, -0.02, 'HTL', ha='center', va='top', fontsize=8)\n    ax.text(x_etl + bar_width / 2, -0.02, 'ETL', ha='center', va='top', fontsize=8)\n\n    # Add panel label (a, b, etc.)\n    if panel_label:\n        ax.text(\n            -0.15,\n            1.02,\n            panel_label,\n            fontsize=14,\n            fontweight='bold',\n            ha='left',\n            va='bottom',\n            transform=ax.transAxes,\n        )\n\n    # Add title\n    if title:\n        ax.text(\n            0.5,\n            1.02,\n            title,\n            fontsize=10,\n            fontweight='bold',\n            ha='center',\n            va='bottom',\n            transform=ax.transAxes,\n        )\n\n    ax.set_xlim(-0.15, 1.0)\n    ax.set_ylim(-0.12, 1.05)\n    ax.axis('off')\n</pre> def format_chemical_formula(name):     \"\"\"Convert chemical formulas to proper subscript notation using mathtext.\"\"\"     formula_map = {         'C60': r'C$_{60}$',         'SnO2': r'SnO$_2$',         'TiO2': r'TiO$_2$',         'NiOx': r'NiO$_x$',         'PbI2': r'PbI$_2$',         'ZnO2': r'ZnO$_2$',         'MAPbI': r'MAPbI',  # no change needed         'FAPbI': r'FAPbI',     }     return formula_map.get(name, name)   def draw_three_layer_alluvial(  # noqa: PLR0913     ax,     abs_htl_flows,     htl_etl_flows,     abs_colors,     htl_colors,     etl_colors,     title='',     panel_label='', ):     \"\"\"Draw a 3-layer alluvial diagram: Absorber \u2192 HTL \u2192 ETL      Uses provided color mappings for consistency across multiple plots.     \"\"\"     # Extract unique materials from flows and sort by frequency     abs_counts = {}     htl_counts = {}     etl_counts = {}      for (abs, htl), count in abs_htl_flows.items():         abs_counts[abs] = abs_counts.get(abs, 0) + count         htl_counts[htl] = htl_counts.get(htl, 0) + count      for (htl, etl), count in htl_etl_flows.items():         htl_counts[htl] = htl_counts.get(htl, 0) + count         etl_counts[etl] = etl_counts.get(etl, 0) + count      # Sort materials by count (most common first) within this dataset     abs_labels = sorted(abs_counts.keys(), key=abs_counts.get, reverse=True)     htl_labels = sorted(htl_counts.keys(), key=htl_counts.get, reverse=True)     etl_labels = sorted(etl_counts.keys(), key=etl_counts.get, reverse=True)      # Calculate totals for normalization     total_abs_htl = sum(abs_htl_flows.values())     total_htl_etl = sum(htl_etl_flows.values())      # Calculate heights for each layer     abs_heights = {label: abs_counts[label] / total_abs_htl for label in abs_labels}      htl_heights_left = {}     htl_heights_right = {}     for label in htl_labels:         htl_heights_left[label] = (             sum(v for (a, h), v in abs_htl_flows.items() if h == label) / total_abs_htl         )         htl_heights_right[label] = (             sum(v for (h, e), v in htl_etl_flows.items() if h == label) / total_htl_etl         )      etl_heights = {label: etl_counts[label] / total_htl_etl for label in etl_labels}      # Positions: x = 0 (Absorber), 0.4 (HTL), 0.8 (ETL)     x_abs, x_htl, x_etl = 0, 0.4, 0.8     bar_width = 0.06      # Draw Absorber nodes     y_pos = 0     abs_positions = {}     for label in abs_labels:         height = abs_heights[label]         if height &gt; 0.01:             rect = plt.Rectangle(                 (x_abs, y_pos),                 bar_width,                 height,                 facecolor=abs_colors[label],                 edgecolor='white',                 linewidth=1,             )             ax.add_patch(rect)             abs_positions[label] = (y_pos, y_pos + height)             if height &gt; 0.03:                 ax.text(                     x_abs - 0.02,                     y_pos + height / 2,                     format_chemical_formula(label),                     ha='right',                     va='center',                     fontsize=7,                     fontweight='normal',                 )             y_pos += height + 0.008      # Draw HTL nodes (use average of left and right heights)     y_pos = 0     htl_positions = {}     for label in htl_labels:         height = (htl_heights_left.get(label, 0) + htl_heights_right.get(label, 0)) / 2         if height &gt; 0.01:             rect = plt.Rectangle(                 (x_htl, y_pos),                 bar_width,                 height,                 facecolor=htl_colors[label],                 edgecolor='white',                 linewidth=1,             )             ax.add_patch(rect)             htl_positions[label] = (y_pos, y_pos + height)             if height &gt; 0.04:                 ax.text(                     x_htl + bar_width / 2,                     y_pos + height / 2,                     format_chemical_formula(label),                     ha='center',                     va='center',                     fontsize=7,                     fontweight='normal',                     rotation=90,                 )             y_pos += height + 0.008      # Draw ETL nodes     y_pos = 0     etl_positions = {}     for label in etl_labels:         height = etl_heights[label]         if height &gt; 0.01:             rect = plt.Rectangle(                 (x_etl, y_pos),                 bar_width,                 height,                 facecolor=etl_colors[label],                 edgecolor='white',                 linewidth=1,             )             ax.add_patch(rect)             etl_positions[label] = (y_pos, y_pos + height)             if height &gt; 0.03:                 ax.text(                     x_etl + bar_width + 0.02,                     y_pos + height / 2,                     format_chemical_formula(label),                     ha='left',                     va='center',                     fontsize=7,                     fontweight='normal',                 )             y_pos += height + 0.008      # Draw flows: Absorber \u2192 HTL     abs_cursors = {l: abs_positions[l][0] for l in abs_positions}     htl_cursors_left = {l: htl_positions[l][0] for l in htl_positions}      for (abs_label, htl_label), value in sorted(         abs_htl_flows.items(), key=lambda x: -x[1]     ):         if abs_label not in abs_positions or htl_label not in htl_positions:             continue          height = value / total_abs_htl         if height &lt; 0.005:             continue          y_abs = abs_cursors[abs_label]         y_htl = htl_cursors_left[htl_label]          verts = [             (x_abs + bar_width, y_abs),             (x_abs + bar_width + 0.1, y_abs),             (x_htl - 0.1, y_htl),             (x_htl, y_htl),             (x_htl, y_htl + height),             (x_htl - 0.1, y_htl + height),             (x_abs + bar_width + 0.1, y_abs + height),             (x_abs + bar_width, y_abs + height),             (x_abs + bar_width, y_abs),         ]          codes = [             Path.MOVETO,             Path.CURVE4,             Path.CURVE4,             Path.CURVE4,             Path.LINETO,             Path.CURVE4,             Path.CURVE4,             Path.CURVE4,             Path.CLOSEPOLY,         ]          path = Path(verts, codes)         patch = PathPatch(             path, facecolor=abs_colors[abs_label], alpha=0.4, edgecolor='none'         )         ax.add_patch(patch)          abs_cursors[abs_label] += height         htl_cursors_left[htl_label] += height      # Draw flows: HTL \u2192 ETL     htl_cursors_right = {l: htl_positions[l][0] for l in htl_positions}     etl_cursors = {l: etl_positions[l][0] for l in etl_positions}      for (htl_label, etl_label), value in sorted(         htl_etl_flows.items(), key=lambda x: -x[1]     ):         if htl_label not in htl_positions or etl_label not in etl_positions:             continue          height = value / total_htl_etl         if height &lt; 0.005:             continue          y_htl = htl_cursors_right[htl_label]         y_etl = etl_cursors[etl_label]          verts = [             (x_htl + bar_width, y_htl),             (x_htl + bar_width + 0.1, y_htl),             (x_etl - 0.1, y_etl),             (x_etl, y_etl),             (x_etl, y_etl + height),             (x_etl - 0.1, y_etl + height),             (x_htl + bar_width + 0.1, y_htl + height),             (x_htl + bar_width, y_htl + height),             (x_htl + bar_width, y_htl),         ]          codes = [             Path.MOVETO,             Path.CURVE4,             Path.CURVE4,             Path.CURVE4,             Path.LINETO,             Path.CURVE4,             Path.CURVE4,             Path.CURVE4,             Path.CLOSEPOLY,         ]          path = Path(verts, codes)         patch = PathPatch(             path, facecolor=htl_colors[htl_label], alpha=0.4, edgecolor='none'         )         ax.add_patch(patch)          htl_cursors_right[htl_label] += height         etl_cursors[etl_label] += height      # Add layer labels     ax.text(x_abs + bar_width / 2, -0.02, 'Absorber', ha='center', va='top', fontsize=8)     ax.text(x_htl + bar_width / 2, -0.02, 'HTL', ha='center', va='top', fontsize=8)     ax.text(x_etl + bar_width / 2, -0.02, 'ETL', ha='center', va='top', fontsize=8)      # Add panel label (a, b, etc.)     if panel_label:         ax.text(             -0.15,             1.02,             panel_label,             fontsize=14,             fontweight='bold',             ha='left',             va='bottom',             transform=ax.transAxes,         )      # Add title     if title:         ax.text(             0.5,             1.02,             title,             fontsize=10,             fontweight='bold',             ha='center',             va='bottom',             transform=ax.transAxes,         )      ax.set_xlim(-0.15, 1.0)     ax.set_ylim(-0.12, 1.05)     ax.axis('off') In\u00a0[34]: Copied! <pre>def get_architecture_evolution(df):\n    \"\"\"Get PIN vs NIP percentages by year.\"\"\"\n    arch_by_year = (\n        df[df['architecture'].isin(['nip', 'pin'])]\n        .groupby(['pub_year', 'architecture'])\n        .size()\n        .unstack(fill_value=0)\n    )\n    arch_pct = arch_by_year.div(arch_by_year.sum(axis=1), axis=0) * 100\n    return arch_pct\n\n\ndef get_etl_evolution(df):\n    \"\"\"Get TiO2 and SnO2 percentages by year.\"\"\"\n    etl_by_year = {}\n\n    for year in sorted(df['pub_year'].dropna().unique()):\n        df_year = df[df['pub_year'] == year]\n        all_etls = []\n\n        for _, row in df_year.iterrows():\n            etl_arr = row['ETL']\n            if etl_arr is None or not hasattr(etl_arr, '__iter__'):\n                continue\n            for etl in etl_arr:\n                if etl and etl not in {'none', 'Unknown'}:\n                    all_etls.append(etl)\n\n        if len(all_etls) == 0:\n            continue\n\n        # Count TiO2 and SnO2\n        tio2_count = sum(1 for e in all_etls if 'TiO2' in e)\n        sno2_count = sum(1 for e in all_etls if 'SnO2' in e)\n        total = len(all_etls)\n\n        etl_by_year[year] = {\n            'TiO2': tio2_count / total * 100,\n            'SnO2': sno2_count / total * 100,\n        }\n\n    return pd.DataFrame(etl_by_year).T\n\n\n# Get evolution data\narch_evolution = get_architecture_evolution(df)\netl_evolution = get_etl_evolution(df)\n\nprint('Architecture evolution:')\nprint(arch_evolution)\nprint('\\nETL evolution:')\nprint(etl_evolution)\n</pre> def get_architecture_evolution(df):     \"\"\"Get PIN vs NIP percentages by year.\"\"\"     arch_by_year = (         df[df['architecture'].isin(['nip', 'pin'])]         .groupby(['pub_year', 'architecture'])         .size()         .unstack(fill_value=0)     )     arch_pct = arch_by_year.div(arch_by_year.sum(axis=1), axis=0) * 100     return arch_pct   def get_etl_evolution(df):     \"\"\"Get TiO2 and SnO2 percentages by year.\"\"\"     etl_by_year = {}      for year in sorted(df['pub_year'].dropna().unique()):         df_year = df[df['pub_year'] == year]         all_etls = []          for _, row in df_year.iterrows():             etl_arr = row['ETL']             if etl_arr is None or not hasattr(etl_arr, '__iter__'):                 continue             for etl in etl_arr:                 if etl and etl not in {'none', 'Unknown'}:                     all_etls.append(etl)          if len(all_etls) == 0:             continue          # Count TiO2 and SnO2         tio2_count = sum(1 for e in all_etls if 'TiO2' in e)         sno2_count = sum(1 for e in all_etls if 'SnO2' in e)         total = len(all_etls)          etl_by_year[year] = {             'TiO2': tio2_count / total * 100,             'SnO2': sno2_count / total * 100,         }      return pd.DataFrame(etl_by_year).T   # Get evolution data arch_evolution = get_architecture_evolution(df) etl_evolution = get_etl_evolution(df)  print('Architecture evolution:') print(arch_evolution) print('\\nETL evolution:') print(etl_evolution) <pre>Architecture evolution:\narchitecture         nip        pin\npub_year                           \n2009.0        100.000000   0.000000\n2011.0        100.000000   0.000000\n2012.0        100.000000   0.000000\n2013.0         95.132743   4.867257\n2014.0         72.067401  27.932599\n2015.0         72.585670  27.414330\n2016.0         69.425213  30.574787\n2017.0         71.309227  28.690773\n2018.0         68.742922  31.257078\n2019.0         69.715791  30.284209\n2020.0         66.541756  33.458244\n2021.0         71.464646  28.535354\n2022.0         57.087753  42.912247\n2023.0         68.902920  31.097080\n2024.0         65.878594  34.121406\n2025.0         56.202144  43.797856\n\nETL evolution:\n              TiO2       SnO2\n2009.0  100.000000   0.000000\n2011.0  100.000000   0.000000\n2012.0   94.285714   0.000000\n2013.0   77.294686   0.000000\n2014.0   65.475743   0.000000\n2015.0   62.765407   1.018471\n2016.0   57.076476   2.333011\n2017.0   55.780584   3.871161\n2018.0   48.009616   7.212345\n2019.0   44.607704  11.578828\n2020.0   39.519946  17.308993\n2021.0   35.543562  25.057826\n2022.0   24.522489  21.441774\n2023.0   29.578438  35.176227\n2024.0   22.216187  35.415535\n2025.0   20.048309  28.985507\n</pre> In\u00a0[40]: Copied! <pre># Create consistent color mapping across both periods\nabs_colors, htl_colors, etl_colors = get_global_color_mapping(\n    (pre_abs_htl_pin, pre_htl_etl_pin), (post_abs_htl_pin, post_htl_etl_pin)\n)\n\n# Define colors for line plots (from plotly theme)\narch_colors = {'pin': '#ff0e5a', 'nip': '#1f77b4'}\netl_line_colors = {'TiO2': '#1f77b4', 'SnO2': '#4cd8a5'}\n\n# Create 2x2 Nature figure with taller alluvial plots (panels a, b)\n# height_ratios makes top row 1.5x taller than bottom row\nfig = plt.figure(figsize=(7.2, 6.5))\ngs = fig.add_gridspec(2, 2, hspace=0.1, wspace=0.3, height_ratios=[3, 1])\n\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1])\nax3 = fig.add_subplot(gs[1, 0])\nax4 = fig.add_subplot(gs[1, 1])\n\n# Draw alluvial diagrams (panels a, b)\ndraw_three_layer_alluvial(\n    ax1,\n    pre_abs_htl_pin,\n    pre_htl_etl_pin,\n    abs_colors,\n    htl_colors,\n    etl_colors,\n    title='Pre-2022',\n    panel_label='a',\n)\ndraw_three_layer_alluvial(\n    ax2,\n    post_abs_htl_pin,\n    post_htl_etl_pin,\n    abs_colors,\n    htl_colors,\n    etl_colors,\n    title='2022+',\n    panel_label='b',\n)\n\n# Draw architecture evolution (panel c)\nax3.text(\n    -0.25,\n    1.05,\n    'c',\n    fontsize=14,\n    fontweight='bold',\n    ha='left',\n    va='bottom',\n    transform=ax3.transAxes,\n)\n\nfor arch in arch_evolution.columns:\n    ax3.plot(\n        arch_evolution.index,\n        arch_evolution[arch],\n        'o-',\n        label=arch.upper(),\n        color=arch_colors[arch],\n        linewidth=2,\n        markersize=4,\n    )\n\nax3.set_xlabel('Publication Year', fontsize=9)\nax3.set_ylabel('Adoption / %', fontsize=9)\nax3.set_title('Device Architecture Evolution', fontsize=10, fontweight='bold', pad=8)\nax3.legend(frameon=False, fontsize=8, loc='best')\nax3.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\nax3.set_xlim(2012, 2026)\nax3.set_ylim(0, 105)\n\n# Draw ETL material evolution (panel d)\nax4.text(\n    -0.25,\n    1.05,\n    'd',\n    fontsize=14,\n    fontweight='bold',\n    ha='left',\n    va='bottom',\n    transform=ax4.transAxes,\n)\n\nfor etl in ['TiO2', 'SnO2']:\n    if etl in etl_evolution.columns:\n        ax4.plot(\n            etl_evolution.index,\n            etl_evolution[etl],\n            'o-',\n            label=format_chemical_formula(etl),\n            color=etl_line_colors[etl],\n            linewidth=2,\n            markersize=4,\n        )\n\nax4.set_xlabel('Publication Year', fontsize=9)\nax4.set_ylabel('Adoption / %', fontsize=9)\nax4.set_title('ETL Material Evolution', fontsize=10, fontweight='bold', pad=8)\nax4.legend(frameon=False, fontsize=8, loc='best')\nax4.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\nax4.set_xlim(2012, 2026)\nax4.set_ylim(0, 105)\n\nfig.tight_layout()\nfig.savefig(\n    'perovskite_solar_cell_evolution_figure_pin.pdf',\n)\n</pre> # Create consistent color mapping across both periods abs_colors, htl_colors, etl_colors = get_global_color_mapping(     (pre_abs_htl_pin, pre_htl_etl_pin), (post_abs_htl_pin, post_htl_etl_pin) )  # Define colors for line plots (from plotly theme) arch_colors = {'pin': '#ff0e5a', 'nip': '#1f77b4'} etl_line_colors = {'TiO2': '#1f77b4', 'SnO2': '#4cd8a5'}  # Create 2x2 Nature figure with taller alluvial plots (panels a, b) # height_ratios makes top row 1.5x taller than bottom row fig = plt.figure(figsize=(7.2, 6.5)) gs = fig.add_gridspec(2, 2, hspace=0.1, wspace=0.3, height_ratios=[3, 1])  ax1 = fig.add_subplot(gs[0, 0]) ax2 = fig.add_subplot(gs[0, 1]) ax3 = fig.add_subplot(gs[1, 0]) ax4 = fig.add_subplot(gs[1, 1])  # Draw alluvial diagrams (panels a, b) draw_three_layer_alluvial(     ax1,     pre_abs_htl_pin,     pre_htl_etl_pin,     abs_colors,     htl_colors,     etl_colors,     title='Pre-2022',     panel_label='a', ) draw_three_layer_alluvial(     ax2,     post_abs_htl_pin,     post_htl_etl_pin,     abs_colors,     htl_colors,     etl_colors,     title='2022+',     panel_label='b', )  # Draw architecture evolution (panel c) ax3.text(     -0.25,     1.05,     'c',     fontsize=14,     fontweight='bold',     ha='left',     va='bottom',     transform=ax3.transAxes, )  for arch in arch_evolution.columns:     ax3.plot(         arch_evolution.index,         arch_evolution[arch],         'o-',         label=arch.upper(),         color=arch_colors[arch],         linewidth=2,         markersize=4,     )  ax3.set_xlabel('Publication Year', fontsize=9) ax3.set_ylabel('Adoption / %', fontsize=9) ax3.set_title('Device Architecture Evolution', fontsize=10, fontweight='bold', pad=8) ax3.legend(frameon=False, fontsize=8, loc='best') ax3.grid(True, alpha=0.3, linestyle='--', linewidth=0.5) ax3.set_xlim(2012, 2026) ax3.set_ylim(0, 105)  # Draw ETL material evolution (panel d) ax4.text(     -0.25,     1.05,     'd',     fontsize=14,     fontweight='bold',     ha='left',     va='bottom',     transform=ax4.transAxes, )  for etl in ['TiO2', 'SnO2']:     if etl in etl_evolution.columns:         ax4.plot(             etl_evolution.index,             etl_evolution[etl],             'o-',             label=format_chemical_formula(etl),             color=etl_line_colors[etl],             linewidth=2,             markersize=4,         )  ax4.set_xlabel('Publication Year', fontsize=9) ax4.set_ylabel('Adoption / %', fontsize=9) ax4.set_title('ETL Material Evolution', fontsize=10, fontweight='bold', pad=8) ax4.legend(frameon=False, fontsize=8, loc='best') ax4.grid(True, alpha=0.3, linestyle='--', linewidth=0.5) ax4.set_xlim(2012, 2026) ax4.set_ylim(0, 105)  fig.tight_layout() fig.savefig(     'perovskite_solar_cell_evolution_figure_pin.pdf', ) <pre>/var/folders/gk/s1v9_48163q2rxpc1x2gq21m0000gn/T/ipykernel_31691/668157619.py:60: UserWarning:\n\nThis figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Create consistent color mapping across both periods\nabs_colors, htl_colors, etl_colors = get_global_color_mapping(\n    (pre_abs_htl_nip, pre_htl_etl_nip), (post_abs_htl_nip, post_htl_etl_nip)\n)\n\n# Create 1x2 figure with just the alluvial plots (panels a, b)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.2, 4))\n\n# Draw alluvial diagrams (panels a, b)\ndraw_three_layer_alluvial(\n    ax1,\n    pre_abs_htl_nip,\n    pre_htl_etl_nip,\n    abs_colors,\n    htl_colors,\n    etl_colors,\n    title='Pre-2022',\n    panel_label='a',\n)\ndraw_three_layer_alluvial(\n    ax2,\n    post_abs_htl_nip,\n    post_htl_etl_nip,\n    abs_colors,\n    htl_colors,\n    etl_colors,\n    title='2022+',\n    panel_label='b',\n)\n\nfig.tight_layout()\nfig.savefig('perovskite_solar_cell_evolution_figure_nip.pdf')\n</pre> # Create consistent color mapping across both periods abs_colors, htl_colors, etl_colors = get_global_color_mapping(     (pre_abs_htl_nip, pre_htl_etl_nip), (post_abs_htl_nip, post_htl_etl_nip) )  # Create 1x2 figure with just the alluvial plots (panels a, b) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.2, 4))  # Draw alluvial diagrams (panels a, b) draw_three_layer_alluvial(     ax1,     pre_abs_htl_nip,     pre_htl_etl_nip,     abs_colors,     htl_colors,     etl_colors,     title='Pre-2022',     panel_label='a', ) draw_three_layer_alluvial(     ax2,     post_abs_htl_nip,     post_htl_etl_nip,     abs_colors,     htl_colors,     etl_colors,     title='2022+',     panel_label='b', )  fig.tight_layout() fig.savefig('perovskite_solar_cell_evolution_figure_nip.pdf')"},{"location":"notebooks/perla_notebooks/architecture-evolution.html#setup-and-data-loading","title":"Setup and Data Loading\u00b6","text":""},{"location":"notebooks/perla_notebooks/architecture-evolution.html#data-preparation-and-filtering","title":"Data Preparation and Filtering\u00b6","text":""},{"location":"notebooks/perla_notebooks/architecture-evolution.html#analysis-functions-and-material-flow-calculations","title":"Analysis Functions and Material Flow Calculations\u00b6","text":""},{"location":"notebooks/perla_notebooks/architecture-evolution.html#material-flow-analysis-for-p-i-n-architecture","title":"Material Flow Analysis for p-i-n Architecture\u00b6","text":"<p>The following alluvial diagrams visualize the material flows from absorber through hole transport layer (HTL) to electron transport layer (ETL) specifically for p-i-n architectures, comparing pre-2022 and post-2022 periods.</p>"},{"location":"notebooks/perla_notebooks/architecture-evolution.html#material-flow-analysis-for-n-i-p-architecture","title":"Material Flow Analysis for n-i-p Architecture\u00b6","text":"<p>The following alluvial diagrams show the evolution of material flows for n-i-p architectures only.</p>"},{"location":"notebooks/perla_notebooks/bandgap-evolution.html","title":"Bandgap Evolution Analysis","text":"In\u00a0[48]: \"hide-cell\" Copied! <pre># ruff: noqa: E402, F601\n</pre> # ruff: noqa: E402, F601           Temporal Evolution of Bandgap and Absorber Composition in Perovskite Solar Cells <p>     This notebook analyzes the temporal evolution of optical bandgap and absorber composition in perovskite solar cells using data from the Perovskite Database in NOMAD.   </p> <p> </p> In\u00a0[1]: Copied! <pre>from plotly_theme import register_template, set_defaults\n\nregister_template()\nset_defaults()\n</pre> from plotly_theme import register_template, set_defaults  register_template() set_defaults() In\u00a0[2]: Copied! <pre># Load the data from the parquet file into a DataFrame\nimport pandas as pd\n\ndf = pd.read_parquet(\"perovskite_solar_cell_database.parquet\")\n</pre> # Load the data from the parquet file into a DataFrame import pandas as pd  df = pd.read_parquet(\"perovskite_solar_cell_database.parquet\") In\u00a0[3]: Copied! <pre># Set the source_database column: if data.ref.person_entering_data is 'LLM Extraction', then 'LLM Extracted', else 'Manual Entry'\ndf['source_database'] = df['data.ref.name_of_person_entering_the_data'].apply(lambda x: 'LLM Extracted' if x == 'LLM Extraction' else 'Manual Entry')\n\n# Convert band gap from Joules to eV (1 eV = 1.60218e-19 J)\ndf[\"results.properties.electronic.band_gap.0.value\"] = df[\"results.properties.electronic.band_gap.0.value\"].apply(lambda x: x / 1.60218e-19 if pd.notnull(x) else x)\n</pre> # Set the source_database column: if data.ref.person_entering_data is 'LLM Extraction', then 'LLM Extracted', else 'Manual Entry' df['source_database'] = df['data.ref.name_of_person_entering_the_data'].apply(lambda x: 'LLM Extracted' if x == 'LLM Extraction' else 'Manual Entry')  # Convert band gap from Joules to eV (1 eV = 1.60218e-19 J) df[\"results.properties.electronic.band_gap.0.value\"] = df[\"results.properties.electronic.band_gap.0.value\"].apply(lambda x: x / 1.60218e-19 if pd.notnull(x) else x) In\u00a0[4]: Copied! <pre># Investigate the evolution of bandgaps over the years\n\nimport plotly.express as px\n\nfig = px.scatter(df,\n                 x=\"data.ref.publication_date\",\n                 y=\"results.properties.electronic.band_gap.0.value\",\n                 color=\"source_database\",\n                 labels={\"data.ref.publication_date\": \"Publication year\", \"results.properties.electronic.band_gap[0].value\": \"Bandgap (eV)\", \"source_database\": \"\"},\n                 opacity=0.5,)\nfig.update_layout(\n    yaxis_title=\"Bandgap (eV)\",\n    # Tight look &amp; feel\n    height=400, width=600,\n    bargap=0.0,\n    hovermode=\"closest\",\n    showlegend=False,\n    margin=dict(l=70, r=20, t=20, b=60)\n)\nfig.update_traces(mode='markers', marker_line_width=0.5, marker_size=7, marker_line_color='white')\nfig.show(renderer=\"notebook\")\n</pre> # Investigate the evolution of bandgaps over the years  import plotly.express as px  fig = px.scatter(df,                  x=\"data.ref.publication_date\",                  y=\"results.properties.electronic.band_gap.0.value\",                  color=\"source_database\",                  labels={\"data.ref.publication_date\": \"Publication year\", \"results.properties.electronic.band_gap[0].value\": \"Bandgap (eV)\", \"source_database\": \"\"},                  opacity=0.5,) fig.update_layout(     yaxis_title=\"Bandgap (eV)\",     # Tight look &amp; feel     height=400, width=600,     bargap=0.0,     hovermode=\"closest\",     showlegend=False,     margin=dict(l=70, r=20, t=20, b=60) ) fig.update_traces(mode='markers', marker_line_width=0.5, marker_size=7, marker_line_color='white') fig.show(renderer=\"notebook\") In\u00a0[5]: Copied! <pre>import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Filter to 1.2\u20132.2 eV range\ndf_bandgap = df[\n    (df[\"results.properties.electronic.band_gap.0.value\"] &gt;= 1.2) &amp;\n    (df[\"results.properties.electronic.band_gap.0.value\"] &lt;= 2.2)\n].copy()\n\n# Print the number of entries in df_bandgap\nprint(f\"Number of entries in df_bandgap: {len(df_bandgap)}\")\n\n# Filter from 2014 onwards\npub_date = pd.to_datetime(df_bandgap[\"data.ref.publication_date\"], errors=\"coerce\")\ndf_bandgap = df_bandgap[pub_date.dt.year &gt;= 2014]\n\n# Short aliases for columns\nxcol = \"data.ref.publication_date\"\nycol = \"results.properties.electronic.band_gap.0.value\"\n\n# If publication_date is datetime, Plotly handles it; if it's a string year, you can cast to int:\n# df_bandgap[xcol] = pd.to_datetime(df_bandgap[xcol], errors=\"coerce\")\n\nfig = make_subplots(\n    rows=2, cols=2,\n    column_widths=[0.85, 0.15],\n    row_heights=[0.15, 0.85],\n    specs=[[{\"type\": \"xy\"}, {\"type\": \"histogram\"}],\n           [{\"type\": \"xy\"}, {\"type\": \"histogram\"}]],\n    shared_xaxes=True,\n    shared_yaxes=True,\n    horizontal_spacing=0.02,\n    vertical_spacing=0.02\n)\n\n# Main filled 2D contour\nfig.add_trace(\n    go.Histogram2dContour(\n        x=df_bandgap[xcol],\n        y=df_bandgap[ycol],\n        contours_coloring=\"fill\",\n        colorscale=\"Blues_r\",\n        reversescale=True,\n        showscale=True,\n        ncontours=15,\n        zauto=False,   # Turn off auto range\n        zmin=1,        # Lower clamp\n        zmax=200,\n    ),\n    row=2, col=1\n)\n\n# Overlay scatter points\nfig.add_trace(\n    go.Scattergl(\n        x=df_bandgap[xcol],\n        y=df_bandgap[ycol],\n        mode=\"markers\",\n        marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),\n        hovertemplate=\"Year: %{x}&lt;br&gt;Bandgap: %{y:.3f} eV&lt;extra&gt;&lt;/extra&gt;\"\n    ),\n    row=2, col=1\n)\n\n# Top histogram (x / publication year)\nfig.add_trace(\n    go.Histogram(\n        x=df_bandgap[xcol],\n        nbinsx=40,\n        marker=dict(color=\"rgba(0,0,0,0.8)\"),\n        showlegend=False\n    ),\n    row=1, col=1\n)\n\n# Right histogram (y / bandgap)\nfig.add_trace(\n    go.Histogram(\n        y=df_bandgap[ycol],\n        nbinsy=40,\n        marker=dict(color=\"rgba(0,0,0,0.8)\"),\n        showlegend=False\n    ),\n    row=2, col=2\n)\n\n# Axes + layout\nfig.update_xaxes(\n    title_text=\"Publication year\",\n    row=2, col=1,\n    showgrid=False, zeroline=False,\n)\nfig.update_yaxes(\n    title_text=\"Bandgap / eV\",\n    row=2, col=1,\n    showgrid=False, zeroline=False,\n    range=[1.42, 1.72]\n)\n\n# Hide redundant axes labels on the marginal plots\nfig.update_xaxes(showticklabels=False, row=1, col=1)\nfig.update_yaxes(showticklabels=False, row=2, col=2)\n\n# Tight look &amp; feel\nfig.update_layout(\n    height=600, width=750,\n    bargap=0.0,\n    hovermode=\"closest\",\n    showlegend=False,\n    margin=dict(l=70, r=20, t=20, b=60)\n)\n\nfig.show(renderer=\"notebook\")\n</pre> import plotly.graph_objects as go from plotly.subplots import make_subplots  # Filter to 1.2\u20132.2 eV range df_bandgap = df[     (df[\"results.properties.electronic.band_gap.0.value\"] &gt;= 1.2) &amp;     (df[\"results.properties.electronic.band_gap.0.value\"] &lt;= 2.2) ].copy()  # Print the number of entries in df_bandgap print(f\"Number of entries in df_bandgap: {len(df_bandgap)}\")  # Filter from 2014 onwards pub_date = pd.to_datetime(df_bandgap[\"data.ref.publication_date\"], errors=\"coerce\") df_bandgap = df_bandgap[pub_date.dt.year &gt;= 2014]  # Short aliases for columns xcol = \"data.ref.publication_date\" ycol = \"results.properties.electronic.band_gap.0.value\"  # If publication_date is datetime, Plotly handles it; if it's a string year, you can cast to int: # df_bandgap[xcol] = pd.to_datetime(df_bandgap[xcol], errors=\"coerce\")  fig = make_subplots(     rows=2, cols=2,     column_widths=[0.85, 0.15],     row_heights=[0.15, 0.85],     specs=[[{\"type\": \"xy\"}, {\"type\": \"histogram\"}],            [{\"type\": \"xy\"}, {\"type\": \"histogram\"}]],     shared_xaxes=True,     shared_yaxes=True,     horizontal_spacing=0.02,     vertical_spacing=0.02 )  # Main filled 2D contour fig.add_trace(     go.Histogram2dContour(         x=df_bandgap[xcol],         y=df_bandgap[ycol],         contours_coloring=\"fill\",         colorscale=\"Blues_r\",         reversescale=True,         showscale=True,         ncontours=15,         zauto=False,   # Turn off auto range         zmin=1,        # Lower clamp         zmax=200,     ),     row=2, col=1 )  # Overlay scatter points fig.add_trace(     go.Scattergl(         x=df_bandgap[xcol],         y=df_bandgap[ycol],         mode=\"markers\",         marker=dict(color=\"rgba(0,0,0,0.05)\", size=4),         hovertemplate=\"Year: %{x}Bandgap: %{y:.3f} eV\"     ),     row=2, col=1 )  # Top histogram (x / publication year) fig.add_trace(     go.Histogram(         x=df_bandgap[xcol],         nbinsx=40,         marker=dict(color=\"rgba(0,0,0,0.8)\"),         showlegend=False     ),     row=1, col=1 )  # Right histogram (y / bandgap) fig.add_trace(     go.Histogram(         y=df_bandgap[ycol],         nbinsy=40,         marker=dict(color=\"rgba(0,0,0,0.8)\"),         showlegend=False     ),     row=2, col=2 )  # Axes + layout fig.update_xaxes(     title_text=\"Publication year\",     row=2, col=1,     showgrid=False, zeroline=False, ) fig.update_yaxes(     title_text=\"Bandgap / eV\",     row=2, col=1,     showgrid=False, zeroline=False,     range=[1.42, 1.72] )  # Hide redundant axes labels on the marginal plots fig.update_xaxes(showticklabels=False, row=1, col=1) fig.update_yaxes(showticklabels=False, row=2, col=2)  # Tight look &amp; feel fig.update_layout(     height=600, width=750,     bargap=0.0,     hovermode=\"closest\",     showlegend=False,     margin=dict(l=70, r=20, t=20, b=60) )  fig.show(renderer=\"notebook\") <pre>Number of entries in df_bandgap: 36788\n</pre> In\u00a0[6]: Copied! <pre># Plot composition vs bandgap using violin plots for the most common compositions\n\ndf_composition_bandgap = df.dropna(subset=['data.perovskite.composition_short_form', 'results.properties.electronic.band_gap.0.value'])\ndf_composition_bandgap = df_composition_bandgap[df_composition_bandgap['results.properties.electronic.band_gap.0.value'] &lt;= 3.0]\n\nimport plotly.express as px\n\n# Plot only data for MAPbI, FAPbI, CsFAPbI, CsMAFAPbI, FAMAPbI\nallowed_compositions = ['MAPbI', 'CsFAMAPbI', 'CsFAPbI',  'FAMAPbI', 'FAPbI',]\n\nfig = px.violin(df_composition_bandgap[df_composition_bandgap['data.perovskite.composition_short_form'].isin(allowed_compositions)],\n                 x=\"data.perovskite.composition_short_form\",\n                 y=\"results.properties.electronic.band_gap.0.value\",\n                 color=\"data.perovskite.composition_short_form\",\n                 box=True, points=\"all\",\n                 labels={\"data.perovskite.composition_short_form\": \"Perovskite composition\", \"results.properties.electronic.band_gap.0.value\": \"Bandgap / eV\", \"data.perovskite.composition_short_form\": \"\"},\n                 category_orders={\"data.perovskite.composition_short_form\": allowed_compositions}\n                )\nfig.update_traces(marker=dict(opacity=0.3, size=4))\nfig.update_layout(\n    yaxis_title=\"Bandgap / eV\",\n    height=400, width=700,\n    yaxis=dict(range=[1.4, 1.7]),\n    showlegend=False,\n)\nfig.show(renderer=\"notebook\")\n\n# Print the number of entries in df_composition_bandgap with allowed compositions by source and composition\n\nfor source in df_composition_bandgap['source_database'].unique():\n    df_source = df_composition_bandgap[df_composition_bandgap['source_database'] == source]\n    print(f\"Source: {source}\")\n    for composition in allowed_compositions:\n        count = len(df_source[df_source['data.perovskite.composition_short_form'] == composition])\n        print(f\"  Composition: {composition}, Count: {count}\")\n    total_count = len(df_source[df_source['data.perovskite.composition_short_form'].isin(allowed_compositions)])\n    print(f\"  Total entries with allowed compositions: {total_count}\\n\")\n\n# Print the total number of entries with allowed compositions regardless of source_database\n\nprint(f\"Total entries with allowed compositions regardless of source_database: {len(df_composition_bandgap[df_composition_bandgap['data.perovskite.composition_short_form'].isin(allowed_compositions)])}\")\n</pre> # Plot composition vs bandgap using violin plots for the most common compositions  df_composition_bandgap = df.dropna(subset=['data.perovskite.composition_short_form', 'results.properties.electronic.band_gap.0.value']) df_composition_bandgap = df_composition_bandgap[df_composition_bandgap['results.properties.electronic.band_gap.0.value'] &lt;= 3.0]  import plotly.express as px  # Plot only data for MAPbI, FAPbI, CsFAPbI, CsMAFAPbI, FAMAPbI allowed_compositions = ['MAPbI', 'CsFAMAPbI', 'CsFAPbI',  'FAMAPbI', 'FAPbI',]  fig = px.violin(df_composition_bandgap[df_composition_bandgap['data.perovskite.composition_short_form'].isin(allowed_compositions)],                  x=\"data.perovskite.composition_short_form\",                  y=\"results.properties.electronic.band_gap.0.value\",                  color=\"data.perovskite.composition_short_form\",                  box=True, points=\"all\",                  labels={\"data.perovskite.composition_short_form\": \"Perovskite composition\", \"results.properties.electronic.band_gap.0.value\": \"Bandgap / eV\", \"data.perovskite.composition_short_form\": \"\"},                  category_orders={\"data.perovskite.composition_short_form\": allowed_compositions}                 ) fig.update_traces(marker=dict(opacity=0.3, size=4)) fig.update_layout(     yaxis_title=\"Bandgap / eV\",     height=400, width=700,     yaxis=dict(range=[1.4, 1.7]),     showlegend=False, ) fig.show(renderer=\"notebook\")  # Print the number of entries in df_composition_bandgap with allowed compositions by source and composition  for source in df_composition_bandgap['source_database'].unique():     df_source = df_composition_bandgap[df_composition_bandgap['source_database'] == source]     print(f\"Source: {source}\")     for composition in allowed_compositions:         count = len(df_source[df_source['data.perovskite.composition_short_form'] == composition])         print(f\"  Composition: {composition}, Count: {count}\")     total_count = len(df_source[df_source['data.perovskite.composition_short_form'].isin(allowed_compositions)])     print(f\"  Total entries with allowed compositions: {total_count}\\n\")  # Print the total number of entries with allowed compositions regardless of source_database  print(f\"Total entries with allowed compositions regardless of source_database: {len(df_composition_bandgap[df_composition_bandgap['data.perovskite.composition_short_form'].isin(allowed_compositions)])}\") <pre>Source: Manual Entry\n  Composition: MAPbI, Count: 26342\n  Composition: CsFAMAPbI, Count: 36\n  Composition: CsFAPbI, Count: 111\n  Composition: FAMAPbI, Count: 260\n  Composition: FAPbI, Count: 565\n  Total entries with allowed compositions: 27314\n\nSource: LLM Extracted\n  Composition: MAPbI, Count: 933\n  Composition: CsFAMAPbI, Count: 102\n  Composition: CsFAPbI, Count: 253\n  Composition: FAMAPbI, Count: 184\n  Composition: FAPbI, Count: 587\n  Total entries with allowed compositions: 2059\n\nTotal entries with allowed compositions regardless of source_database: 29373\n</pre> In\u00a0[7]: Copied! <pre>import pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# -----------------------------\n# Data preparation\n# -----------------------------\ndf = df_composition_bandgap.copy()\n\ndf[\"publication_year\"] = pd.to_datetime(\n    df[\"data.ref.publication_date\"],\n    errors=\"coerce\"\n).dt.year\ndf = df.dropna(subset=[\"publication_year\"])\n\ndf_early = df[df[\"publication_year\"] &lt;= 2021]\ndf_late  = df[df[\"publication_year\"] &gt;= 2022]\n\nallowed = [\"MAPbI\", \"CsFAMAPbI\", \"CsFAPbI\", \"FAMAPbI\", \"FAPbI\"]\ncol = \"data.perovskite.composition_short_form\"\ndf_early = df_early[df_early[col].isin(allowed)]\ndf_late  = df_late[df_late[col].isin(allowed)]\n\n# -----------------------------\n# Helpers\n# -----------------------------\ndef counts_with_other(series, min_pct_for_other=None):\n    counts = series.value_counts()\n    total = int(counts.sum())\n    if total == 0:\n        return pd.Series(dtype=int), 0\n\n    if min_pct_for_other is not None:\n        keep = counts[(counts / total * 100) &gt;= min_pct_for_other]\n        drop = counts[(counts / total * 100) &lt;  min_pct_for_other]\n        if len(drop) &gt; 0:\n            counts = pd.concat([keep, pd.Series({\"Other\": int(drop.sum())})])\n            counts = counts.reindex([*keep.index.tolist(), \"Other\"])\n    return counts.astype(int), total\n\ndef prep_text(counts, total, inside_pct=10.0, force_inside_label=None):\n    labels = counts.index.tolist()\n    values = counts.values.astype(int).tolist()\n    pcts = [v * 100.0 / total for v in values]\n\n    text, textpos, pull = [], [], []\n    for lab, v, p in zip(labels, values, pcts):\n        if force_inside_label is not None and lab == force_inside_label:\n            text.append(f\"{lab}&lt;br&gt; ({p:.1f}%)\")\n            textpos.append(\"inside\")\n        elif p &gt;= inside_pct:\n            text.append(f\"{lab}&lt;br&gt; ({p:.1f}%)\")\n            textpos.append(\"inside\")\n        else:\n            text.append(f\"{lab} ({p:.1f}%)\")\n            textpos.append(\"outside\")\n\n        pull.append(0.03 if p &lt; 7 else 0.0)\n\n    return labels, values, text, textpos, pull\n\n# Left side: group tiny categories into Other and DO NOT rely on inside text\ncounts1, total1 = counts_with_other(df_early[col], min_pct_for_other=1.0)\nl1, v1, t1, tp1, pull1 = prep_text(counts1, total1, inside_pct=10.0)\n\n# Right side: normal behavior is fine\ncounts2, total2 = counts_with_other(df_late[col], min_pct_for_other=None)\nl2, v2, t2, tp2, pull2 = prep_text(counts2, total2, inside_pct=10.0)\n\n# Compute the MAPbI share on the left for a manual annotation (guaranteed visible)\nmapbi_left = int(counts1.get(\"MAPbI\", 0))\nmapbi_left_pct = (mapbi_left / total1 * 100.0) if total1 else 0.0\n\n# -----------------------------\n# Figure\n# -----------------------------\nfig = make_subplots(\n    rows=1, cols=2,\n    specs=[[{\"type\": \"domain\"}, {\"type\": \"domain\"}]],\n    horizontal_spacing=0.20\n)\n\n# Left donut\nfig.add_trace(\n    go.Pie(\n        labels=l1, values=v1, hole=0.33,\n        text=t1, textinfo=\"text\", textposition=tp1, pull=pull1,\n        sort=False, direction=\"clockwise\",\n        domain=dict(x=[0.00, 0.44]),\n        automargin=True,\n        hovertemplate=\"%{label}&lt;br&gt;%{value} (%{percent})&lt;extra&gt;&lt;/extra&gt;\",\n    ),\n    1, 1\n)\n\n# Right donut\nfig.add_trace(\n    go.Pie(\n        labels=l2, values=v2, hole=0.33,\n        text=t2, textinfo=\"text\", textposition=tp2, pull=pull2,\n        sort=False, direction=\"clockwise\",\n        domain=dict(x=[0.56, 1.00]),\n        automargin=True,\n        hovertemplate=\"%{label}&lt;br&gt;%{value} (%{percent})&lt;extra&gt;&lt;/extra&gt;\",\n    ),\n    1, 2\n)\n\n# -----------------------------\n# Titles (panel labels)\n# -----------------------------\nfig.add_annotation(\n    text=\"&lt;b&gt;Until 2021&lt;/b&gt;\",\n    x=0.22, y=1.08, xref=\"paper\", yref=\"paper\",\n    showarrow=False, font=dict(size=26)\n)\nfig.add_annotation(\n    text=\"&lt;b&gt;From 2022&lt;/b&gt;\",\n    x=1.02, y=1.08, xref=\"paper\", yref=\"paper\",\n    showarrow=False, font=dict(size=26)\n)\n\n\n# -----------------------------\n# Styling for publication\n# -----------------------------\nfig.update_layout(\n    height=400,\n    width=700,\n    showlegend=False,\n    margin=dict(l=30, r=30, t=70, b=30),\n    font=dict(size=18),\n    # Keep overlap protection for outside labels; MAPbI is now manual so it won't disappear\n    uniformtext=dict(\n        minsize=14,\n        #mode=\"hide\"\n        ),\n)\n\nfig.update_traces(\n    marker_line_width=1.2,\n    marker_line_color=\"white\",\n    outsidetextfont=dict(size=18),\n    insidetextfont=dict(size=28),\n    insidetextorientation=\"radial\",\n)\n\nfig.show(renderer=\"notebook\")\n\n# print the number of entries in each split df_early and df_late\n\nprint(f\"Number of entries in df_early: {len(df_early)}\")\nprint(f\"Number of entries in df_late: {len(df_late)}\")\n</pre> import pandas as pd import plotly.graph_objects as go from plotly.subplots import make_subplots  # ----------------------------- # Data preparation # ----------------------------- df = df_composition_bandgap.copy()  df[\"publication_year\"] = pd.to_datetime(     df[\"data.ref.publication_date\"],     errors=\"coerce\" ).dt.year df = df.dropna(subset=[\"publication_year\"])  df_early = df[df[\"publication_year\"] &lt;= 2021] df_late  = df[df[\"publication_year\"] &gt;= 2022]  allowed = [\"MAPbI\", \"CsFAMAPbI\", \"CsFAPbI\", \"FAMAPbI\", \"FAPbI\"] col = \"data.perovskite.composition_short_form\" df_early = df_early[df_early[col].isin(allowed)] df_late  = df_late[df_late[col].isin(allowed)]  # ----------------------------- # Helpers # ----------------------------- def counts_with_other(series, min_pct_for_other=None):     counts = series.value_counts()     total = int(counts.sum())     if total == 0:         return pd.Series(dtype=int), 0      if min_pct_for_other is not None:         keep = counts[(counts / total * 100) &gt;= min_pct_for_other]         drop = counts[(counts / total * 100) &lt;  min_pct_for_other]         if len(drop) &gt; 0:             counts = pd.concat([keep, pd.Series({\"Other\": int(drop.sum())})])             counts = counts.reindex([*keep.index.tolist(), \"Other\"])     return counts.astype(int), total  def prep_text(counts, total, inside_pct=10.0, force_inside_label=None):     labels = counts.index.tolist()     values = counts.values.astype(int).tolist()     pcts = [v * 100.0 / total for v in values]      text, textpos, pull = [], [], []     for lab, v, p in zip(labels, values, pcts):         if force_inside_label is not None and lab == force_inside_label:             text.append(f\"{lab} ({p:.1f}%)\")             textpos.append(\"inside\")         elif p &gt;= inside_pct:             text.append(f\"{lab} ({p:.1f}%)\")             textpos.append(\"inside\")         else:             text.append(f\"{lab} ({p:.1f}%)\")             textpos.append(\"outside\")          pull.append(0.03 if p &lt; 7 else 0.0)      return labels, values, text, textpos, pull  # Left side: group tiny categories into Other and DO NOT rely on inside text counts1, total1 = counts_with_other(df_early[col], min_pct_for_other=1.0) l1, v1, t1, tp1, pull1 = prep_text(counts1, total1, inside_pct=10.0)  # Right side: normal behavior is fine counts2, total2 = counts_with_other(df_late[col], min_pct_for_other=None) l2, v2, t2, tp2, pull2 = prep_text(counts2, total2, inside_pct=10.0)  # Compute the MAPbI share on the left for a manual annotation (guaranteed visible) mapbi_left = int(counts1.get(\"MAPbI\", 0)) mapbi_left_pct = (mapbi_left / total1 * 100.0) if total1 else 0.0  # ----------------------------- # Figure # ----------------------------- fig = make_subplots(     rows=1, cols=2,     specs=[[{\"type\": \"domain\"}, {\"type\": \"domain\"}]],     horizontal_spacing=0.20 )  # Left donut fig.add_trace(     go.Pie(         labels=l1, values=v1, hole=0.33,         text=t1, textinfo=\"text\", textposition=tp1, pull=pull1,         sort=False, direction=\"clockwise\",         domain=dict(x=[0.00, 0.44]),         automargin=True,         hovertemplate=\"%{label}%{value} (%{percent})\",     ),     1, 1 )  # Right donut fig.add_trace(     go.Pie(         labels=l2, values=v2, hole=0.33,         text=t2, textinfo=\"text\", textposition=tp2, pull=pull2,         sort=False, direction=\"clockwise\",         domain=dict(x=[0.56, 1.00]),         automargin=True,         hovertemplate=\"%{label}%{value} (%{percent})\",     ),     1, 2 )  # ----------------------------- # Titles (panel labels) # ----------------------------- fig.add_annotation(     text=\"Until 2021\",     x=0.22, y=1.08, xref=\"paper\", yref=\"paper\",     showarrow=False, font=dict(size=26) ) fig.add_annotation(     text=\"From 2022\",     x=1.02, y=1.08, xref=\"paper\", yref=\"paper\",     showarrow=False, font=dict(size=26) )   # ----------------------------- # Styling for publication # ----------------------------- fig.update_layout(     height=400,     width=700,     showlegend=False,     margin=dict(l=30, r=30, t=70, b=30),     font=dict(size=18),     # Keep overlap protection for outside labels; MAPbI is now manual so it won't disappear     uniformtext=dict(         minsize=14,         #mode=\"hide\"         ), )  fig.update_traces(     marker_line_width=1.2,     marker_line_color=\"white\",     outsidetextfont=dict(size=18),     insidetextfont=dict(size=28),     insidetextorientation=\"radial\", )  fig.show(renderer=\"notebook\")  # print the number of entries in each split df_early and df_late  print(f\"Number of entries in df_early: {len(df_early)}\") print(f\"Number of entries in df_late: {len(df_late)}\") <pre>Number of entries in df_early: 27593\nNumber of entries in df_late: 1777\n</pre>"},{"location":"notebooks/perla_notebooks/bandgap-evolution.html#setup-and-data-loading","title":"Setup and Data Loading\u00b6","text":""},{"location":"notebooks/perla_notebooks/bandgap-evolution.html#bandgap-evolution-over-time","title":"Bandgap Evolution Over Time\u00b6","text":"<p>The reported bandgap values have evolved systematically over publication years. The link below opens a filtered view in the NOMAD dashboard. You can use the scatter plot widget to view the data from the figure directly in NOMAD. Below is a YAML snippet that you can use to reproduce a helpful dashboard to explore the influence of different cation compositions.</p> Dashboard YAML <pre>- type: scatter_plot\n  autorange: true\n  size: 10000\n  markers:\n    color:\n      search_quantity: data.perovskite.composition_a_ions#perovskite_solar_cell_database.schema.PerovskiteSolarCell\n  y:\n    search_quantity: results.properties.electronic.band_gap[0].value\n    title: Bandgap\n  x:\n    search_quantity: data.ref.publication_date#perovskite_solar_cell_database.schema.PerovskiteSolarCell\n  layout:\n    xxl:\n      minH: 3\n      minW: 3\n      h: 6\n      w: 9\n      y: 0\n      x: Infinity\n    xl:\n      minH: 3\n      minW: 3\n      h: 6\n      w: 9\n      y: 0\n      x: 0\n    lg:\n      minH: 3\n      minW: 3\n      h: 6\n      w: 9\n      y: 0\n      x: 0\n    md:\n      minH: 3\n      minW: 3\n      h: 6\n      w: 12\n      y: 0\n      x: 0\n    sm:\n      minH: 3\n      minW: 3\n      h: 6\n      w: 9\n      y: 0\n      x: 0\n- type: terms\n  show_input: false\n  scale: linear\n  search_quantity: data.perovskite.composition_a_ions#perovskite_solar_cell_database.schema.PerovskiteSolarCell\n  layout:\n    xxl:\n      minH: 3\n      minW: 3\n      h: 9\n      w: 6\n      y: 0\n      x: Infinity\n    xl:\n      minH: 3\n      minW: 3\n      h: 9\n      w: 6\n      y: 0\n      x: 9\n    lg:\n      minH: 3\n      minW: 3\n      h: 9\n      w: 6\n      y: 0\n      x: 9\n    md:\n      minH: 3\n      minW: 3\n      h: 6\n      w: 5\n      y: 0\n      x: 12\n    sm:\n      minH: 3\n      minW: 3\n      h: 9\n      w: 6\n      y: 6\n      x: 0\n</pre>"},{"location":"notebooks/perla_notebooks/bandgap-evolution.html#density-plot-analysis","title":"Density Plot Analysis\u00b6","text":"<p>The following contour plot with marginal histograms provides a detailed view of bandgap distribution evolution over time.</p>"},{"location":"notebooks/perla_notebooks/bandgap-evolution.html#compositional-analysis","title":"Compositional Analysis\u00b6","text":"<p>The density plot reveals a clear trend toward bandgap values of approximately 1.55 eV since 2021. To understand the underlying compositional changes, we analyze the most common pure iodide perovskite compositions: MAPbI, FAPbI, CsFAPbI, CsMAFAPbI, and FAMAPbI.</p>"},{"location":"notebooks/perla_notebooks/bandgap-evolution.html#temporal-compositional-shifts","title":"Temporal Compositional Shifts\u00b6","text":"<p>The median bandgaps for CsFAPbI, CsMAFAPbI, and FAMAPbI are centered around 1.55 eV. To quantify the temporal evolution of compositional preferences, we compare the distribution of perovskite compositions before and after 2022, when comprehensive database integration expanded significantly.</p>"},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html","title":"CrabNet Bandgap Prediction","text":"Compositional Based Property Prediction of Perovskites with CrabNet <p>     This notebook demonstrates how to use CrabNet to predict the bandgap of perovskites using data from the Perovskite Database in NOMAD for training. The method can be extended to predict other properties of perovskites and can also be combined with other methodologies available in the community for this task. To explore more of these methodologies, we recommend taking a look at MatBench, a benchmarking suite for these tasks in the materials informatics community. </p> <p> </p> In\u00a0[\u00a0]: Copied! <pre>! pip install torch\n! pip install crabnet\n! pip install pandas\n</pre> ! pip install torch ! pip install crabnet ! pip install pandas In\u00a0[11]: Copied! <pre>from time import monotonic\n\nimport jmespath\nimport requests\n\nbase_url = 'https://nomad-lab.eu/prod/v1/api/v1'\nbandgaps = []\nreduced_formulas = []\ndescriptive_formulas = []\npage_after_value = None\n\n\ndef extract_values(entry):\n    bandgaps.append(\n        jmespath.search(\n            'results.properties.electronic.band_structure_electronic[0].band_gap[0].value',\n            entry,\n        )\n    )\n    reduced_formulas.append(\n        jmespath.search('results.material.chemical_formula_reduced', entry)\n    )\n    descriptive_formulas.append(\n        jmespath.search('results.material.chemical_formula_descriptive', entry)\n    )\n\n\nstart = monotonic()\nwhile True:\n    response = requests.post(\n        f'{base_url}/entries/query',\n        json={\n            'owner': 'visible',\n            'query': {\n                'and': [\n                    # {\"results.material.elements:all\": [\"Sn\"]},\n                    {'sections:all': ['nomad.datamodel.results.SolarCell']}\n                ]\n            },\n            'pagination': {'page_size': 1000, 'page_after_value': page_after_value},\n        },\n    )\n    response_code = response.status_code\n    data = response.json()\n    pagination = data['pagination']\n    if page_after_value is None:\n        print(f'Total number of entries: {pagination[\"total\"]}')\n    print(response_code)\n    page_after_value = data['pagination'].get('next_page_after_value')\n\n    for entry in data['data']:\n        extract_values(entry)\n    if not page_after_value:\n        break\n    end = monotonic()\n\nprint(f'Query took {end - start:.2f} seconds')\n</pre> from time import monotonic  import jmespath import requests  base_url = 'https://nomad-lab.eu/prod/v1/api/v1' bandgaps = [] reduced_formulas = [] descriptive_formulas = [] page_after_value = None   def extract_values(entry):     bandgaps.append(         jmespath.search(             'results.properties.electronic.band_structure_electronic[0].band_gap[0].value',             entry,         )     )     reduced_formulas.append(         jmespath.search('results.material.chemical_formula_reduced', entry)     )     descriptive_formulas.append(         jmespath.search('results.material.chemical_formula_descriptive', entry)     )   start = monotonic() while True:     response = requests.post(         f'{base_url}/entries/query',         json={             'owner': 'visible',             'query': {                 'and': [                     # {\"results.material.elements:all\": [\"Sn\"]},                     {'sections:all': ['nomad.datamodel.results.SolarCell']}                 ]             },             'pagination': {'page_size': 1000, 'page_after_value': page_after_value},         },     )     response_code = response.status_code     data = response.json()     pagination = data['pagination']     if page_after_value is None:         print(f'Total number of entries: {pagination[\"total\"]}')     print(response_code)     page_after_value = data['pagination'].get('next_page_after_value')      for entry in data['data']:         extract_values(entry)     if not page_after_value:         break     end = monotonic()  print(f'Query took {end - start:.2f} seconds') <pre>Total number of entries: 43108\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\n200\nQuery took 1230.80 seconds\n</pre> <p>You can verify that the data was fetched correctly by checking the length of the lists, e.g., bandgap</p> In\u00a0[13]: Copied! <pre>import os\n\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        'reduced_formulas': reduced_formulas,\n        'descriptive_formulas': descriptive_formulas,\n        'bandgap': bandgaps,\n    }\n)\n\ndf['bandgap'] = pd.to_numeric(df['bandgap'], errors='coerce')\ndf['bandgap'] = df['bandgap'] * 6.24150974e18\ndf.head()\n\nif not os.path.exists('data'):\n    os.makedirs('data')\ndf.to_csv('data/perovskite_bandgap_devices.csv', index=False)\n</pre> import os  import pandas as pd  df = pd.DataFrame(     {         'reduced_formulas': reduced_formulas,         'descriptive_formulas': descriptive_formulas,         'bandgap': bandgaps,     } )  df['bandgap'] = pd.to_numeric(df['bandgap'], errors='coerce') df['bandgap'] = df['bandgap'] * 6.24150974e18 df.head()  if not os.path.exists('data'):     os.makedirs('data') df.to_csv('data/perovskite_bandgap_devices.csv', index=False) In\u00a0[14]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv('data/perovskite_bandgap_devices.csv')\ndf.head()\n</pre> import pandas as pd  df = pd.read_csv('data/perovskite_bandgap_devices.csv') df.head() Out[14]: reduced_formulas descriptive_formulas bandgap 0 CH6I3NPb MAPbI3 1.6 1 CH6I3NPb MAPbI3 NaN 2 CH6I3NPb MAPbI3 1.5 3 CH6I3NPb MAPbI3 1.6 4 CH6I3NPb MAPbI3 1.6 In\u00a0[\u00a0]: Copied! <pre>df.describe()\n</pre> df.describe() <p>We can see that the dataset contains a diverse range of bandgap values (1.16 to 3.05 eV). Some parameters such as open circuit voltage (voc) and fill factor (ff), include extreme or zero values, which might need attention for data cleaning.</p> <p>The dataset might include missing values, let's check if there are any:</p> In\u00a0[\u00a0]: Copied! <pre>df.isna().sum()\n</pre> df.isna().sum() <p>Let's now keep only the part of the dataframe, where the value for bandgap is not Na.</p> In\u00a0[\u00a0]: Copied! <pre>df = df[df['bandgap'].notna()]\ndf = df[df['reduced_formulas'].notna()]\n</pre> df = df[df['bandgap'].notna()] df = df[df['reduced_formulas'].notna()] In\u00a0[\u00a0]: Copied! <pre>df.isna().sum()\n</pre> df.isna().sum() <p>Even before checking the dataset, we can already guess that are many repeated formulas and bandgap values in the DataFrame. But let's verify it:</p> In\u00a0[\u00a0]: Copied! <pre>df['reduced_formulas'].value_counts().head(10)\n</pre> df['reduced_formulas'].value_counts().head(10) <p>The dataset contains many repeated formulas and corresponding bandgap values. To make sure each formula is unique, we will use the <code>groupby_formula</code> function from CrabNet to group entries by their formulas and use the mean values for their bandgap. We make a new final DataFrame for our model, call it df_reduced_formula and adjust colum names etc.</p> <p>Let's rename columns and use the <code>gourpby_formula</code> function from CrabNet:</p> In\u00a0[\u00a0]: Copied! <pre>from crabnet.utils.data import groupby_formula  # type: ignore\n\n# Rename the column 'bandgap' to 'target', and 'reduced_formula' to 'formula'\ndf.rename(columns={'bandgap': 'target'}, inplace=True)\ndf.rename(columns={'reduced_formulas': 'formula'}, inplace=True)\n\n# Group repeated formulas and take the mean of the target\ndf_grouped_formula = groupby_formula(df, how='mean')\ndf_grouped_formula.head()\n</pre> from crabnet.utils.data import groupby_formula  # type: ignore  # Rename the column 'bandgap' to 'target', and 'reduced_formula' to 'formula' df.rename(columns={'bandgap': 'target'}, inplace=True) df.rename(columns={'reduced_formulas': 'formula'}, inplace=True)  # Group repeated formulas and take the mean of the target df_grouped_formula = groupby_formula(df, how='mean') df_grouped_formula.head() <p>Let's check the shape of the DataFrame we would like to use for our ML:</p> In\u00a0[\u00a0]: Copied! <pre>df_grouped_formula.shape\n</pre> df_grouped_formula.shape In\u00a0[\u00a0]: Copied! <pre>! pip install pymatviz\n</pre> ! pip install pymatviz <p>in case of problems, try it from its developer repo:</p> <p>! pip install git+https://github.com/janosh/pymatviz</p> <p>Once installed, we import and use it:</p> In\u00a0[\u00a0]: Copied! <pre>from pymatviz import ptable_heatmap_plotly  # type: ignore\n\nptable_heatmap_plotly(\n    df_grouped_formula['formula'],\n    log=True,\n    colorscale='BuPu',\n    font_colors='black',\n    fmt='.3g',\n    colorbar=dict(orientation='v', title='Element Prevalence'),\n)\n</pre> from pymatviz import ptable_heatmap_plotly  # type: ignore  ptable_heatmap_plotly(     df_grouped_formula['formula'],     log=True,     colorscale='BuPu',     font_colors='black',     fmt='.3g',     colorbar=dict(orientation='v', title='Element Prevalence'), ) <p>As you can see the current data is heavily based on hybrid halide perovskites, so we expect the model to perform better when predicting these materials. Let's continue and build the model in the next section!</p> <p>We randomize the dataset and split it into training, validation, and test sets in a ratio of 80%, 10%, and 10%, respectively.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\ntrain_df, val_df, test_df = np.split(\n    df_grouped_formula.sample(frac=1, random_state=42),\n    [int(0.8 * len(df_grouped_formula)), int(0.9 * len(df_grouped_formula))],\n)\n</pre> import numpy as np  train_df, val_df, test_df = np.split(     df_grouped_formula.sample(frac=1, random_state=42),     [int(0.8 * len(df_grouped_formula)), int(0.9 * len(df_grouped_formula))], ) <p>We then fit the model using the CrabNet implementation.</p> In\u00a0[\u00a0]: Copied! <pre>from crabnet.crabnet_ import CrabNet  # type: ignore\n\ncrabnet_bandgap = CrabNet(\n    mat_prop='bandgap',\n    model_name='perovskite_bg_prediction',\n    elem_prop='mat2vec',\n    learningcurve=True,\n)\ncrabnet_bandgap.fit(train_df, val_df)\n</pre> from crabnet.crabnet_ import CrabNet  # type: ignore  crabnet_bandgap = CrabNet(     mat_prop='bandgap',     model_name='perovskite_bg_prediction',     elem_prop='mat2vec',     learningcurve=True, ) crabnet_bandgap.fit(train_df, val_df) <p>After training, we evaluate the model using the validation data.</p> In\u00a0[\u00a0]: Copied! <pre>from crabnet.utils.figures import act_pred  # type: ignore\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Train data\ntrain_df_zeros = pd.DataFrame(\n    {'formula': train_df['formula'], 'target': [0.0] * len(train_df['formula'])}\n)\ntrain_df_predicted, train_df_predicted_sigma = crabnet_bandgap.predict(\n    train_df_zeros, return_uncertainty=True\n)\n\nact_pred(train_df['target'], train_df_predicted)\nr2 = r2_score(train_df['target'], train_df_predicted)\nprint(f'R2 score: {r2}')\nmse = mean_squared_error(train_df['target'], train_df_predicted)\nprint(f'MSE: {mse}')\nmae = mean_absolute_error(train_df['target'], train_df_predicted)\nprint(f'MAE: {mae} eV')\n</pre> from crabnet.utils.figures import act_pred  # type: ignore from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # Train data train_df_zeros = pd.DataFrame(     {'formula': train_df['formula'], 'target': [0.0] * len(train_df['formula'])} ) train_df_predicted, train_df_predicted_sigma = crabnet_bandgap.predict(     train_df_zeros, return_uncertainty=True )  act_pred(train_df['target'], train_df_predicted) r2 = r2_score(train_df['target'], train_df_predicted) print(f'R2 score: {r2}') mse = mean_squared_error(train_df['target'], train_df_predicted) print(f'MSE: {mse}') mae = mean_absolute_error(train_df['target'], train_df_predicted) print(f'MAE: {mae} eV') <p>for validation data we have:</p> In\u00a0[\u00a0]: Copied! <pre># Validation data\nval_df_zeros = pd.DataFrame(\n    {'formula': val_df['formula'], 'target': [0.0] * len(val_df['formula'])}\n)\nval_df_predicted, val_df_predicted_sigma = crabnet_bandgap.predict(\n    val_df_zeros, return_uncertainty=True\n)\n\nact_pred(val_df['target'], val_df_predicted)\nr2 = r2_score(val_df['target'], val_df_predicted)\nprint(f'R2 score: {r2}')\nmse = mean_squared_error(val_df['target'], val_df_predicted)\nprint(f'MSE: {mse}')\nmae = mean_absolute_error(val_df['target'], val_df_predicted)\nprint(f'MAE: {mae} eV')\n</pre> # Validation data val_df_zeros = pd.DataFrame(     {'formula': val_df['formula'], 'target': [0.0] * len(val_df['formula'])} ) val_df_predicted, val_df_predicted_sigma = crabnet_bandgap.predict(     val_df_zeros, return_uncertainty=True )  act_pred(val_df['target'], val_df_predicted) r2 = r2_score(val_df['target'], val_df_predicted) print(f'R2 score: {r2}') mse = mean_squared_error(val_df['target'], val_df_predicted) print(f'MSE: {mse}') mae = mean_absolute_error(val_df['target'], val_df_predicted) print(f'MAE: {mae} eV') <p>and finally for test data:</p> In\u00a0[\u00a0]: Copied! <pre># Test data\n\ntest_df_zeros = pd.DataFrame(\n    {'formula': test_df['formula'], 'target': [0.0] * len(test_df['formula'])}\n)\ntest_df_predicted, test_df_predicted_sigma = crabnet_bandgap.predict(\n    test_df_zeros, return_uncertainty=True\n)\n\nact_pred(test_df['target'], test_df_predicted)\n\nr2 = r2_score(test_df['target'], test_df_predicted)\nprint(f'R2 score: {r2}')\nmse = mean_squared_error(test_df['target'], test_df_predicted)\nprint(f'MSE: {mse}')\nmae = mean_absolute_error(test_df['target'], test_df_predicted)\nprint(f'MAE: {mae} eV')\n</pre> # Test data  test_df_zeros = pd.DataFrame(     {'formula': test_df['formula'], 'target': [0.0] * len(test_df['formula'])} ) test_df_predicted, test_df_predicted_sigma = crabnet_bandgap.predict(     test_df_zeros, return_uncertainty=True )  act_pred(test_df['target'], test_df_predicted)  r2 = r2_score(test_df['target'], test_df_predicted) print(f'R2 score: {r2}') mse = mean_squared_error(test_df['target'], test_df_predicted) print(f'MSE: {mse}') mae = mean_absolute_error(test_df['target'], test_df_predicted) print(f'MAE: {mae} eV') In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd  # only if you jump to this cell directly\nfrom crabnet.crabnet_ import CrabNet  # type: ignore\nfrom crabnet.kingcrab import SubCrab  # type: ignore\n\n# Instantiate SubCrab\nsub_crab_model = SubCrab()\n\n# Instantiate CrabNet and set its model to SubCrab\ncrabnet_model = CrabNet()\ncrabnet_model.model = sub_crab_model\n\n# Load the pre-trained network\nfile_path = r'perovskite_bg_prediction.pth'\ncrabnet_model.load_network(file_path)\n</pre> import numpy as np import pandas as pd  # only if you jump to this cell directly from crabnet.crabnet_ import CrabNet  # type: ignore from crabnet.kingcrab import SubCrab  # type: ignore  # Instantiate SubCrab sub_crab_model = SubCrab()  # Instantiate CrabNet and set its model to SubCrab crabnet_model = CrabNet() crabnet_model.model = sub_crab_model  # Load the pre-trained network file_path = r'perovskite_bg_prediction.pth' crabnet_model.load_network(file_path) <p>Then define a function and run it for predicting the bandgap from individual formulas:</p> In\u00a0[\u00a0]: Copied! <pre># Function to predict the bandgap of a given formula\ndef predict_bandgap(formula):\n    input_df = pd.DataFrame({'formula': [formula], 'target': [0.0]})\n    prediction, prediction_sigma = crabnet_bandgap.predict(\n        input_df, return_uncertainty=True\n    )\n    return prediction, prediction_sigma\n\n\n# Main script to take user input and display predictions\nwhile True:\n    formula = input(\n        \"Enter a formula (e.g., CsPbBr3, CH3NH3PbI3) or type 'exit' to quit: \"\n    )\n    if formula.lower() == 'exit':\n        print('Exiting prediction tool. Goodbye!')\n        break\n    try:\n        prediction, prediction_sigma = predict_bandgap(formula)\n        print(\n            f'Predicted bandgap: {np.round(prediction[0], 3)} +/- {np.round(prediction_sigma[0], 3)} eV'\n        )\n    except Exception as e:\n        print(f'Error during prediction: {e}')\n</pre> # Function to predict the bandgap of a given formula def predict_bandgap(formula):     input_df = pd.DataFrame({'formula': [formula], 'target': [0.0]})     prediction, prediction_sigma = crabnet_bandgap.predict(         input_df, return_uncertainty=True     )     return prediction, prediction_sigma   # Main script to take user input and display predictions while True:     formula = input(         \"Enter a formula (e.g., CsPbBr3, CH3NH3PbI3) or type 'exit' to quit: \"     )     if formula.lower() == 'exit':         print('Exiting prediction tool. Goodbye!')         break     try:         prediction, prediction_sigma = predict_bandgap(formula)         print(             f'Predicted bandgap: {np.round(prediction[0], 3)} +/- {np.round(prediction_sigma[0], 3)} eV'         )     except Exception as e:         print(f'Error during prediction: {e}') <p>Alternatively, this interactive widget allows you to input a chemical formula, predict its bandgap using the trained model, and check if the formula exists in the dataset. If it does, the widget displays the average bandgap value used during training.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom IPython.display import display\nfrom ipywidgets import Button, HBox, Output, Text, VBox\n\n\n# Function to predict the bandgap of a given formula\ndef predict_bandgap(formula):\n    val_df = pd.DataFrame({'formula': [formula], 'target': [0.0]})\n    prediction, prediction_sigma = crabnet_bandgap.predict(\n        val_df, return_uncertainty=True\n    )\n    return prediction, prediction_sigma\n\n\n# Function to check if the formula exists in the dataset\ndef check_formula_in_dataset(formula):\n    if formula in df_grouped_formula['formula'].values:\n        avg_bandgap = df_grouped_formula.loc[\n            df_grouped_formula['formula'] == formula, 'target'\n        ].values[0]\n        return avg_bandgap\n    else:\n        return None\n\n\n# Setting up the widget interface\nformula_input = Text(\n    value='',\n    placeholder='Enter formula (e.g., CsPbBr3)',\n    description='Formula:',\n)\npredict_button = Button(description='Predict Bandgap', button_style='success')\noutput = Output()\n\n\ndef on_click(b):\n    with output:\n        output.clear_output()\n        try:\n            formula = formula_input.value.strip()\n            if not formula:\n                print('Please enter a valid chemical formula.')\n                return\n\n            # Prediction\n            prediction, sigma = predict_bandgap(formula)\n            print(\n                f'Predicted Bandgap: {np.round(prediction[0], 3)} \u00b1 {np.round(sigma[0], 3)} eV'\n            )\n\n            # Dataset check\n            avg_bandgap = check_formula_in_dataset(formula)\n            if avg_bandgap is not None:\n                print(\n                    f'The averaged literature bandgap for {formula} is {avg_bandgap:.3f} eV (from dataset).'\n                )\n            else:\n                print(f\"The formula '{formula}' is not contained in the dataset.\")\n        except Exception as e:\n            print(f'Error: {e}')\n\n\npredict_button.on_click(on_click)\n\ndisplay(VBox([HBox([formula_input, predict_button]), output]))\n</pre> import numpy as np import pandas as pd from IPython.display import display from ipywidgets import Button, HBox, Output, Text, VBox   # Function to predict the bandgap of a given formula def predict_bandgap(formula):     val_df = pd.DataFrame({'formula': [formula], 'target': [0.0]})     prediction, prediction_sigma = crabnet_bandgap.predict(         val_df, return_uncertainty=True     )     return prediction, prediction_sigma   # Function to check if the formula exists in the dataset def check_formula_in_dataset(formula):     if formula in df_grouped_formula['formula'].values:         avg_bandgap = df_grouped_formula.loc[             df_grouped_formula['formula'] == formula, 'target'         ].values[0]         return avg_bandgap     else:         return None   # Setting up the widget interface formula_input = Text(     value='',     placeholder='Enter formula (e.g., CsPbBr3)',     description='Formula:', ) predict_button = Button(description='Predict Bandgap', button_style='success') output = Output()   def on_click(b):     with output:         output.clear_output()         try:             formula = formula_input.value.strip()             if not formula:                 print('Please enter a valid chemical formula.')                 return              # Prediction             prediction, sigma = predict_bandgap(formula)             print(                 f'Predicted Bandgap: {np.round(prediction[0], 3)} \u00b1 {np.round(sigma[0], 3)} eV'             )              # Dataset check             avg_bandgap = check_formula_in_dataset(formula)             if avg_bandgap is not None:                 print(                     f'The averaged literature bandgap for {formula} is {avg_bandgap:.3f} eV (from dataset).'                 )             else:                 print(f\"The formula '{formula}' is not contained in the dataset.\")         except Exception as e:             print(f'Error: {e}')   predict_button.on_click(on_click)  display(VBox([HBox([formula_input, predict_button]), output]))"},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#outline","title":"Outline\u00b6","text":"<ul> <li>Installations</li> <li>Retrieve Data using NOMAD API</li> <li>Save Data</li> <li>Load data</li> <li>EDA &amp; Data Cleaning<ul> <li>Insights from NOMAD GUI</li> <li>Remove NaNs</li> <li>Group Repeated Formulas</li> <li>Check Element Prevalence</li> </ul> </li> <li>Build and Fit the Model</li> <li>Model evaluation</li> <li>Predict Bandgap from Individual Formulas</li> </ul>"},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#installations","title":"Installations\u00b6","text":"<p>We will start by running a couple of pip installers. Skip this part if you have the libraries installed in your environment.</p>"},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#retrieve-data-using-nomad-api","title":"Retrieve Data using NOMAD API\u00b6","text":"<p>We will now fetch Perovskite solar cell data from the NOMAD API.  If you already have the data, the perovskite_bandgap_devices.csv in the data folder, you can skip this part, and continue with Loading Data. Note that calling the database through the API may take a while.</p>"},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#save-data","title":"Save Data\u00b6","text":""},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#put-data-into-a-pandas-dataframe-and-save","title":"Put Data into a Pandas DataFrame and Save\u00b6","text":"<p>We can also have a quick look on the DataFrame, and change the units of the bandgap from J to eV.</p>  \u26a0\ufe0f The data in the `results` section of NOMAD is stored in SI units."},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#load-data","title":"Load data\u00b6","text":"<p>In the previous part, we retrieved perovskite solar cell data using the NOMAD API, converted it to a Pandas DataFrame, and saved it as perovskite_bandgap_devices.csv in the data folder. If you already have the data, you can start from this section, where we will import and clean it.</p> <p>The distribution of the chemical diversity of the dataset can be viewed in the dynamic periodic table of the NOMAD entries explorer. The down right corner of every element shows the number of entries (solar cells) that contain in the absorber a given element. It becomes obvious that the data set is imbalanced with the majority of the entries being Pb based, containing C, N and H (hybrid perovskites) and mostly halide compounds, with most of the entries having I and/or Br.</p>"},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#eda-data-cleaning","title":"EDA &amp; Data Cleaning\u00b6","text":"<p>Exploratory Data Analysis (EDA) is a useful step in understanding and preparing datasets for modeling by summarizing data, checking for anomalies, finding patterns and relationships.</p>"},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#insights-from-nomad-gui","title":"Insights from NOMAD GUI\u00b6","text":"<p>The distribution of chemical diversity of the dataset can be explored using the periodic table in the NOMAD solar cell app. The bottom-right corner of each element displays the number of entries (solar cells) that include the element in the absorber. The dataset is notably imbalanced, with the majority of entries being Pb-based, containing C, N, H (hybrid perovskites), and halides like I and Br.</p> <p>Let's have a look on the statistical summary of the dataset:</p>"},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#remove-nans","title":"Remove NaNs\u00b6","text":""},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#group-repeated-formulas","title":"Group Repeated Formulas\u00b6","text":""},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#check-element-prevalence","title":"Check Element Prevalence\u00b6","text":"<p>We will use <code>pymatviz</code> (link to docs) for visualizing the element prevalence.</p> <p>Run the following snippet if the package is not installed in the environment yet, otherwise skip it.</p> <p>You can simply try:</p>"},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#build-and-fit-the-model","title":"Build and Fit the Model\u00b6","text":""},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#model-evaluation","title":"Model evaluation\u00b6","text":""},{"location":"notebooks/perla_notebooks/crabnet-perovskite-bandgap-prediction.html#predict-bandgap-from-individual-formulas","title":"Predict Bandgap from Individual Formulas\u00b6","text":"<p>Now we are ready to run some predictions using our trainned model. We will start loading the model just in case you want to start direcly here in a new session and the model weights are available.</p>"},{"location":"notebooks/perla_notebooks/diversity-analysis.html","title":"Diversity Evolution Analysis","text":"Diversity Evolution Analysis <p>     This notebook analyzes the temporal evolution of material diversity in the Perovskite Solar Cell Database. </p> In\u00a0[10]: Copied! <pre>import warnings\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.lines import Line2D\nfrom scipy import stats\n\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n</pre> import warnings from collections import Counter  import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib.lines import Line2D from scipy import stats  warnings.filterwarnings('ignore')  # Set random seed for reproducibility np.random.seed(42) In\u00a0[4]: Copied! <pre># Load the database (adjust path as needed)\ndf = pd.read_parquet('perovskite_solar_cell_database.parquet')\n\n# Extract key columns\ndf['pub_year'] = pd.to_datetime(\n    df['data.ref.publication_date'], errors='coerce'\n).dt.year\ndf['source'] = df['data.ref.extraction_method'].apply(\n    lambda x: 'LLM' if x == 'LLM' else 'Manual'\n)\ndf['absorber'] = df['results.properties.optoelectronic.solar_cell.absorber']\ndf['HTL'] = df['results.properties.optoelectronic.solar_cell.hole_transport_layer']\ndf['ETL'] = df['results.properties.optoelectronic.solar_cell.electron_transport_layer']\ndf['band_gap'] = pd.to_numeric(df['data.perovskite.band_gap'], errors='coerce')\n\nprint(\n    f'Loaded {len(df):,} device records ({df[\"pub_year\"].min():.0f}\u2013{df[\"pub_year\"].max():.0f})'\n)\nprint(f'  Manual: {(df[\"source\"] == \"Manual\").sum():,}')\nprint(f'  LLM:    {(df[\"source\"] == \"LLM\").sum():,}')\n</pre> # Load the database (adjust path as needed) df = pd.read_parquet('perovskite_solar_cell_database.parquet')  # Extract key columns df['pub_year'] = pd.to_datetime(     df['data.ref.publication_date'], errors='coerce' ).dt.year df['source'] = df['data.ref.extraction_method'].apply(     lambda x: 'LLM' if x == 'LLM' else 'Manual' ) df['absorber'] = df['results.properties.optoelectronic.solar_cell.absorber'] df['HTL'] = df['results.properties.optoelectronic.solar_cell.hole_transport_layer'] df['ETL'] = df['results.properties.optoelectronic.solar_cell.electron_transport_layer'] df['band_gap'] = pd.to_numeric(df['data.perovskite.band_gap'], errors='coerce')  print(     f'Loaded {len(df):,} device records ({df[\"pub_year\"].min():.0f}\u2013{df[\"pub_year\"].max():.0f})' ) print(f'  Manual: {(df[\"source\"] == \"Manual\").sum():,}') print(f'  LLM:    {(df[\"source\"] == \"LLM\").sum():,}') <pre>Loaded 48,380 device records (2009\u20132025)\n  Manual: 42,834\n  LLM:    5,546\n</pre> In\u00a0[5]: Copied! <pre>def flatten_materials(series):\n    \"\"\"\n    Flatten arrays of materials into a single list.\n    The database stores materials as arrays (e.g., multiple HTL layers).\n    \"\"\"\n    materials = []\n    for arr in series.dropna():\n        if hasattr(arr, '__iter__') and not isinstance(arr, str):\n            materials.extend([str(m).strip() for m in arr])\n    return materials\n\n\ndef compute_shannon_entropy(materials_list):\n    \"\"\"\n    Compute Shannon entropy H = -\u03a3 p_i log\u2082(p_i).\n    Returns entropy in bits.\n    \"\"\"\n    if not materials_list:\n        return np.nan\n    counts = Counter(materials_list)\n    total = sum(counts.values())\n    probabilities = [count / total for count in counts.values()]\n    entropy = -sum(p * np.log2(p) for p in probabilities if p &gt; 0)\n    return entropy\n\n\ndef top_k_concentration(materials_list, k=1):\n    \"\"\"\n    Compute the fraction of materials in the top-k most common.\n    \"\"\"\n    if not materials_list:\n        return np.nan\n    counts = Counter(materials_list)\n    total = sum(counts.values())\n    sorted_counts = sorted(counts.values(), reverse=True)\n    return sum(sorted_counts[:k]) / total\n\n\ndef get_top_material(materials_list):\n    \"\"\"Return the most common material and its share.\"\"\"\n    if not materials_list:\n        return None, np.nan\n    counts = Counter(materials_list)\n    total = sum(counts.values())\n    top_mat, top_count = counts.most_common(1)[0]\n    return top_mat, top_count / total\n</pre> def flatten_materials(series):     \"\"\"     Flatten arrays of materials into a single list.     The database stores materials as arrays (e.g., multiple HTL layers).     \"\"\"     materials = []     for arr in series.dropna():         if hasattr(arr, '__iter__') and not isinstance(arr, str):             materials.extend([str(m).strip() for m in arr])     return materials   def compute_shannon_entropy(materials_list):     \"\"\"     Compute Shannon entropy H = -\u03a3 p_i log\u2082(p_i).     Returns entropy in bits.     \"\"\"     if not materials_list:         return np.nan     counts = Counter(materials_list)     total = sum(counts.values())     probabilities = [count / total for count in counts.values()]     entropy = -sum(p * np.log2(p) for p in probabilities if p &gt; 0)     return entropy   def top_k_concentration(materials_list, k=1):     \"\"\"     Compute the fraction of materials in the top-k most common.     \"\"\"     if not materials_list:         return np.nan     counts = Counter(materials_list)     total = sum(counts.values())     sorted_counts = sorted(counts.values(), reverse=True)     return sum(sorted_counts[:k]) / total   def get_top_material(materials_list):     \"\"\"Return the most common material and its share.\"\"\"     if not materials_list:         return None, np.nan     counts = Counter(materials_list)     total = sum(counts.values())     top_mat, top_count = counts.most_common(1)[0]     return top_mat, top_count / total In\u00a0[6]: Copied! <pre>years = range(2012, 2026)\nlayers = ['absorber', 'HTL', 'ETL']\n\n# Storage for results\nentropy_data = []\ncumulative_data = []\ncumulative_materials = {layer: set() for layer in layers}\ncumulative_devices = 0\n\nfor year in years:\n    year_df = df[df['pub_year'] == year]\n    n_devices = len(year_df)\n\n    if n_devices &lt; 10:\n        continue\n\n    cumulative_devices += n_devices\n\n    year_record = {'year': year, 'n_devices': n_devices}\n    cumul_record = {\n        'year': year,\n        'n_devices': n_devices,\n        'cumulative_devices': cumulative_devices,\n    }\n\n    for layer in layers:\n        materials = flatten_materials(year_df[layer])\n        unique_materials = set(materials)\n\n        # Entropy and concentration\n        year_record[f'{layer}_entropy'] = compute_shannon_entropy(materials)\n        year_record[f'{layer}_unique'] = len(unique_materials)\n        year_record[f'{layer}_top1'] = top_k_concentration(materials, 1)\n\n        # Cumulative discovery\n        new_materials = unique_materials - cumulative_materials[layer]\n        cumulative_materials[layer].update(unique_materials)\n        cumul_record[f'{layer}_new'] = len(new_materials)\n        cumul_record[f'{layer}_cumulative'] = len(cumulative_materials[layer])\n\n    entropy_data.append(year_record)\n    cumulative_data.append(cumul_record)\n\nentropy_df = pd.DataFrame(entropy_data)\ncumul_df = pd.DataFrame(cumulative_data)\n\n# Display\nprint('Shannon Entropy by Year (bits):')\ndisplay(\n    entropy_df[\n        ['year', 'n_devices', 'absorber_entropy', 'HTL_entropy', 'ETL_entropy']\n    ].round(2)\n)\n</pre> years = range(2012, 2026) layers = ['absorber', 'HTL', 'ETL']  # Storage for results entropy_data = [] cumulative_data = [] cumulative_materials = {layer: set() for layer in layers} cumulative_devices = 0  for year in years:     year_df = df[df['pub_year'] == year]     n_devices = len(year_df)      if n_devices &lt; 10:         continue      cumulative_devices += n_devices      year_record = {'year': year, 'n_devices': n_devices}     cumul_record = {         'year': year,         'n_devices': n_devices,         'cumulative_devices': cumulative_devices,     }      for layer in layers:         materials = flatten_materials(year_df[layer])         unique_materials = set(materials)          # Entropy and concentration         year_record[f'{layer}_entropy'] = compute_shannon_entropy(materials)         year_record[f'{layer}_unique'] = len(unique_materials)         year_record[f'{layer}_top1'] = top_k_concentration(materials, 1)          # Cumulative discovery         new_materials = unique_materials - cumulative_materials[layer]         cumulative_materials[layer].update(unique_materials)         cumul_record[f'{layer}_new'] = len(new_materials)         cumul_record[f'{layer}_cumulative'] = len(cumulative_materials[layer])      entropy_data.append(year_record)     cumulative_data.append(cumul_record)  entropy_df = pd.DataFrame(entropy_data) cumul_df = pd.DataFrame(cumulative_data)  # Display print('Shannon Entropy by Year (bits):') display(     entropy_df[         ['year', 'n_devices', 'absorber_entropy', 'HTL_entropy', 'ETL_entropy']     ].round(2) ) <pre>Shannon Entropy by Year (bits):\n</pre> year n_devices absorber_entropy HTL_entropy ETL_entropy 0 2012 19 0.30 0.77 1.40 1 2013 226 0.54 2.16 2.42 2 2014 1543 0.71 3.16 3.11 3 2015 3532 0.71 2.73 3.18 4 2016 6211 1.68 3.15 3.50 5 2017 8028 2.26 3.35 3.60 6 2018 10621 2.85 3.67 3.82 7 2019 10635 3.26 3.88 3.84 8 2020 1927 3.92 3.97 3.77 9 2021 951 4.04 4.32 3.78 10 2022 1172 4.03 3.73 3.51 11 2023 1271 4.45 3.48 3.48 12 2024 1572 4.35 3.70 3.60 13 2025 657 3.83 4.16 3.21 In\u00a0[7]: Copied! <pre>heaps_params = {}\n\nprint(\"Heaps' Law Fit: V = K \u00b7 n^\u03b2\")\nprint('-' * 50)\n\nfor layer in layers:\n    n = cumul_df['cumulative_devices'].values\n    V = cumul_df[f'{layer}_cumulative'].values\n\n    mask = (n &gt; 0) &amp; (V &gt; 0)\n    log_n = np.log(n[mask])\n    log_V = np.log(V[mask])\n\n    slope, intercept, r_value, p_value, std_err = stats.linregress(log_n, log_V)\n\n    heaps_params[layer] = {\n        'beta': slope,\n        'beta_se': std_err,\n        'K': np.exp(intercept),\n        'R_squared': r_value**2,\n        'p_value': p_value,\n    }\n\n    print(\n        f'{layer:8s}:  \u03b2 = {slope:.3f} \u00b1 {std_err:.3f},  R\u00b2 = {r_value**2:.4f},  p = {p_value:.2e}'\n    )\n</pre> heaps_params = {}  print(\"Heaps' Law Fit: V = K \u00b7 n^\u03b2\") print('-' * 50)  for layer in layers:     n = cumul_df['cumulative_devices'].values     V = cumul_df[f'{layer}_cumulative'].values      mask = (n &gt; 0) &amp; (V &gt; 0)     log_n = np.log(n[mask])     log_V = np.log(V[mask])      slope, intercept, r_value, p_value, std_err = stats.linregress(log_n, log_V)      heaps_params[layer] = {         'beta': slope,         'beta_se': std_err,         'K': np.exp(intercept),         'R_squared': r_value**2,         'p_value': p_value,     }      print(         f'{layer:8s}:  \u03b2 = {slope:.3f} \u00b1 {std_err:.3f},  R\u00b2 = {r_value**2:.4f},  p = {p_value:.2e}'     ) <pre>Heaps' Law Fit: V = K \u00b7 n^\u03b2\n--------------------------------------------------\nabsorber:  \u03b2 = 0.770 \u00b1 0.052,  R\u00b2 = 0.9474,  p = 4.91e-09\nHTL     :  \u03b2 = 0.851 \u00b1 0.016,  R\u00b2 = 0.9958,  p = 1.20e-15\nETL     :  \u03b2 = 0.748 \u00b1 0.022,  R\u00b2 = 0.9894,  p = 3.27e-13\n</pre> In\u00a0[\u00a0]: Copied! <pre># Style configuration\nplt.rcParams.update(\n    {\n        'font.size': 10,\n        'axes.linewidth': 1,\n        'axes.labelsize': 11,\n        'legend.fontsize': 9,\n        'figure.dpi': 150,\n    }\n)\n\ncolors = {\n    'absorber': '#1f77b4',\n    'HTL': '#ff0e5a',\n    'ETL': '#4cd8a5',\n}\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.5, 3.2))\n\n# Panel A: Entropy\nfor layer in layers:\n    label = 'Absorber' if layer == 'absorber' else layer\n    ax1.plot(\n        entropy_df['year'],\n        entropy_df[f'{layer}_entropy'],\n        'o-',\n        color=colors[layer],\n        label=label,\n        linewidth=1.8,\n        markersize=5,\n    )\n\nax1.set_xlabel('Publication Year')\nax1.set_ylabel('Entropy (H) / bits')\nax1.set_title('Layer entropy over time', fontsize=10)\nax1.legend(frameon=False, loc='lower right')\nax1.set_xticks(range(2012, 2026, 2))\nax1.set_xlim(2011.5, 2025.5)\nax1.set_ylim(0, 5)\nax1.text(\n    -0.12,\n    1.02,\n    'a',\n    transform=ax1.transAxes,\n    fontsize=12,\n    fontweight='bold',\n    va='bottom',\n)\n\n# Panel B: Heaps' Law\nfor layer in layers:\n    n = cumul_df['cumulative_devices'].values\n    V = cumul_df[f'{layer}_cumulative'].values\n    ax2.loglog(n, V, 'o', color=colors[layer], markersize=5)\n\n    params = heaps_params[layer]\n    n_fit = np.logspace(np.log10(n.min()), np.log10(n.max()), 100)\n    V_fit = params['K'] * n_fit ** params['beta']\n    ax2.loglog(n_fit, V_fit, '-', color=colors[layer], linewidth=1.5, alpha=0.7)\n\nax2.set_xlabel('Cumulative devices (n)')\nax2.set_ylabel('Unique materials (V)')\nax2.set_title(\"Heaps' law: $V = K\u00b7n^\u03b2$\", fontsize=10)\n\n# Create custom handles that show both marker and line\ncustom_handles = []\nfor layer in layers:\n    handle = Line2D(\n        [0],\n        [0],\n        color=colors[layer],\n        marker='o',\n        markersize=5,\n        linewidth=1.5,\n        alpha=0.7,\n    )\n    custom_handles.append(handle)\n\nlegend_labels = [\n    f'Absorber (\u03b2={heaps_params[\"absorber\"][\"beta\"]:.2f})',\n    f'HTL (\u03b2={heaps_params[\"HTL\"][\"beta\"]:.2f})',\n    f'ETL (\u03b2={heaps_params[\"ETL\"][\"beta\"]:.2f})',\n]\nax2.legend(custom_handles, legend_labels, frameon=False, loc='lower right', fontsize=8)\n\nax2.text(\n    -0.12,\n    1.02,\n    'b',\n    transform=ax2.transAxes,\n    fontsize=12,\n    fontweight='bold',\n    va='bottom',\n)\n\nplt.tight_layout()\nfig.savefig('fig_diversity_evolution.pdf', bbox_inches='tight', dpi=300)\nplt.show()\n</pre> # Style configuration plt.rcParams.update(     {         'font.size': 10,         'axes.linewidth': 1,         'axes.labelsize': 11,         'legend.fontsize': 9,         'figure.dpi': 150,     } )  colors = {     'absorber': '#1f77b4',     'HTL': '#ff0e5a',     'ETL': '#4cd8a5', }  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.5, 3.2))  # Panel A: Entropy for layer in layers:     label = 'Absorber' if layer == 'absorber' else layer     ax1.plot(         entropy_df['year'],         entropy_df[f'{layer}_entropy'],         'o-',         color=colors[layer],         label=label,         linewidth=1.8,         markersize=5,     )  ax1.set_xlabel('Publication Year') ax1.set_ylabel('Entropy (H) / bits') ax1.set_title('Layer entropy over time', fontsize=10) ax1.legend(frameon=False, loc='lower right') ax1.set_xticks(range(2012, 2026, 2)) ax1.set_xlim(2011.5, 2025.5) ax1.set_ylim(0, 5) ax1.text(     -0.12,     1.02,     'a',     transform=ax1.transAxes,     fontsize=12,     fontweight='bold',     va='bottom', )  # Panel B: Heaps' Law for layer in layers:     n = cumul_df['cumulative_devices'].values     V = cumul_df[f'{layer}_cumulative'].values     ax2.loglog(n, V, 'o', color=colors[layer], markersize=5)      params = heaps_params[layer]     n_fit = np.logspace(np.log10(n.min()), np.log10(n.max()), 100)     V_fit = params['K'] * n_fit ** params['beta']     ax2.loglog(n_fit, V_fit, '-', color=colors[layer], linewidth=1.5, alpha=0.7)  ax2.set_xlabel('Cumulative devices (n)') ax2.set_ylabel('Unique materials (V)') ax2.set_title(\"Heaps' law: $V = K\u00b7n^\u03b2$\", fontsize=10)  # Create custom handles that show both marker and line custom_handles = [] for layer in layers:     handle = Line2D(         [0],         [0],         color=colors[layer],         marker='o',         markersize=5,         linewidth=1.5,         alpha=0.7,     )     custom_handles.append(handle)  legend_labels = [     f'Absorber (\u03b2={heaps_params[\"absorber\"][\"beta\"]:.2f})',     f'HTL (\u03b2={heaps_params[\"HTL\"][\"beta\"]:.2f})',     f'ETL (\u03b2={heaps_params[\"ETL\"][\"beta\"]:.2f})', ] ax2.legend(custom_handles, legend_labels, frameon=False, loc='lower right', fontsize=8)  ax2.text(     -0.12,     1.02,     'b',     transform=ax2.transAxes,     fontsize=12,     fontweight='bold',     va='bottom', )  plt.tight_layout() fig.savefig('fig_diversity_evolution.pdf', bbox_inches='tight', dpi=300) plt.show() In\u00a0[9]: Copied! <pre># Compute metrics for Manual data only\nmanual_df = df[\n    (df['source'] == 'Manual') &amp; (df['pub_year'] &gt;= 2012) &amp; (df['pub_year'] &lt;= 2019)\n]\n\nmanual_metrics = []\nfor year in range(2012, 2020):\n    year_df = manual_df[manual_df['pub_year'] == year]\n    if len(year_df) &lt; 50:\n        continue\n\n    mats = flatten_materials(year_df['absorber'])\n    top_mat, top_share = get_top_material(mats)\n\n    manual_metrics.append(\n        {\n            'year': year,\n            'n_devices': len(year_df),\n            'absorber_entropy': compute_shannon_entropy(mats),\n            'absorber_top1': top_k_concentration(mats, 1),\n            'top_material': top_mat,\n            'top_share': top_share,\n        }\n    )\n\nmanual_metrics_df = pd.DataFrame(manual_metrics)\n\n# Linear regression for entropy trend\nslope, intercept, r, p, se = stats.linregress(\n    manual_metrics_df['year'], manual_metrics_df['absorber_entropy']\n)\n\nprint('Absorber Entropy Trend (Manual Data Only, 2012-2019)')\nprint('=' * 55)\nprint(f'  Slope:     {slope:.3f} bits/year')\nprint(f'  R\u00b2:        {r**2:.3f}')\nprint(f'  p-value:   {p:.2e}')\nprint(\n    f'  Result:    {\"SIGNIFICANT\" if p &lt; 0.01 else \"Not significant\"} increasing trend'\n)\nprint()\ndisplay(\n    manual_metrics_df[\n        ['year', 'n_devices', 'absorber_entropy', 'top_material', 'top_share']\n    ].round(3)\n)\n</pre> # Compute metrics for Manual data only manual_df = df[     (df['source'] == 'Manual') &amp; (df['pub_year'] &gt;= 2012) &amp; (df['pub_year'] &lt;= 2019) ]  manual_metrics = [] for year in range(2012, 2020):     year_df = manual_df[manual_df['pub_year'] == year]     if len(year_df) &lt; 50:         continue      mats = flatten_materials(year_df['absorber'])     top_mat, top_share = get_top_material(mats)      manual_metrics.append(         {             'year': year,             'n_devices': len(year_df),             'absorber_entropy': compute_shannon_entropy(mats),             'absorber_top1': top_k_concentration(mats, 1),             'top_material': top_mat,             'top_share': top_share,         }     )  manual_metrics_df = pd.DataFrame(manual_metrics)  # Linear regression for entropy trend slope, intercept, r, p, se = stats.linregress(     manual_metrics_df['year'], manual_metrics_df['absorber_entropy'] )  print('Absorber Entropy Trend (Manual Data Only, 2012-2019)') print('=' * 55) print(f'  Slope:     {slope:.3f} bits/year') print(f'  R\u00b2:        {r**2:.3f}') print(f'  p-value:   {p:.2e}') print(     f'  Result:    {\"SIGNIFICANT\" if p &lt; 0.01 else \"Not significant\"} increasing trend' ) print() display(     manual_metrics_df[         ['year', 'n_devices', 'absorber_entropy', 'top_material', 'top_share']     ].round(3) ) <pre>Absorber Entropy Trend (Manual Data Only, 2012-2019)\n=======================================================\n  Slope:     0.499 bits/year\n  R\u00b2:        0.949\n  p-value:   2.05e-04\n  Result:    SIGNIFICANT increasing trend\n\n</pre> year n_devices absorber_entropy top_material top_share 0 2013 226 0.537 MAPbI 0.912 1 2014 1543 0.709 MAPbI 0.911 2 2015 3532 0.715 MAPbI 0.915 3 2016 6211 1.683 MAPbI 0.782 4 2017 8028 2.257 MAPbI 0.683 5 2018 10621 2.850 MAPbI 0.582 6 2019 10561 3.250 MAPbI 0.468 In\u00a0[15]: Copied! <pre>n_subsample = 500\nn_bootstrap = 100\nrarefaction_years = [2014, 2016, 2018, 2019, 2020, 2022, 2024]\n\nrarefaction_results = []\n\nfor year in rarefaction_years:\n    year_df = df[df['pub_year'] == year]\n    if len(year_df) &lt; n_subsample:\n        continue\n\n    entropies = []\n    for _ in range(n_bootstrap):\n        sample = year_df.sample(n=n_subsample, replace=False)\n        mats = flatten_materials(sample['absorber'])\n        entropies.append(compute_shannon_entropy(mats))\n\n    rarefaction_results.append(\n        {\n            'year': year,\n            'entropy_mean': np.mean(entropies),\n            'entropy_std': np.std(entropies),\n            'entropy_ci_low': np.percentile(entropies, 2.5),\n            'entropy_ci_high': np.percentile(entropies, 97.5),\n        }\n    )\n\nrarefaction_df = pd.DataFrame(rarefaction_results)\n\nprint(f'Rarefaction Analysis (N={n_subsample}, {n_bootstrap} bootstrap replicates)')\nprint('=' * 55)\nprint('\\n  Year  | Entropy (mean \u00b1 std)  | 95% CI')\nprint('  ------|----------------------|----------------')\nfor _, row in rarefaction_df.iterrows():\n    print(\n        f'  {row[\"year\"]:.0f}  |   {row[\"entropy_mean\"]:.2f} \u00b1 {row[\"entropy_std\"]:.2f}         | [{row[\"entropy_ci_low\"]:.2f}, {row[\"entropy_ci_high\"]:.2f}]'\n    )\n</pre> n_subsample = 500 n_bootstrap = 100 rarefaction_years = [2014, 2016, 2018, 2019, 2020, 2022, 2024]  rarefaction_results = []  for year in rarefaction_years:     year_df = df[df['pub_year'] == year]     if len(year_df) &lt; n_subsample:         continue      entropies = []     for _ in range(n_bootstrap):         sample = year_df.sample(n=n_subsample, replace=False)         mats = flatten_materials(sample['absorber'])         entropies.append(compute_shannon_entropy(mats))      rarefaction_results.append(         {             'year': year,             'entropy_mean': np.mean(entropies),             'entropy_std': np.std(entropies),             'entropy_ci_low': np.percentile(entropies, 2.5),             'entropy_ci_high': np.percentile(entropies, 97.5),         }     )  rarefaction_df = pd.DataFrame(rarefaction_results)  print(f'Rarefaction Analysis (N={n_subsample}, {n_bootstrap} bootstrap replicates)') print('=' * 55) print('\\n  Year  | Entropy (mean \u00b1 std)  | 95% CI') print('  ------|----------------------|----------------') for _, row in rarefaction_df.iterrows():     print(         f'  {row[\"year\"]:.0f}  |   {row[\"entropy_mean\"]:.2f} \u00b1 {row[\"entropy_std\"]:.2f}         | [{row[\"entropy_ci_low\"]:.2f}, {row[\"entropy_ci_high\"]:.2f}]'     ) <pre>Rarefaction Analysis (N=500, 100 bootstrap replicates)\n=======================================================\n\n  Year  | Entropy (mean \u00b1 std)  | 95% CI\n  ------|----------------------|----------------\n  2014  |   0.70 \u00b1 0.08         | [0.54, 0.84]\n  2016  |   1.59 \u00b1 0.10         | [1.39, 1.80]\n  2018  |   2.70 \u00b1 0.11         | [2.46, 2.89]\n  2019  |   3.09 \u00b1 0.12         | [2.89, 3.33]\n  2020  |   3.79 \u00b1 0.10         | [3.56, 3.95]\n  2022  |   3.96 \u00b1 0.08         | [3.80, 4.12]\n  2024  |   4.23 \u00b1 0.09         | [4.05, 4.39]\n</pre> In\u00a0[\u00a0]: Copied! <pre>bandgap_stats = []\n\nperiods = [\n    ('2012-2015', 2012, 2015),\n    ('2016-2018', 2016, 2018),\n    ('2019-2021', 2019, 2021),\n    ('2022-2025', 2022, 2025),\n]\n\nprint('Bandgap Distribution Statistics')\nprint('=' * 65)\nprint('\\n  Period     | N (valid) | Mean (eV) | Std (eV) | IQR (eV) | Median (eV)')\nprint('  -----------|-----------|-----------|----------|----------|----------')\n\nfor period_name, y_start, y_end in periods:\n    mask = (df['pub_year'] &gt;= y_start) &amp; (df['pub_year'] &lt;= y_end)\n    bg = df.loc[mask, 'band_gap'].dropna()\n\n    iqr = bg.quantile(0.75) - bg.quantile(0.25)\n    bandgap_stats.append(\n        {\n            'period': period_name,\n            'n': len(bg),\n            'mean': bg.mean(),\n            'median': bg.median(),\n            'std': bg.std(),\n            'iqr': iqr,\n        }\n    )\n    print(\n        f'  {period_name}  | {len(bg):&gt;9,} | {bg.mean():&gt;9.3f} |  {bg.std():&gt;8.3f} | {iqr:&gt;8.3f}'\n    )\n\nbandgap_stats_df = pd.DataFrame(bandgap_stats)\n\nprint(\n    f'\\n  \u2192 Bandgap std increased from {bandgap_stats_df[\"std\"].iloc[0]:.3f} to {bandgap_stats_df[\"std\"].iloc[-1]:.3f} eV'\n)\n</pre> bandgap_stats = []  periods = [     ('2012-2015', 2012, 2015),     ('2016-2018', 2016, 2018),     ('2019-2021', 2019, 2021),     ('2022-2025', 2022, 2025), ]  print('Bandgap Distribution Statistics') print('=' * 65) print('\\n  Period     | N (valid) | Mean (eV) | Std (eV) | IQR (eV) | Median (eV)') print('  -----------|-----------|-----------|----------|----------|----------')  for period_name, y_start, y_end in periods:     mask = (df['pub_year'] &gt;= y_start) &amp; (df['pub_year'] &lt;= y_end)     bg = df.loc[mask, 'band_gap'].dropna()      iqr = bg.quantile(0.75) - bg.quantile(0.25)     bandgap_stats.append(         {             'period': period_name,             'n': len(bg),             'mean': bg.mean(),             'median': bg.median(),             'std': bg.std(),             'iqr': iqr,         }     )     print(         f'  {period_name}  | {len(bg):&gt;9,} | {bg.mean():&gt;9.3f} |  {bg.std():&gt;8.3f} | {iqr:&gt;8.3f}'     )  bandgap_stats_df = pd.DataFrame(bandgap_stats)  print(     f'\\n  \u2192 Bandgap std increased from {bandgap_stats_df[\"std\"].iloc[0]:.3f} to {bandgap_stats_df[\"std\"].iloc[-1]:.3f} eV' ) <pre>Bandgap Distribution Statistics\n=================================================================\n\n  Period     | N (valid) | Mean (eV) | Std (eV) | IQR (eV)\n  -----------|-----------|-----------|----------|----------\n  2012-2015  |     5,099 |     1.600 |    0.089 |    0.000\n  2016-2018  |    19,481 |     1.606 |    0.115 |    0.000\n  2019-2021  |     8,525 |     1.628 |    0.168 |    0.010\n  2022-2025  |     4,370 |     1.597 |    0.184 |    0.050\n\n  \u2192 Bandgap std increased from 0.089 to 0.184 eV\n</pre> In\u00a0[17]: Copied! <pre>fig, axes = plt.subplots(2, 2, figsize=(8, 7))\n\ncolors_method = {'Manual': '#1f77b4', 'LLM': '#ff7f0e'}\n\n# Compute metrics by year and source for panels a,b\nmethod_results = []\nfor year in range(2012, 2026):\n    for source in ['Manual', 'LLM']:\n        year_df = df[(df['pub_year'] == year) &amp; (df['source'] == source)]\n        if len(year_df) &lt; 30:\n            continue\n        mats = flatten_materials(year_df['absorber'])\n        method_results.append(\n            {\n                'year': year,\n                'source': source,\n                'entropy': compute_shannon_entropy(mats),\n                'top1': top_k_concentration(mats, 1),\n            }\n        )\nmethod_df = pd.DataFrame(method_results)\n\n# Panel A: Entropy by extraction method\nax = axes[0, 0]\nfor source in ['Manual', 'LLM']:\n    sdf = method_df[method_df['source'] == source]\n    ax.plot(\n        sdf['year'],\n        sdf['entropy'],\n        'o-',\n        color=colors_method[source],\n        label=source,\n        linewidth=1.5,\n        markersize=5,\n    )\nax.axvspan(2019, 2021, alpha=0.15, color='gray')\nax.set_xlabel('Publication Year')\nax.set_ylabel('Absorber Entropy (H) / bits')\nax.set_title('a  Entropy by extraction method', loc='left', fontweight='bold')\nax.legend(frameon=False)\nax.set_xlim(2011.5, 2025.5)\nax.annotate('Overlap', xy=(2020, 0.5), fontsize=8, color='gray', ha='center')\n\n# Panel B: Top-1 concentration by method\nax = axes[0, 1]\nfor source in ['Manual', 'LLM']:\n    sdf = method_df[method_df['source'] == source]\n    ax.plot(\n        sdf['year'],\n        sdf['top1'] * 100,\n        'o-',\n        color=colors_method[source],\n        label=source,\n        linewidth=1.5,\n        markersize=5,\n    )\nax.axhline(50, color='gray', linestyle=':', alpha=0.5)\nax.axvspan(2019, 2021, alpha=0.15, color='gray')\nax.set_xlabel('Publication Year')\nax.set_ylabel('Top-1 Concentration (%)')\nax.set_title('b  MAPbI\u2083 dominance decay', loc='left', fontweight='bold')\nax.legend(frameon=False)\nax.set_xlim(2011.5, 2025.5)\n\n# Panel C: Rarefaction\nax = axes[1, 0]\nax.errorbar(\n    rarefaction_df['year'],\n    rarefaction_df['entropy_mean'],\n    yerr=rarefaction_df['entropy_std'] * 1.96,\n    fmt='o-',\n    color='#2ca02c',\n    capsize=4,\n    linewidth=1.5,\n    markersize=6,\n)\nax.set_xlabel('Publication Year')\nax.set_ylabel('Absorber Entropy (H) / bits')\nax.set_title(\n    f'c  Rarefaction analysis (N={n_subsample}, 95% CI)', loc='left', fontweight='bold'\n)\nax.set_xlim(2013, 2025)\n\n# Panel D: Bandgap spread\nax = axes[1, 1]\nbg_yearly = []\nfor year in range(2012, 2026):\n    bg = df[df['pub_year'] == year]['band_gap'].dropna()\n    if len(bg) &gt; 50:\n        bg_yearly.append({'year': year, 'std': bg.std()})\nbg_yearly_df = pd.DataFrame(bg_yearly)\nax.plot(\n    bg_yearly_df['year'],\n    bg_yearly_df['std'],\n    'o-',\n    color='#9467bd',\n    linewidth=1.5,\n    markersize=6,\n)\nax.set_xlabel('Publication Year')\nax.set_ylabel('Bandgap Std. Dev. (eV)')\nax.set_title('d  Physical validation: bandgap spread', loc='left', fontweight='bold')\nax.set_xlim(2011.5, 2025.5)\n\nplt.tight_layout()\nfig.savefig('fig_SI_robustness.pdf', bbox_inches='tight', dpi=300)\nplt.show()\n</pre> fig, axes = plt.subplots(2, 2, figsize=(8, 7))  colors_method = {'Manual': '#1f77b4', 'LLM': '#ff7f0e'}  # Compute metrics by year and source for panels a,b method_results = [] for year in range(2012, 2026):     for source in ['Manual', 'LLM']:         year_df = df[(df['pub_year'] == year) &amp; (df['source'] == source)]         if len(year_df) &lt; 30:             continue         mats = flatten_materials(year_df['absorber'])         method_results.append(             {                 'year': year,                 'source': source,                 'entropy': compute_shannon_entropy(mats),                 'top1': top_k_concentration(mats, 1),             }         ) method_df = pd.DataFrame(method_results)  # Panel A: Entropy by extraction method ax = axes[0, 0] for source in ['Manual', 'LLM']:     sdf = method_df[method_df['source'] == source]     ax.plot(         sdf['year'],         sdf['entropy'],         'o-',         color=colors_method[source],         label=source,         linewidth=1.5,         markersize=5,     ) ax.axvspan(2019, 2021, alpha=0.15, color='gray') ax.set_xlabel('Publication Year') ax.set_ylabel('Absorber Entropy (H) / bits') ax.set_title('a  Entropy by extraction method', loc='left', fontweight='bold') ax.legend(frameon=False) ax.set_xlim(2011.5, 2025.5) ax.annotate('Overlap', xy=(2020, 0.5), fontsize=8, color='gray', ha='center')  # Panel B: Top-1 concentration by method ax = axes[0, 1] for source in ['Manual', 'LLM']:     sdf = method_df[method_df['source'] == source]     ax.plot(         sdf['year'],         sdf['top1'] * 100,         'o-',         color=colors_method[source],         label=source,         linewidth=1.5,         markersize=5,     ) ax.axhline(50, color='gray', linestyle=':', alpha=0.5) ax.axvspan(2019, 2021, alpha=0.15, color='gray') ax.set_xlabel('Publication Year') ax.set_ylabel('Top-1 Concentration (%)') ax.set_title('b  MAPbI\u2083 dominance decay', loc='left', fontweight='bold') ax.legend(frameon=False) ax.set_xlim(2011.5, 2025.5)  # Panel C: Rarefaction ax = axes[1, 0] ax.errorbar(     rarefaction_df['year'],     rarefaction_df['entropy_mean'],     yerr=rarefaction_df['entropy_std'] * 1.96,     fmt='o-',     color='#2ca02c',     capsize=4,     linewidth=1.5,     markersize=6, ) ax.set_xlabel('Publication Year') ax.set_ylabel('Absorber Entropy (H) / bits') ax.set_title(     f'c  Rarefaction analysis (N={n_subsample}, 95% CI)', loc='left', fontweight='bold' ) ax.set_xlim(2013, 2025)  # Panel D: Bandgap spread ax = axes[1, 1] bg_yearly = [] for year in range(2012, 2026):     bg = df[df['pub_year'] == year]['band_gap'].dropna()     if len(bg) &gt; 50:         bg_yearly.append({'year': year, 'std': bg.std()}) bg_yearly_df = pd.DataFrame(bg_yearly) ax.plot(     bg_yearly_df['year'],     bg_yearly_df['std'],     'o-',     color='#9467bd',     linewidth=1.5,     markersize=6, ) ax.set_xlabel('Publication Year') ax.set_ylabel('Bandgap Std. Dev. (eV)') ax.set_title('d  Physical validation: bandgap spread', loc='left', fontweight='bold') ax.set_xlim(2011.5, 2025.5)  plt.tight_layout() fig.savefig('fig_SI_robustness.pdf', bbox_inches='tight', dpi=300) plt.show() In\u00a0[13]: Copied! <pre>import os\n\n# Create SI directory\nos.makedirs('SI_tables', exist_ok=True)\n\n# ============================================================================\n# TABLE S1: Yearly diversity metrics\n# ============================================================================\ntable_s1 = entropy_df[\n    [\n        'year',\n        'n_devices',\n        'absorber_entropy',\n        'absorber_unique',\n        'absorber_top1',\n        'HTL_entropy',\n        'HTL_unique',\n        'HTL_top1',\n        'ETL_entropy',\n        'ETL_unique',\n        'ETL_top1',\n    ]\n].copy()\n\ntable_s1.columns = [\n    'Year',\n    'N_devices',\n    'Absorber_H_bits',\n    'Absorber_unique',\n    'Absorber_top1_frac',\n    'HTL_H_bits',\n    'HTL_unique',\n    'HTL_top1_frac',\n    'ETL_H_bits',\n    'ETL_unique',\n    'ETL_top1_frac',\n]\n\ntable_s1.to_csv(\n    'SI_tables/Table_S1_yearly_diversity_metrics.csv', index=False, float_format='%.3f'\n)\nprint('\u2713 Table S1: Yearly diversity metrics')\n\n# ============================================================================\n# TABLE S2: Cumulative material discovery\n# ============================================================================\ntable_s2 = cumul_df[\n    [\n        'year',\n        'n_devices',\n        'cumulative_devices',\n        'absorber_new',\n        'absorber_cumulative',\n        'HTL_new',\n        'HTL_cumulative',\n        'ETL_new',\n        'ETL_cumulative',\n    ]\n].copy()\n\ntable_s2.columns = [\n    'Year',\n    'N_devices',\n    'Cumulative_devices',\n    'Absorber_new',\n    'Absorber_cumulative',\n    'HTL_new',\n    'HTL_cumulative',\n    'ETL_new',\n    'ETL_cumulative',\n]\n\ntable_s2.to_csv('SI_tables/Table_S2_cumulative_discovery.csv', index=False)\nprint('\u2713 Table S2: Cumulative material discovery')\n\n# ============================================================================\n# TABLE S3: Heaps' law parameters\n# ============================================================================\ntable_s3 = pd.DataFrame(heaps_params).T\ntable_s3.index.name = 'Layer'\ntable_s3 = table_s3[['beta', 'beta_se', 'K', 'R_squared', 'p_value']]\ntable_s3.columns = ['Beta', 'Beta_SE', 'K', 'R_squared', 'p_value']\ntable_s3.to_csv('SI_tables/Table_S3_heaps_law_parameters.csv', float_format='%.4f')\nprint(\"\u2713 Table S3: Heaps' law parameters\")\n\n# ============================================================================\n# TABLE S4: Robustness - Manual data only trend\n# ============================================================================\ntable_s4 = manual_metrics_df.copy()\ntable_s4.columns = [\n    'Year',\n    'N_devices',\n    'Absorber_H_bits',\n    'Absorber_top1_frac',\n    'Top_material',\n    'Top_material_share',\n]\ntable_s4.to_csv(\n    'SI_tables/Table_S4_manual_only_metrics.csv', index=False, float_format='%.3f'\n)\nprint('\u2713 Table S4: Manual data only metrics')\n\n# ============================================================================\n# TABLE S5: Rarefaction results\n# ============================================================================\ntable_s5 = rarefaction_df.copy()\ntable_s5['n_subsample'] = n_subsample\ntable_s5['n_bootstrap'] = n_bootstrap\ntable_s5.columns = [\n    'Year',\n    'Entropy_mean',\n    'Entropy_std',\n    'CI_low_2.5',\n    'CI_high_97.5',\n    'N_subsample',\n    'N_bootstrap',\n]\ntable_s5.to_csv(\n    'SI_tables/Table_S5_rarefaction_analysis.csv', index=False, float_format='%.3f'\n)\nprint('\u2713 Table S5: Rarefaction analysis')\n\n# ============================================================================\n# TABLE S6: Bandgap distribution by period\n# ============================================================================\ntable_s6 = bandgap_stats_df.copy()\ntable_s6.columns = [\n    'Period',\n    'N_valid',\n    'Bandgap_mean_eV',\n    'Bandgap_std_eV',\n    'Bandgap_IQR_eV',\n]\ntable_s6.to_csv(\n    'SI_tables/Table_S6_bandgap_statistics.csv', index=False, float_format='%.4f'\n)\nprint('\u2713 Table S6: Bandgap distribution statistics')\n\n# ============================================================================\n# TABLE S7: Statistical tests summary\n# ============================================================================\nslope_ent, _, r_ent, p_ent, se_ent = stats.linregress(\n    manual_metrics_df['year'], manual_metrics_df['absorber_entropy']\n)\nslope_top1, _, r_top1, p_top1, se_top1 = stats.linregress(\n    manual_metrics_df['year'], manual_metrics_df['absorber_top1']\n)\n\ntable_s7 = pd.DataFrame(\n    [\n        {\n            'Test': 'Entropy trend (Manual 2012-2019)',\n            'Statistic': 'Linear regression slope',\n            'Value': slope_ent,\n            'SE': se_ent,\n            'R_squared': r_ent**2,\n            'p_value': p_ent,\n            'Interpretation': 'Significant increasing trend',\n        },\n        {\n            'Test': 'Top-1 concentration trend (Manual 2012-2019)',\n            'Statistic': 'Linear regression slope',\n            'Value': slope_top1,\n            'SE': se_top1,\n            'R_squared': r_top1**2,\n            'p_value': p_top1,\n            'Interpretation': 'Significant decreasing trend',\n        },\n    ]\n)\ntable_s7.to_csv(\n    'SI_tables/Table_S7_statistical_tests.csv', index=False, float_format='%.4e'\n)\nprint('\u2713 Table S7: Statistical tests summary')\n\nprint('\\n' + '=' * 60)\nprint(\"All SI tables saved to 'SI_tables/' directory\")\nprint('=' * 60)\n</pre> import os  # Create SI directory os.makedirs('SI_tables', exist_ok=True)  # ============================================================================ # TABLE S1: Yearly diversity metrics # ============================================================================ table_s1 = entropy_df[     [         'year',         'n_devices',         'absorber_entropy',         'absorber_unique',         'absorber_top1',         'HTL_entropy',         'HTL_unique',         'HTL_top1',         'ETL_entropy',         'ETL_unique',         'ETL_top1',     ] ].copy()  table_s1.columns = [     'Year',     'N_devices',     'Absorber_H_bits',     'Absorber_unique',     'Absorber_top1_frac',     'HTL_H_bits',     'HTL_unique',     'HTL_top1_frac',     'ETL_H_bits',     'ETL_unique',     'ETL_top1_frac', ]  table_s1.to_csv(     'SI_tables/Table_S1_yearly_diversity_metrics.csv', index=False, float_format='%.3f' ) print('\u2713 Table S1: Yearly diversity metrics')  # ============================================================================ # TABLE S2: Cumulative material discovery # ============================================================================ table_s2 = cumul_df[     [         'year',         'n_devices',         'cumulative_devices',         'absorber_new',         'absorber_cumulative',         'HTL_new',         'HTL_cumulative',         'ETL_new',         'ETL_cumulative',     ] ].copy()  table_s2.columns = [     'Year',     'N_devices',     'Cumulative_devices',     'Absorber_new',     'Absorber_cumulative',     'HTL_new',     'HTL_cumulative',     'ETL_new',     'ETL_cumulative', ]  table_s2.to_csv('SI_tables/Table_S2_cumulative_discovery.csv', index=False) print('\u2713 Table S2: Cumulative material discovery')  # ============================================================================ # TABLE S3: Heaps' law parameters # ============================================================================ table_s3 = pd.DataFrame(heaps_params).T table_s3.index.name = 'Layer' table_s3 = table_s3[['beta', 'beta_se', 'K', 'R_squared', 'p_value']] table_s3.columns = ['Beta', 'Beta_SE', 'K', 'R_squared', 'p_value'] table_s3.to_csv('SI_tables/Table_S3_heaps_law_parameters.csv', float_format='%.4f') print(\"\u2713 Table S3: Heaps' law parameters\")  # ============================================================================ # TABLE S4: Robustness - Manual data only trend # ============================================================================ table_s4 = manual_metrics_df.copy() table_s4.columns = [     'Year',     'N_devices',     'Absorber_H_bits',     'Absorber_top1_frac',     'Top_material',     'Top_material_share', ] table_s4.to_csv(     'SI_tables/Table_S4_manual_only_metrics.csv', index=False, float_format='%.3f' ) print('\u2713 Table S4: Manual data only metrics')  # ============================================================================ # TABLE S5: Rarefaction results # ============================================================================ table_s5 = rarefaction_df.copy() table_s5['n_subsample'] = n_subsample table_s5['n_bootstrap'] = n_bootstrap table_s5.columns = [     'Year',     'Entropy_mean',     'Entropy_std',     'CI_low_2.5',     'CI_high_97.5',     'N_subsample',     'N_bootstrap', ] table_s5.to_csv(     'SI_tables/Table_S5_rarefaction_analysis.csv', index=False, float_format='%.3f' ) print('\u2713 Table S5: Rarefaction analysis')  # ============================================================================ # TABLE S6: Bandgap distribution by period # ============================================================================ table_s6 = bandgap_stats_df.copy() table_s6.columns = [     'Period',     'N_valid',     'Bandgap_mean_eV',     'Bandgap_std_eV',     'Bandgap_IQR_eV', ] table_s6.to_csv(     'SI_tables/Table_S6_bandgap_statistics.csv', index=False, float_format='%.4f' ) print('\u2713 Table S6: Bandgap distribution statistics')  # ============================================================================ # TABLE S7: Statistical tests summary # ============================================================================ slope_ent, _, r_ent, p_ent, se_ent = stats.linregress(     manual_metrics_df['year'], manual_metrics_df['absorber_entropy'] ) slope_top1, _, r_top1, p_top1, se_top1 = stats.linregress(     manual_metrics_df['year'], manual_metrics_df['absorber_top1'] )  table_s7 = pd.DataFrame(     [         {             'Test': 'Entropy trend (Manual 2012-2019)',             'Statistic': 'Linear regression slope',             'Value': slope_ent,             'SE': se_ent,             'R_squared': r_ent**2,             'p_value': p_ent,             'Interpretation': 'Significant increasing trend',         },         {             'Test': 'Top-1 concentration trend (Manual 2012-2019)',             'Statistic': 'Linear regression slope',             'Value': slope_top1,             'SE': se_top1,             'R_squared': r_top1**2,             'p_value': p_top1,             'Interpretation': 'Significant decreasing trend',         },     ] ) table_s7.to_csv(     'SI_tables/Table_S7_statistical_tests.csv', index=False, float_format='%.4e' ) print('\u2713 Table S7: Statistical tests summary')  print('\\n' + '=' * 60) print(\"All SI tables saved to 'SI_tables/' directory\") print('=' * 60) <pre>\u2713 Table S1: Yearly diversity metrics\n\u2713 Table S2: Cumulative material discovery\n\u2713 Table S3: Heaps' law parameters\n\u2713 Table S4: Manual data only metrics\n\u2713 Table S5: Rarefaction analysis\n\u2713 Table S6: Bandgap distribution statistics\n\u2713 Table S7: Statistical tests summary\n\n============================================================\nAll SI tables saved to 'SI_tables/' directory\n============================================================\n</pre>"},{"location":"notebooks/perla_notebooks/diversity-analysis.html#overview","title":"Overview\u00b6","text":"<p>This notebook quantifies the entropy and the evolution of the number of unique devices in the Perovskite Solar Cell Database.</p> <p>The entropy measures the \"flatness\" of a distribution, i.e., if of all known materials only one is used (minimum entropy) or if all are used with the same frequency (maximum entropy). We quantify the evolution also by plotting the cumulative number of devices vs. the number of unique devices. If every device would be a new device architecture, we would expect the relationship to be perfectly linear. In practice, one observes sublinear relationships that can be quantified using relationships such as Heap's law.</p>"},{"location":"notebooks/perla_notebooks/diversity-analysis.html#1-load-data","title":"1. Load Data\u00b6","text":""},{"location":"notebooks/perla_notebooks/diversity-analysis.html#2-define-helper-functions","title":"2. Define Helper Functions\u00b6","text":""},{"location":"notebooks/perla_notebooks/diversity-analysis.html#3-compute-diversity-metrics-by-year","title":"3. Compute Diversity Metrics by Year\u00b6","text":""},{"location":"notebooks/perla_notebooks/diversity-analysis.html#4-heaps-law-analysis","title":"4. Heaps' Law Analysis\u00b6","text":"<p>Heaps' Law describes vocabulary growth: $V(n) = K \\cdot n^\\beta$</p> <ul> <li>$V$ = vocabulary size (unique materials)</li> <li>$n$ = corpus size (cumulative devices)</li> <li>$\\beta &lt; 1$ indicates sublinear growth (diminishing discovery rate)</li> </ul>"},{"location":"notebooks/perla_notebooks/diversity-analysis.html#5-create-main-text-figure","title":"5. Create Main Text Figure\u00b6","text":""},{"location":"notebooks/perla_notebooks/diversity-analysis.html#robustness-analysis","title":"Robustness Analysis\u00b6","text":"<p>The following sections test whether the observed trends are robust to potential confounding factors:</p> <ol> <li>Methodology confounding: Do trends exist within Manual-only data?</li> <li>Effect size argument: Is the dominance decay too large to be a naming artifact?</li> <li>Rarefaction: Do trends persist after controlling for sample size?</li> <li>Physical validation: Does bandgap distribution confirm compositional broadening?</li> </ol>"},{"location":"notebooks/perla_notebooks/diversity-analysis.html#6-test-1-trends-within-manual-data-only-20122019","title":"6. Test 1: Trends Within Manual Data Only (2012\u20132019)\u00b6","text":""},{"location":"notebooks/perla_notebooks/diversity-analysis.html#8-test-2-rarefaction-analysis","title":"8. Test 2: Rarefaction Analysis\u00b6","text":"<p>Control for sample size effects by subsampling to equal N and computing entropy with bootstrap confidence intervals.</p>"},{"location":"notebooks/perla_notebooks/diversity-analysis.html#9-test-3-physical-validation-via-bandgap-distribution","title":"9. Test 3: Physical Validation via Bandgap Distribution\u00b6","text":"<p>Bandgap is a measured physical quantity unaffected by naming conventions. If absorber diversification is real, the bandgap distribution should broaden beyond MAPbI\u2083's characteristic ~1.55\u20131.60 eV.</p>"},{"location":"notebooks/perla_notebooks/diversity-analysis.html#10-create-supplementary-figure","title":"10. Create Supplementary Figure\u00b6","text":""},{"location":"notebooks/perla_notebooks/diversity-analysis.html#11-generate-supplementary-information-tables","title":"11. Generate Supplementary Information Tables\u00b6","text":""},{"location":"notebooks/perla_notebooks/ml-distribution-shift-case-study.html","title":"ML Distribution Shift Case Study","text":"In\u00a0[\u00a0]: Copied! <pre># ruff: noqa: E402, F601, E741, I001, PLR0913\n</pre> # ruff: noqa: E402, F601, E741, I001, PLR0913          Case Study: Distribution Shift in Perovskite Property Prediction <p>     This notebook demonstrates how compositional evolution in perovskite research creates distribution shift challenges for machine learning models. Models trained on historical data underperform when applied to modern compositions. </p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\nimport warnings\n\nwarnings.filterwarnings('ignore')\nimport os\n\nnp.random.seed(42)\n\n# Nature-inspired style\nplt.rcParams.update(\n    {\n        'font.family': 'sans-serif',\n        'font.size': 9,\n        'axes.linewidth': 0.8,\n        'axes.labelsize': 9,\n        'axes.titlesize': 10,\n        'xtick.labelsize': 8,\n        'ytick.labelsize': 8,\n        'legend.fontsize': 7,\n        'lines.linewidth': 1.2,\n        'lines.markersize': 4,\n        'axes.spines.top': True,\n        'axes.spines.right': True,\n        'xtick.direction': 'in',\n        'ytick.direction': 'in',\n        'xtick.major.size': 3,\n        'ytick.major.size': 3,\n        'figure.dpi': 300,\n    }\n)\n\n# Color scheme\ncolors = {\n    'blue': '#1f77b4',\n    'red': '#ff0e5a',\n    'orange': '#ff9408',\n    'green': '#4cd8a5',\n}\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error import warnings  warnings.filterwarnings('ignore') import os  np.random.seed(42)  # Nature-inspired style plt.rcParams.update(     {         'font.family': 'sans-serif',         'font.size': 9,         'axes.linewidth': 0.8,         'axes.labelsize': 9,         'axes.titlesize': 10,         'xtick.labelsize': 8,         'ytick.labelsize': 8,         'legend.fontsize': 7,         'lines.linewidth': 1.2,         'lines.markersize': 4,         'axes.spines.top': True,         'axes.spines.right': True,         'xtick.direction': 'in',         'ytick.direction': 'in',         'xtick.major.size': 3,         'ytick.major.size': 3,         'figure.dpi': 300,     } )  # Color scheme colors = {     'blue': '#1f77b4',     'red': '#ff0e5a',     'orange': '#ff9408',     'green': '#4cd8a5', } In\u00a0[2]: Copied! <pre># Load the database\ndf = pd.read_parquet(\n    'perovskite_solar_cell_database.parquet',\n    columns=[\n        'data.ref.publication_date',\n        'data.perovskite.band_gap',\n        'data.jv.default_PCE',\n        'data.perovskite.composition_a_ions',\n        'data.perovskite.composition_b_ions',\n        'data.perovskite.composition_c_ions',\n        'data.perovskite.composition_a_ions_coefficients',\n        'data.perovskite.composition_b_ions_coefficients',\n        'data.perovskite.composition_c_ions_coefficients',\n    ],\n)\n\ndf.columns = [\n    'pub_date',\n    'band_gap',\n    'PCE',\n    'a_ions',\n    'b_ions',\n    'c_ions',\n    'a_coef',\n    'b_coef',\n    'c_coef',\n]\ndf['pub_year'] = pd.to_datetime(df['pub_date'], errors='coerce').dt.year\ndf['band_gap'] = pd.to_numeric(df['band_gap'], errors='coerce')\ndf['PCE'] = pd.to_numeric(df['PCE'], errors='coerce')\n\nprint(f'Loaded {len(df):,} records')\nprint(f'  With bandgap: {df[\"band_gap\"].notna().sum():,}')\nprint(f'  With PCE: {df[\"PCE\"].notna().sum():,}')\n</pre> # Load the database df = pd.read_parquet(     'perovskite_solar_cell_database.parquet',     columns=[         'data.ref.publication_date',         'data.perovskite.band_gap',         'data.jv.default_PCE',         'data.perovskite.composition_a_ions',         'data.perovskite.composition_b_ions',         'data.perovskite.composition_c_ions',         'data.perovskite.composition_a_ions_coefficients',         'data.perovskite.composition_b_ions_coefficients',         'data.perovskite.composition_c_ions_coefficients',     ], )  df.columns = [     'pub_date',     'band_gap',     'PCE',     'a_ions',     'b_ions',     'c_ions',     'a_coef',     'b_coef',     'c_coef', ] df['pub_year'] = pd.to_datetime(df['pub_date'], errors='coerce').dt.year df['band_gap'] = pd.to_numeric(df['band_gap'], errors='coerce') df['PCE'] = pd.to_numeric(df['PCE'], errors='coerce')  print(f'Loaded {len(df):,} records') print(f'  With bandgap: {df[\"band_gap\"].notna().sum():,}') print(f'  With PCE: {df[\"PCE\"].notna().sum():,}') <pre>Loaded 48,380 records\n  With bandgap: 37,490\n  With PCE: 47,399\n</pre> In\u00a0[\u00a0]: Copied! <pre>def get_features(row):\n    \"\"\"Extract normalized ion composition features.\"\"\"\n\n    def parse(ion_str, coef_str):\n        if pd.isna(ion_str) or pd.isna(coef_str):\n            return {}\n        ions = [i.strip() for i in str(ion_str).split('; ') if i.strip()]\n        try:\n            coeffs = [\n                float(c.strip()) for c in str(coef_str).replace('|', ';').split('; ')\n            ]\n        except ValueError:\n            coeffs = [1.0] * len(ions)\n        result = {\n            ions[i]: coeffs[i] if i &lt; len(coeffs) else 1.0 for i in range(len(ions))\n        }\n        total = sum(result.values())\n        return {k: v / total for k, v in result.items()} if total &gt; 0 else {}\n\n    a = parse(row['a_ions'], row['a_coef'])\n    b = parse(row['b_ions'], row['b_coef'])\n    c = parse(row['c_ions'], row['c_coef'])\n\n    return [\n        a.get('MA', 0),\n        a.get('FA', 0),\n        a.get('Cs', 0),\n        sum(v for k, v in a.items() if k not in ['MA', 'FA', 'Cs']),\n        b.get('Pb', 0),\n        b.get('Sn', 0),\n        sum(v for k, v in b.items() if k not in ['Pb', 'Sn']),\n        c.get('I', 0),\n        c.get('Br', 0),\n        c.get('Cl', 0),\n        sum(v for k, v in c.items() if k not in ['I', 'Br', 'Cl']),\n    ]\n\n\n# Extract features\ndf_valid = df[df['a_ions'].notna() &amp; df['pub_year'].notna()].copy()\nprint(f'Extracting features for {len(df_valid):,} records...')\n\nfeatures = np.array([get_features(row) for _, row in df_valid.iterrows()])\nfeature_names = [\n    'A_MA',\n    'A_FA',\n    'A_Cs',\n    'A_other',\n    'B_Pb',\n    'B_Sn',\n    'B_other',\n    'X_I',\n    'X_Br',\n    'X_Cl',\n    'X_other',\n]\n\nfor i, name in enumerate(feature_names):\n    df_valid[name] = features[:, i]\n\nprint(f'Created {len(feature_names)} composition features')\n</pre> def get_features(row):     \"\"\"Extract normalized ion composition features.\"\"\"      def parse(ion_str, coef_str):         if pd.isna(ion_str) or pd.isna(coef_str):             return {}         ions = [i.strip() for i in str(ion_str).split('; ') if i.strip()]         try:             coeffs = [                 float(c.strip()) for c in str(coef_str).replace('|', ';').split('; ')             ]         except ValueError:             coeffs = [1.0] * len(ions)         result = {             ions[i]: coeffs[i] if i &lt; len(coeffs) else 1.0 for i in range(len(ions))         }         total = sum(result.values())         return {k: v / total for k, v in result.items()} if total &gt; 0 else {}      a = parse(row['a_ions'], row['a_coef'])     b = parse(row['b_ions'], row['b_coef'])     c = parse(row['c_ions'], row['c_coef'])      return [         a.get('MA', 0),         a.get('FA', 0),         a.get('Cs', 0),         sum(v for k, v in a.items() if k not in ['MA', 'FA', 'Cs']),         b.get('Pb', 0),         b.get('Sn', 0),         sum(v for k, v in b.items() if k not in ['Pb', 'Sn']),         c.get('I', 0),         c.get('Br', 0),         c.get('Cl', 0),         sum(v for k, v in c.items() if k not in ['I', 'Br', 'Cl']),     ]   # Extract features df_valid = df[df['a_ions'].notna() &amp; df['pub_year'].notna()].copy() print(f'Extracting features for {len(df_valid):,} records...')  features = np.array([get_features(row) for _, row in df_valid.iterrows()]) feature_names = [     'A_MA',     'A_FA',     'A_Cs',     'A_other',     'B_Pb',     'B_Sn',     'B_other',     'X_I',     'X_Br',     'X_Cl',     'X_other', ]  for i, name in enumerate(feature_names):     df_valid[name] = features[:, i]  print(f'Created {len(feature_names)} composition features') <pre>Extracting features for 48,305 records...\nCreated 11 composition features\n</pre> In\u00a0[\u00a0]: Copied! <pre>old_data = df_valid[df_valid['pub_year'] &lt;= 2018]\nnew_data = df_valid[df_valid['pub_year'] &gt;= 2022]\n\nprint('A-site Composition Shift')\nprint('=' * 45)\nprint(f'{\"Ion\":&lt;6} | {\"Old (\u22642018)\":&gt;12} | {\"New (\u22652022)\":&gt;12} | {\"\u0394\":&gt;8}')\nprint('-' * 45)\nfor ion in ['A_MA', 'A_FA', 'A_Cs']:\n    old_val = old_data[ion].mean()\n    new_val = new_data[ion].mean()\n    print(\n        f'{ion[2:]:&lt;6} | {old_val:&gt;11.0%} | {new_val:&gt;11.0%} | {new_val - old_val:&gt;+7.0%}'\n    )\n</pre> old_data = df_valid[df_valid['pub_year'] &lt;= 2018] new_data = df_valid[df_valid['pub_year'] &gt;= 2022]  print('A-site Composition Shift') print('=' * 45) print(f'{\"Ion\":&lt;6} | {\"Old (\u22642018)\":&gt;12} | {\"New (\u22652022)\":&gt;12} | {\"\u0394\":&gt;8}') print('-' * 45) for ion in ['A_MA', 'A_FA', 'A_Cs']:     old_val = old_data[ion].mean()     new_val = new_data[ion].mean()     print(         f'{ion[2:]:&lt;6} | {old_val:&gt;11.0%} | {new_val:&gt;11.0%} | {new_val - old_val:&gt;+7.0%}'     ) <pre>A-site Composition Shift\n=============================================\nIon    |  Old (\u22642018) |  New (\u22652022) |        \u0394\n---------------------------------------------\nMA     |         80% |         27% |    -53%\nFA     |         14% |         56% |    +42%\nCs     |          4% |         12% |     +8%\n</pre> In\u00a0[\u00a0]: Copied! <pre># Bandgap datasets\nbg_valid = df_valid['band_gap'].notna()\nbg_old = df_valid[(df_valid['pub_year'] &lt;= 2018) &amp; bg_valid]\nbg_new = df_valid[(df_valid['pub_year'] &gt;= 2022) &amp; bg_valid]\n\n# PCE datasets\npce_valid = df_valid['PCE'].notna()\npce_old = df_valid[(df_valid['pub_year'] &lt;= 2018) &amp; pce_valid]\npce_new = df_valid[(df_valid['pub_year'] &gt;= 2022) &amp; pce_valid]\n\n# Create test sets from new data\nbg_test = bg_new.sample(n=min(1000, len(bg_new)), random_state=42)\npce_test = pce_new.sample(n=min(1000, len(pce_new)), random_state=42)\n\nX_test_bg = bg_test[feature_names].values\ny_test_bg = bg_test['band_gap'].values\nX_test_pce = pce_test[feature_names].values\ny_test_pce = pce_test['PCE'].values\n\n# Mixed pools (excluding test data)\nmixed_bg = df_valid[~df_valid.index.isin(bg_test.index) &amp; bg_valid]\nmixed_pce = df_valid[~df_valid.index.isin(pce_test.index) &amp; pce_valid]\n\nprint('Dataset sizes:')\nprint(\n    f'  Bandgap - Old: {len(bg_old):,}, Mixed: {len(mixed_bg):,}, Test: {len(bg_test):,}'\n)\nprint(\n    f'  PCE     - Old: {len(pce_old):,}, Mixed: {len(mixed_pce):,}, Test: {len(pce_test):,}'\n)\n</pre> # Bandgap datasets bg_valid = df_valid['band_gap'].notna() bg_old = df_valid[(df_valid['pub_year'] &lt;= 2018) &amp; bg_valid] bg_new = df_valid[(df_valid['pub_year'] &gt;= 2022) &amp; bg_valid]  # PCE datasets pce_valid = df_valid['PCE'].notna() pce_old = df_valid[(df_valid['pub_year'] &lt;= 2018) &amp; pce_valid] pce_new = df_valid[(df_valid['pub_year'] &gt;= 2022) &amp; pce_valid]  # Create test sets from new data bg_test = bg_new.sample(n=min(1000, len(bg_new)), random_state=42) pce_test = pce_new.sample(n=min(1000, len(pce_new)), random_state=42)  X_test_bg = bg_test[feature_names].values y_test_bg = bg_test['band_gap'].values X_test_pce = pce_test[feature_names].values y_test_pce = pce_test['PCE'].values  # Mixed pools (excluding test data) mixed_bg = df_valid[~df_valid.index.isin(bg_test.index) &amp; bg_valid] mixed_pce = df_valid[~df_valid.index.isin(pce_test.index) &amp; pce_valid]  print('Dataset sizes:') print(     f'  Bandgap - Old: {len(bg_old):,}, Mixed: {len(mixed_bg):,}, Test: {len(bg_test):,}' ) print(     f'  PCE     - Old: {len(pce_old):,}, Mixed: {len(mixed_pce):,}, Test: {len(pce_test):,}' ) <pre>Dataset sizes:\n  Bandgap - Old: 24,580, Mixed: 36,474, Test: 1,000\n  PCE     - Old: 29,556, Mixed: 46,363, Test: 1,000\n</pre> In\u00a0[6]: Copied! <pre>def run_learning_curves_averaged(\n    X_old, y_old, X_mix, y_mix, X_test, y_test, sizes, n_runs=5\n):\n    \"\"\"Run learning curve experiment averaged over multiple random seeds.\"\"\"\n    results = []\n    for n in sizes:\n        if n &gt; min(len(X_old), len(X_mix)):\n            break\n\n        maes_old, maes_mix = [], []\n        for seed in range(n_runs):\n            np.random.seed(seed * 100 + n)\n            idx_old = np.random.choice(len(X_old), size=n, replace=False)\n            idx_mix = np.random.choice(len(X_mix), size=n, replace=False)\n\n            rf = RandomForestRegressor(\n                n_estimators=50, max_depth=8, random_state=42, n_jobs=-1\n            )\n            rf.fit(X_old[idx_old], y_old[idx_old])\n            maes_old.append(mean_absolute_error(y_test, rf.predict(X_test)))\n\n            rf.fit(X_mix[idx_mix], y_mix[idx_mix])\n            maes_mix.append(mean_absolute_error(y_test, rf.predict(X_test)))\n\n        results.append(\n            {\n                'n': n,\n                'mae_old': np.mean(maes_old),\n                'mae_old_std': np.std(maes_old),\n                'mae_mixed': np.mean(maes_mix),\n                'mae_mixed_std': np.std(maes_mix),\n            }\n        )\n        print(\n            f'  n={n:&gt;5}: OLD={np.mean(maes_old):.4f}\u00b1{np.std(maes_old):.4f}, '\n            f'MIXED={np.mean(maes_mix):.4f}\u00b1{np.std(maes_mix):.4f}'\n        )\n\n    return pd.DataFrame(results)\n\n\ntrain_sizes = [100, 250, 500, 1000, 2000, 3000, 5000, 10000, 15000]\n\nprint('Bandgap Learning Curves:')\nlc_bg = run_learning_curves_averaged(\n    bg_old[feature_names].values,\n    bg_old['band_gap'].values,\n    mixed_bg[feature_names].values,\n    mixed_bg['band_gap'].values,\n    X_test_bg,\n    y_test_bg,\n    train_sizes,\n)\n\nprint('\\nPCE Learning Curves:')\nlc_pce = run_learning_curves_averaged(\n    pce_old[feature_names].values,\n    pce_old['PCE'].values,\n    mixed_pce[feature_names].values,\n    mixed_pce['PCE'].values,\n    X_test_pce,\n    y_test_pce,\n    train_sizes,\n)\n</pre> def run_learning_curves_averaged(     X_old, y_old, X_mix, y_mix, X_test, y_test, sizes, n_runs=5 ):     \"\"\"Run learning curve experiment averaged over multiple random seeds.\"\"\"     results = []     for n in sizes:         if n &gt; min(len(X_old), len(X_mix)):             break          maes_old, maes_mix = [], []         for seed in range(n_runs):             np.random.seed(seed * 100 + n)             idx_old = np.random.choice(len(X_old), size=n, replace=False)             idx_mix = np.random.choice(len(X_mix), size=n, replace=False)              rf = RandomForestRegressor(                 n_estimators=50, max_depth=8, random_state=42, n_jobs=-1             )             rf.fit(X_old[idx_old], y_old[idx_old])             maes_old.append(mean_absolute_error(y_test, rf.predict(X_test)))              rf.fit(X_mix[idx_mix], y_mix[idx_mix])             maes_mix.append(mean_absolute_error(y_test, rf.predict(X_test)))          results.append(             {                 'n': n,                 'mae_old': np.mean(maes_old),                 'mae_old_std': np.std(maes_old),                 'mae_mixed': np.mean(maes_mix),                 'mae_mixed_std': np.std(maes_mix),             }         )         print(             f'  n={n:&gt;5}: OLD={np.mean(maes_old):.4f}\u00b1{np.std(maes_old):.4f}, '             f'MIXED={np.mean(maes_mix):.4f}\u00b1{np.std(maes_mix):.4f}'         )      return pd.DataFrame(results)   train_sizes = [100, 250, 500, 1000, 2000, 3000, 5000, 10000, 15000]  print('Bandgap Learning Curves:') lc_bg = run_learning_curves_averaged(     bg_old[feature_names].values,     bg_old['band_gap'].values,     mixed_bg[feature_names].values,     mixed_bg['band_gap'].values,     X_test_bg,     y_test_bg,     train_sizes, )  print('\\nPCE Learning Curves:') lc_pce = run_learning_curves_averaged(     pce_old[feature_names].values,     pce_old['PCE'].values,     mixed_pce[feature_names].values,     mixed_pce['PCE'].values,     X_test_pce,     y_test_pce,     train_sizes, ) <pre>Bandgap Learning Curves:\n  n=  100: OLD=0.0670\u00b10.0069, MIXED=0.0681\u00b10.0076\n  n=  250: OLD=0.0692\u00b10.0098, MIXED=0.0600\u00b10.0058\n  n=  500: OLD=0.0629\u00b10.0030, MIXED=0.0493\u00b10.0038\n  n= 1000: OLD=0.0587\u00b10.0039, MIXED=0.0519\u00b10.0027\n  n= 2000: OLD=0.0561\u00b10.0061, MIXED=0.0466\u00b10.0025\n  n= 3000: OLD=0.0628\u00b10.0041, MIXED=0.0450\u00b10.0021\n  n= 5000: OLD=0.0618\u00b10.0039, MIXED=0.0442\u00b10.0016\n  n=10000: OLD=0.0654\u00b10.0038, MIXED=0.0440\u00b10.0010\n  n=15000: OLD=0.0656\u00b10.0024, MIXED=0.0430\u00b10.0007\n\nPCE Learning Curves:\n  n=  100: OLD=6.7456\u00b10.8197, MIXED=5.1822\u00b10.5262\n  n=  250: OLD=6.5970\u00b10.2666, MIXED=4.9876\u00b10.2410\n  n=  500: OLD=6.7244\u00b10.4808, MIXED=5.1138\u00b10.2300\n  n= 1000: OLD=6.6040\u00b10.4058, MIXED=4.8484\u00b10.2536\n  n= 2000: OLD=6.5354\u00b10.1601, MIXED=4.8609\u00b10.1792\n  n= 3000: OLD=6.5447\u00b10.1121, MIXED=4.6339\u00b10.1119\n  n= 5000: OLD=6.4583\u00b10.0927, MIXED=4.6478\u00b10.0977\n  n=10000: OLD=6.4875\u00b10.1167, MIXED=4.6306\u00b10.0623\n  n=15000: OLD=6.4881\u00b10.0365, MIXED=4.5993\u00b10.0314\n</pre> In\u00a0[\u00a0]: Copied! <pre># Use the largest training set size from learning curves\nn_final = 15000\nn_runs = 5  # Match learning curves averaging\n\n# Bandgap models - use maximum available up to n_final\nn_bg_old = min(n_final, len(bg_old))\nn_bg_mix = min(n_final, len(mixed_bg))\n\n# Average over multiple runs to match learning curves\nmaes_bg_old, maes_bg_mix = [], []\nnmaes_bg_old, nmaes_bg_mix = [], []  # Normalized MAE instead of MAPE\npreds_bg_old_list, preds_bg_mix_list = [], []\n\n# Calculate normalization factor (median of test set bandgap values)\nbg_median = np.median(y_test_bg)\n\nfor seed in range(n_runs):\n    np.random.seed(seed * 100 + n_final)\n    train_old = bg_old.sample(n=n_bg_old, random_state=seed * 100 + n_final)\n    train_mix = mixed_bg.sample(n=n_bg_mix, random_state=seed * 100 + n_final)\n\n    # Use same hyperparameters as learning curves for consistency\n    rf = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42, n_jobs=-1)\n    rf.fit(train_old[feature_names].values, train_old['band_gap'].values)\n    pred_old = rf.predict(X_test_bg)\n    mae = mean_absolute_error(y_test_bg, pred_old)\n    maes_bg_old.append(mae)\n    # Use normalized MAE (nMAE) = MAE / median(actual) * 100\n    nmaes_bg_old.append(mae / bg_median * 100)\n    preds_bg_old_list.append(pred_old)\n\n    rf.fit(train_mix[feature_names].values, train_mix['band_gap'].values)\n    pred_mix = rf.predict(X_test_bg)\n    mae = mean_absolute_error(y_test_bg, pred_mix)\n    maes_bg_mix.append(mae)\n    nmaes_bg_mix.append(mae / bg_median * 100)\n    preds_bg_mix_list.append(pred_mix)\n\n# Average predictions and errors\nmae_bg_old = np.mean(maes_bg_old)\nnmae_bg_old = np.mean(nmaes_bg_old)  # Normalized MAE for bandgap\npred_bg_old = np.mean(preds_bg_old_list, axis=0)\n\nmae_bg_mix = np.mean(maes_bg_mix)\nnmae_bg_mix = np.mean(nmaes_bg_mix)  # Normalized MAE for bandgap\npred_bg_mix = np.mean(preds_bg_mix_list, axis=0)\n\n# PCE models - use maximum available up to n_final\nn_pce_old = min(n_final, len(pce_old))\nn_pce_mix = min(n_final, len(mixed_pce))\n\n# Average over multiple runs to match learning curves\nmaes_pce_old, maes_pce_mix = [], []\nnmaes_pce_old, nmaes_pce_mix = [], []  # Normalized MAE instead of MAPE\npreds_pce_old_list, preds_pce_mix_list = [], []\n\n# Calculate normalization factor (median of test set PCE values)\npce_median = np.median(y_test_pce)\n\nfor seed in range(n_runs):\n    np.random.seed(seed * 100 + n_final)\n    train_old = pce_old.sample(n=n_pce_old, random_state=seed * 100 + n_final)\n    train_mix = mixed_pce.sample(n=n_pce_mix, random_state=seed * 100 + n_final)\n\n    rf = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42, n_jobs=-1)\n    rf.fit(train_old[feature_names].values, train_old['PCE'].values)\n    pred_old = rf.predict(X_test_pce)\n    mae = mean_absolute_error(y_test_pce, pred_old)\n    maes_pce_old.append(mae)\n    # Use normalized MAE (nMAE) = MAE / median(actual) * 100\n    # This gives percentage-like interpretation without MAPE's issues with small values\n    nmaes_pce_old.append(mae / pce_median * 100)\n    preds_pce_old_list.append(pred_old)\n\n    rf.fit(train_mix[feature_names].values, train_mix['PCE'].values)\n    pred_mix = rf.predict(X_test_pce)\n    mae = mean_absolute_error(y_test_pce, pred_mix)\n    maes_pce_mix.append(mae)\n    nmaes_pce_mix.append(mae / pce_median * 100)\n    preds_pce_mix_list.append(pred_mix)\n\n# Average predictions and errors\nmae_pce_old = np.mean(maes_pce_old)\nnmae_pce_old = np.mean(nmaes_pce_old)  # Normalized MAE for PCE\npred_pce_old = np.mean(preds_pce_old_list, axis=0)\n\nmae_pce_mix = np.mean(maes_pce_mix)\nnmae_pce_mix = np.mean(nmaes_pce_mix)  # Normalized MAE for PCE\npred_pce_mix = np.mean(preds_pce_mix_list, axis=0)\n\n# Calculate improvements\nbg_impr = (mae_bg_old - mae_bg_mix) / mae_bg_old * 100\npce_impr = (mae_pce_old - mae_pce_mix) / mae_pce_old * 100\nnmae_bg_impr = (nmae_bg_old - nmae_bg_mix) / nmae_bg_old * 100\nnmae_pce_impr = (nmae_pce_old - nmae_pce_mix) / nmae_pce_old * 100\n\nprint(f'\\nFinal Model Comparison (n={n_final} training samples - largest size)')\nprint(f'  Bandgap: Old={n_bg_old:,}, Mixed={n_bg_mix:,}')\nprint(f'  PCE: Old={n_pce_old:,}, Mixed={n_pce_mix:,}')\nprint('=' * 65)\nprint(f'{\"Target\":&lt;10} | {\"Training\":&lt;12} | {\"MAE\":&gt;10} | {\"nMAE\":&gt;12}')\nprint('-' * 65)\nprint(\n    f'{\"Bandgap\":&lt;10} | {\"Old (\u22642018)\":&lt;12} | {mae_bg_old:&gt;8.4f} eV | {nmae_bg_old:&gt;10.1f}%'\n)\nprint(\n    f'{\"Bandgap\":&lt;10} | {\"Mixed\":&lt;12} | {mae_bg_mix:&gt;8.4f} eV | {nmae_bg_mix:&gt;10.1f}%'\n)\nprint('-' * 65)\nprint(\n    f'{\"PCE\":&lt;10} | {\"Old (\u22642018)\":&lt;12} | {mae_pce_old:&gt;8.2f}%  | {nmae_pce_old:&gt;10.1f}%'\n)\nprint(f'{\"PCE\":&lt;10} | {\"Mixed\":&lt;12} | {mae_pce_mix:&gt;8.2f}%  | {nmae_pce_mix:&gt;10.1f}%')\nprint(f'\\nImprovement: Bandgap {nmae_bg_impr:.0f}%, PCE {nmae_pce_impr:.0f}%')\nprint('Note: nMAE = MAE/median(actual) \u00d7 100 (normalized mean absolute error)')\n</pre> # Use the largest training set size from learning curves n_final = 15000 n_runs = 5  # Match learning curves averaging  # Bandgap models - use maximum available up to n_final n_bg_old = min(n_final, len(bg_old)) n_bg_mix = min(n_final, len(mixed_bg))  # Average over multiple runs to match learning curves maes_bg_old, maes_bg_mix = [], [] nmaes_bg_old, nmaes_bg_mix = [], []  # Normalized MAE instead of MAPE preds_bg_old_list, preds_bg_mix_list = [], []  # Calculate normalization factor (median of test set bandgap values) bg_median = np.median(y_test_bg)  for seed in range(n_runs):     np.random.seed(seed * 100 + n_final)     train_old = bg_old.sample(n=n_bg_old, random_state=seed * 100 + n_final)     train_mix = mixed_bg.sample(n=n_bg_mix, random_state=seed * 100 + n_final)      # Use same hyperparameters as learning curves for consistency     rf = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42, n_jobs=-1)     rf.fit(train_old[feature_names].values, train_old['band_gap'].values)     pred_old = rf.predict(X_test_bg)     mae = mean_absolute_error(y_test_bg, pred_old)     maes_bg_old.append(mae)     # Use normalized MAE (nMAE) = MAE / median(actual) * 100     nmaes_bg_old.append(mae / bg_median * 100)     preds_bg_old_list.append(pred_old)      rf.fit(train_mix[feature_names].values, train_mix['band_gap'].values)     pred_mix = rf.predict(X_test_bg)     mae = mean_absolute_error(y_test_bg, pred_mix)     maes_bg_mix.append(mae)     nmaes_bg_mix.append(mae / bg_median * 100)     preds_bg_mix_list.append(pred_mix)  # Average predictions and errors mae_bg_old = np.mean(maes_bg_old) nmae_bg_old = np.mean(nmaes_bg_old)  # Normalized MAE for bandgap pred_bg_old = np.mean(preds_bg_old_list, axis=0)  mae_bg_mix = np.mean(maes_bg_mix) nmae_bg_mix = np.mean(nmaes_bg_mix)  # Normalized MAE for bandgap pred_bg_mix = np.mean(preds_bg_mix_list, axis=0)  # PCE models - use maximum available up to n_final n_pce_old = min(n_final, len(pce_old)) n_pce_mix = min(n_final, len(mixed_pce))  # Average over multiple runs to match learning curves maes_pce_old, maes_pce_mix = [], [] nmaes_pce_old, nmaes_pce_mix = [], []  # Normalized MAE instead of MAPE preds_pce_old_list, preds_pce_mix_list = [], []  # Calculate normalization factor (median of test set PCE values) pce_median = np.median(y_test_pce)  for seed in range(n_runs):     np.random.seed(seed * 100 + n_final)     train_old = pce_old.sample(n=n_pce_old, random_state=seed * 100 + n_final)     train_mix = mixed_pce.sample(n=n_pce_mix, random_state=seed * 100 + n_final)      rf = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42, n_jobs=-1)     rf.fit(train_old[feature_names].values, train_old['PCE'].values)     pred_old = rf.predict(X_test_pce)     mae = mean_absolute_error(y_test_pce, pred_old)     maes_pce_old.append(mae)     # Use normalized MAE (nMAE) = MAE / median(actual) * 100     # This gives percentage-like interpretation without MAPE's issues with small values     nmaes_pce_old.append(mae / pce_median * 100)     preds_pce_old_list.append(pred_old)      rf.fit(train_mix[feature_names].values, train_mix['PCE'].values)     pred_mix = rf.predict(X_test_pce)     mae = mean_absolute_error(y_test_pce, pred_mix)     maes_pce_mix.append(mae)     nmaes_pce_mix.append(mae / pce_median * 100)     preds_pce_mix_list.append(pred_mix)  # Average predictions and errors mae_pce_old = np.mean(maes_pce_old) nmae_pce_old = np.mean(nmaes_pce_old)  # Normalized MAE for PCE pred_pce_old = np.mean(preds_pce_old_list, axis=0)  mae_pce_mix = np.mean(maes_pce_mix) nmae_pce_mix = np.mean(nmaes_pce_mix)  # Normalized MAE for PCE pred_pce_mix = np.mean(preds_pce_mix_list, axis=0)  # Calculate improvements bg_impr = (mae_bg_old - mae_bg_mix) / mae_bg_old * 100 pce_impr = (mae_pce_old - mae_pce_mix) / mae_pce_old * 100 nmae_bg_impr = (nmae_bg_old - nmae_bg_mix) / nmae_bg_old * 100 nmae_pce_impr = (nmae_pce_old - nmae_pce_mix) / nmae_pce_old * 100  print(f'\\nFinal Model Comparison (n={n_final} training samples - largest size)') print(f'  Bandgap: Old={n_bg_old:,}, Mixed={n_bg_mix:,}') print(f'  PCE: Old={n_pce_old:,}, Mixed={n_pce_mix:,}') print('=' * 65) print(f'{\"Target\":&lt;10} | {\"Training\":&lt;12} | {\"MAE\":&gt;10} | {\"nMAE\":&gt;12}') print('-' * 65) print(     f'{\"Bandgap\":&lt;10} | {\"Old (\u22642018)\":&lt;12} | {mae_bg_old:&gt;8.4f} eV | {nmae_bg_old:&gt;10.1f}%' ) print(     f'{\"Bandgap\":&lt;10} | {\"Mixed\":&lt;12} | {mae_bg_mix:&gt;8.4f} eV | {nmae_bg_mix:&gt;10.1f}%' ) print('-' * 65) print(     f'{\"PCE\":&lt;10} | {\"Old (\u22642018)\":&lt;12} | {mae_pce_old:&gt;8.2f}%  | {nmae_pce_old:&gt;10.1f}%' ) print(f'{\"PCE\":&lt;10} | {\"Mixed\":&lt;12} | {mae_pce_mix:&gt;8.2f}%  | {nmae_pce_mix:&gt;10.1f}%') print(f'\\nImprovement: Bandgap {nmae_bg_impr:.0f}%, PCE {nmae_pce_impr:.0f}%') print('Note: nMAE = MAE/median(actual) \u00d7 100 (normalized mean absolute error)') <pre>Final Model Comparison (n=15000 training samples - largest size)\n  Bandgap: Old=15,000, Mixed=15,000\n  PCE: Old=15,000, Mixed=15,000\n=================================================================\nTarget     | Training     |        MAE |         nMAE\n-----------------------------------------------------------------\nBandgap    | Old (\u22642018)  |   0.0656 eV |        4.2%\nBandgap    | Mixed        |   0.0437 eV |        2.8%\n-----------------------------------------------------------------\nPCE        | Old (\u22642018)  |     6.49%  |       33.2%\nPCE        | Mixed        |     4.63%  |       23.7%\n\nImprovement: Bandgap 33%, PCE 29%\nNote: nMAE = MAE/median(actual) \u00d7 100 (normalized mean absolute error)\n</pre> In\u00a0[8]: Copied! <pre>fig = plt.figure(figsize=(7.2, 5.0))\n\nc_old = colors['red']\nc_mix = colors['blue']\n\n# a: Composition shift\nax1 = fig.add_subplot(2, 3, 1)\nyearly = df_valid.groupby('pub_year')[['A_MA', 'A_FA', 'A_Cs']].mean()\nax1.stackplot(\n    yearly.index,\n    yearly['A_MA'],\n    yearly['A_FA'],\n    yearly['A_Cs'],\n    labels=['MA', 'FA', 'Cs'],\n    colors=[colors['blue'], colors['orange'], colors['green']],\n    alpha=0.85,\n)\nax1.axvline(2018.5, color=colors['red'], linestyle='--', linewidth=1.5, alpha=0.8)\nax1.set_xlabel('Publication year')\nax1.set_ylabel('A-site fraction')\nax1.set_title('a', loc='left', fontweight='bold', fontsize=11)\nax1.legend(loc='center left', fontsize=6, framealpha=0.9)\nax1.set_xlim(2012, 2025)\nax1.set_ylim(0, 1)\n\n# b: Learning curve - Bandgap\nax2 = fig.add_subplot(2, 3, 2)\nax2.errorbar(\n    lc_bg['n'],\n    lc_bg['mae_old'],\n    yerr=lc_bg['mae_old_std'],\n    fmt='o-',\n    color=c_old,\n    label='Old (\u22642018)',\n    capsize=2,\n    capthick=0.8,\n)\nax2.errorbar(\n    lc_bg['n'],\n    lc_bg['mae_mixed'],\n    yerr=lc_bg['mae_mixed_std'],\n    fmt='s-',\n    color=c_mix,\n    label='Mixed',\n    capsize=2,\n    capthick=0.8,\n)\nax2.set_xlabel('Training set size')\nax2.set_ylabel('MAE / eV')\nax2.set_title('b', loc='left', fontweight='bold', fontsize=11)\nax2.legend(fontsize=6, framealpha=0.9)\nax2.set_xscale('log')\nax2.set_ylim(0.03, 0.10)\n\n# c: Learning curve - PCE\nax3 = fig.add_subplot(2, 3, 3)\nax3.errorbar(\n    lc_pce['n'],\n    lc_pce['mae_old'],\n    yerr=lc_pce['mae_old_std'],\n    fmt='o-',\n    color=c_old,\n    label='Old (\u22642018)',\n    capsize=2,\n    capthick=0.8,\n)\nax3.errorbar(\n    lc_pce['n'],\n    lc_pce['mae_mixed'],\n    yerr=lc_pce['mae_mixed_std'],\n    fmt='s-',\n    color=c_mix,\n    label='Mixed',\n    capsize=2,\n    capthick=0.8,\n)\nax3.set_xlabel('Training set size')\nax3.set_ylabel('MAE / %')\nax3.set_title('c', loc='left', fontweight='bold', fontsize=11)\nax3.legend(fontsize=6, framealpha=0.9)\nax3.set_xscale('log')\n\n# d: Bandgap parity\nax4 = fig.add_subplot(2, 3, 4)\nax4.scatter(\n    y_test_bg,\n    pred_bg_old,\n    alpha=0.25,\n    s=6,\n    c=c_old,\n    label=f'Old: {mae_bg_old:.3f} eV',\n    edgecolors='none',\n)\nax4.scatter(\n    y_test_bg,\n    pred_bg_mix,\n    alpha=0.25,\n    s=6,\n    c=c_mix,\n    label=f'Mixed: {mae_bg_mix:.3f} eV',\n    edgecolors='none',\n)\nax4.plot([1.2, 2.8], [1.2, 2.8], 'k-', linewidth=0.8, alpha=0.5)\nax4.set_xlabel('Actual bandgap / eV')\nax4.set_ylabel('Predicted bandgap / eV')\nax4.set_title('d', loc='left', fontweight='bold', fontsize=11)\nax4.set_xlim(1.25, 2.75)\nax4.set_ylim(1.25, 2.75)\nax4.set_aspect('equal')\nax4.legend(fontsize=6, loc='upper left', framealpha=0.9)\n\n# e: PCE parity\nax5 = fig.add_subplot(2, 3, 5)\nax5.scatter(\n    y_test_pce,\n    pred_pce_old,\n    alpha=0.25,\n    s=6,\n    c=c_old,\n    label=f'Old: {mae_pce_old:.1f}%',\n    edgecolors='none',\n)\nax5.scatter(\n    y_test_pce,\n    pred_pce_mix,\n    alpha=0.25,\n    s=6,\n    c=c_mix,\n    label=f'Mixed: {mae_pce_mix:.1f}%',\n    edgecolors='none',\n)\nax5.plot([0, 28], [0, 28], 'k-', linewidth=0.8, alpha=0.5)\nax5.set_xlabel('Actual PCE / %')\nax5.set_ylabel('Predicted PCE / %')\nax5.set_title('e', loc='left', fontweight='bold', fontsize=11)\nax5.set_xlim(0, 28)\nax5.set_ylim(0, 28)\nax5.set_aspect('equal')\nax5.legend(fontsize=6, loc='upper left', framealpha=0.9)\n\n# f: nMAE comparison\nax6 = fig.add_subplot(2, 3, 6)\nx = np.arange(2)\nw = 0.32\nbars1 = ax6.bar(\n    x - w / 2,\n    [nmae_bg_old, nmae_pce_old],\n    w,\n    label='Old (\u22642018)',\n    color=c_old,\n    alpha=0.85,\n    edgecolor='black',\n    linewidth=0.5,\n)\nbars2 = ax6.bar(\n    x + w / 2,\n    [nmae_bg_mix, nmae_pce_mix],\n    w,\n    label='Mixed',\n    color=c_mix,\n    alpha=0.85,\n    edgecolor='black',\n    linewidth=0.5,\n)\nax6.set_ylabel('nMAE / %')\nax6.set_title('f', loc='left', fontweight='bold', fontsize=11)\nax6.set_xticks(x)\nax6.set_xticklabels(['Bandgap', 'PCE'])\nax6.legend(fontsize=6, framealpha=0.9)\n\n# Add improvement annotations\nax6.annotate(\n    f'\u2212{nmae_bg_impr:.0f}%',\n    xy=(w / 2, nmae_bg_mix + 0.3),\n    ha='center',\n    va='bottom',\n    fontsize=7,\n    color=c_mix,\n    fontweight='bold',\n)\nax6.annotate(\n    f'\u2212{nmae_pce_impr:.0f}%',\n    xy=(1 + w / 2, nmae_pce_mix + 0.5),\n    ha='center',\n    va='bottom',\n    fontsize=7,\n    color=c_mix,\n    fontweight='bold',\n)\n\nplt.tight_layout()\nplt.savefig('fig_SI_ml_distribution_shift.pdf', bbox_inches='tight', dpi=300)\nplt.savefig('fig_SI_ml_distribution_shift.png', bbox_inches='tight', dpi=300)\nplt.show()\n\nprint('\\n\u2713 Figure saved as fig_SI_ml_distribution_shift.pdf')\n</pre> fig = plt.figure(figsize=(7.2, 5.0))  c_old = colors['red'] c_mix = colors['blue']  # a: Composition shift ax1 = fig.add_subplot(2, 3, 1) yearly = df_valid.groupby('pub_year')[['A_MA', 'A_FA', 'A_Cs']].mean() ax1.stackplot(     yearly.index,     yearly['A_MA'],     yearly['A_FA'],     yearly['A_Cs'],     labels=['MA', 'FA', 'Cs'],     colors=[colors['blue'], colors['orange'], colors['green']],     alpha=0.85, ) ax1.axvline(2018.5, color=colors['red'], linestyle='--', linewidth=1.5, alpha=0.8) ax1.set_xlabel('Publication year') ax1.set_ylabel('A-site fraction') ax1.set_title('a', loc='left', fontweight='bold', fontsize=11) ax1.legend(loc='center left', fontsize=6, framealpha=0.9) ax1.set_xlim(2012, 2025) ax1.set_ylim(0, 1)  # b: Learning curve - Bandgap ax2 = fig.add_subplot(2, 3, 2) ax2.errorbar(     lc_bg['n'],     lc_bg['mae_old'],     yerr=lc_bg['mae_old_std'],     fmt='o-',     color=c_old,     label='Old (\u22642018)',     capsize=2,     capthick=0.8, ) ax2.errorbar(     lc_bg['n'],     lc_bg['mae_mixed'],     yerr=lc_bg['mae_mixed_std'],     fmt='s-',     color=c_mix,     label='Mixed',     capsize=2,     capthick=0.8, ) ax2.set_xlabel('Training set size') ax2.set_ylabel('MAE / eV') ax2.set_title('b', loc='left', fontweight='bold', fontsize=11) ax2.legend(fontsize=6, framealpha=0.9) ax2.set_xscale('log') ax2.set_ylim(0.03, 0.10)  # c: Learning curve - PCE ax3 = fig.add_subplot(2, 3, 3) ax3.errorbar(     lc_pce['n'],     lc_pce['mae_old'],     yerr=lc_pce['mae_old_std'],     fmt='o-',     color=c_old,     label='Old (\u22642018)',     capsize=2,     capthick=0.8, ) ax3.errorbar(     lc_pce['n'],     lc_pce['mae_mixed'],     yerr=lc_pce['mae_mixed_std'],     fmt='s-',     color=c_mix,     label='Mixed',     capsize=2,     capthick=0.8, ) ax3.set_xlabel('Training set size') ax3.set_ylabel('MAE / %') ax3.set_title('c', loc='left', fontweight='bold', fontsize=11) ax3.legend(fontsize=6, framealpha=0.9) ax3.set_xscale('log')  # d: Bandgap parity ax4 = fig.add_subplot(2, 3, 4) ax4.scatter(     y_test_bg,     pred_bg_old,     alpha=0.25,     s=6,     c=c_old,     label=f'Old: {mae_bg_old:.3f} eV',     edgecolors='none', ) ax4.scatter(     y_test_bg,     pred_bg_mix,     alpha=0.25,     s=6,     c=c_mix,     label=f'Mixed: {mae_bg_mix:.3f} eV',     edgecolors='none', ) ax4.plot([1.2, 2.8], [1.2, 2.8], 'k-', linewidth=0.8, alpha=0.5) ax4.set_xlabel('Actual bandgap / eV') ax4.set_ylabel('Predicted bandgap / eV') ax4.set_title('d', loc='left', fontweight='bold', fontsize=11) ax4.set_xlim(1.25, 2.75) ax4.set_ylim(1.25, 2.75) ax4.set_aspect('equal') ax4.legend(fontsize=6, loc='upper left', framealpha=0.9)  # e: PCE parity ax5 = fig.add_subplot(2, 3, 5) ax5.scatter(     y_test_pce,     pred_pce_old,     alpha=0.25,     s=6,     c=c_old,     label=f'Old: {mae_pce_old:.1f}%',     edgecolors='none', ) ax5.scatter(     y_test_pce,     pred_pce_mix,     alpha=0.25,     s=6,     c=c_mix,     label=f'Mixed: {mae_pce_mix:.1f}%',     edgecolors='none', ) ax5.plot([0, 28], [0, 28], 'k-', linewidth=0.8, alpha=0.5) ax5.set_xlabel('Actual PCE / %') ax5.set_ylabel('Predicted PCE / %') ax5.set_title('e', loc='left', fontweight='bold', fontsize=11) ax5.set_xlim(0, 28) ax5.set_ylim(0, 28) ax5.set_aspect('equal') ax5.legend(fontsize=6, loc='upper left', framealpha=0.9)  # f: nMAE comparison ax6 = fig.add_subplot(2, 3, 6) x = np.arange(2) w = 0.32 bars1 = ax6.bar(     x - w / 2,     [nmae_bg_old, nmae_pce_old],     w,     label='Old (\u22642018)',     color=c_old,     alpha=0.85,     edgecolor='black',     linewidth=0.5, ) bars2 = ax6.bar(     x + w / 2,     [nmae_bg_mix, nmae_pce_mix],     w,     label='Mixed',     color=c_mix,     alpha=0.85,     edgecolor='black',     linewidth=0.5, ) ax6.set_ylabel('nMAE / %') ax6.set_title('f', loc='left', fontweight='bold', fontsize=11) ax6.set_xticks(x) ax6.set_xticklabels(['Bandgap', 'PCE']) ax6.legend(fontsize=6, framealpha=0.9)  # Add improvement annotations ax6.annotate(     f'\u2212{nmae_bg_impr:.0f}%',     xy=(w / 2, nmae_bg_mix + 0.3),     ha='center',     va='bottom',     fontsize=7,     color=c_mix,     fontweight='bold', ) ax6.annotate(     f'\u2212{nmae_pce_impr:.0f}%',     xy=(1 + w / 2, nmae_pce_mix + 0.5),     ha='center',     va='bottom',     fontsize=7,     color=c_mix,     fontweight='bold', )  plt.tight_layout() plt.savefig('fig_SI_ml_distribution_shift.pdf', bbox_inches='tight', dpi=300) plt.savefig('fig_SI_ml_distribution_shift.png', bbox_inches='tight', dpi=300) plt.show()  print('\\n\u2713 Figure saved as fig_SI_ml_distribution_shift.pdf') <pre>\u2713 Figure saved as fig_SI_ml_distribution_shift.pdf\n</pre> In\u00a0[9]: Copied! <pre>os.makedirs('SI_ml_case_study', exist_ok=True)\n\n# Save learning curves\nlc_bg.to_csv('SI_ml_case_study/learning_curve_bandgap.csv', index=False)\nlc_pce.to_csv('SI_ml_case_study/learning_curve_pce.csv', index=False)\n\n# Save model comparison\nsummary = pd.DataFrame(\n    [\n        {\n            'Target': 'Bandgap',\n            'Units': 'eV',\n            'Training': 'Old',\n            'MAE': mae_bg_old,\n            'nMAE': nmae_bg_old,\n        },\n        {\n            'Target': 'Bandgap',\n            'Units': 'eV',\n            'Training': 'Mixed',\n            'MAE': mae_bg_mix,\n            'nMAE': nmae_bg_mix,\n        },\n        {\n            'Target': 'PCE',\n            'Units': '%',\n            'Training': 'Old',\n            'MAE': mae_pce_old,\n            'nMAE': nmae_pce_old,\n        },\n        {\n            'Target': 'PCE',\n            'Units': '%',\n            'Training': 'Mixed',\n            'MAE': mae_pce_mix,\n            'nMAE': nmae_pce_mix,\n        },\n    ]\n)\nsummary.to_csv('SI_ml_case_study/model_comparison_summary.csv', index=False)\n\nprint('\u2713 Results saved to SI_ml_case_study/')\nprint('\\nFiles:')\nfor f in os.listdir('SI_ml_case_study'):\n    print(f'  - {f}')\n</pre> os.makedirs('SI_ml_case_study', exist_ok=True)  # Save learning curves lc_bg.to_csv('SI_ml_case_study/learning_curve_bandgap.csv', index=False) lc_pce.to_csv('SI_ml_case_study/learning_curve_pce.csv', index=False)  # Save model comparison summary = pd.DataFrame(     [         {             'Target': 'Bandgap',             'Units': 'eV',             'Training': 'Old',             'MAE': mae_bg_old,             'nMAE': nmae_bg_old,         },         {             'Target': 'Bandgap',             'Units': 'eV',             'Training': 'Mixed',             'MAE': mae_bg_mix,             'nMAE': nmae_bg_mix,         },         {             'Target': 'PCE',             'Units': '%',             'Training': 'Old',             'MAE': mae_pce_old,             'nMAE': nmae_pce_old,         },         {             'Target': 'PCE',             'Units': '%',             'Training': 'Mixed',             'MAE': mae_pce_mix,             'nMAE': nmae_pce_mix,         },     ] ) summary.to_csv('SI_ml_case_study/model_comparison_summary.csv', index=False)  print('\u2713 Results saved to SI_ml_case_study/') print('\\nFiles:') for f in os.listdir('SI_ml_case_study'):     print(f'  - {f}') <pre>\u2713 Results saved to SI_ml_case_study/\n\nFiles:\n  - model_comparison_summary.csv\n  - learning_curve_bandgap.csv\n  - learning_curve_pce.csv\n</pre> In\u00a0[11]: Copied! <pre>print(f\"\"\"\n{'=' * 65}\nCASE STUDY SUMMARY: Distribution Shift in ML Property Prediction\n{'=' * 65}\n\nCOMPOSITION SHIFT (2012\u20132018 \u2192 2022\u20132025):\n  MA: {old_data['A_MA'].mean():.0%} \u2192 {new_data['A_MA'].mean():.0%}\n  FA: {old_data['A_FA'].mean():.0%} \u2192 {new_data['A_FA'].mean():.0%}\n  Cs: {old_data['A_Cs'].mean():.0%} \u2192 {new_data['A_Cs'].mean():.0%}\n\nMODEL PERFORMANCE (n=15,000 training samples - largest size, tested on 2022\u20132025 data):\n  Bandgap: Old nMAE = {nmae_bg_old:.1f}%  \u2192  Mixed nMAE = {nmae_bg_mix:.1f}%  ({nmae_bg_impr:.0f}% improvement)\n  PCE:     Old nMAE = {nmae_pce_old:.1f}%  \u2192  Mixed nMAE = {nmae_pce_mix:.1f}%  ({nmae_pce_impr:.0f}% improvement)\n  Note: nMAE = MAE/median(actual) \u00d7 100 (normalized mean absolute error)\n\"\"\")\n</pre> print(f\"\"\" {'=' * 65} CASE STUDY SUMMARY: Distribution Shift in ML Property Prediction {'=' * 65}  COMPOSITION SHIFT (2012\u20132018 \u2192 2022\u20132025):   MA: {old_data['A_MA'].mean():.0%} \u2192 {new_data['A_MA'].mean():.0%}   FA: {old_data['A_FA'].mean():.0%} \u2192 {new_data['A_FA'].mean():.0%}   Cs: {old_data['A_Cs'].mean():.0%} \u2192 {new_data['A_Cs'].mean():.0%}  MODEL PERFORMANCE (n=15,000 training samples - largest size, tested on 2022\u20132025 data):   Bandgap: Old nMAE = {nmae_bg_old:.1f}%  \u2192  Mixed nMAE = {nmae_bg_mix:.1f}%  ({nmae_bg_impr:.0f}% improvement)   PCE:     Old nMAE = {nmae_pce_old:.1f}%  \u2192  Mixed nMAE = {nmae_pce_mix:.1f}%  ({nmae_pce_impr:.0f}% improvement)   Note: nMAE = MAE/median(actual) \u00d7 100 (normalized mean absolute error) \"\"\") <pre>=================================================================\nCASE STUDY SUMMARY: Distribution Shift in ML Property Prediction\n=================================================================\n\nCOMPOSITION SHIFT (2012\u20132018 \u2192 2022\u20132025):\n  MA: 80% \u2192 27%\n  FA: 14% \u2192 56%\n  Cs: 4% \u2192 12%\n\nMODEL PERFORMANCE (n=15,000 training samples - largest size, tested on 2022\u20132025 data):\n  Bandgap: Old nMAE = 4.2%  \u2192  Mixed nMAE = 2.8%  (33% improvement)\n  PCE:     Old nMAE = 33.2%  \u2192  Mixed nMAE = 23.7%  (29% improvement)\n  Note: nMAE = MAE/median(actual) \u00d7 100 (normalized mean absolute error)\n\n</pre>"},{"location":"notebooks/perla_notebooks/ml-distribution-shift-case-study.html#1-load-and-prepare-data","title":"1. Load and Prepare Data\u00b6","text":""},{"location":"notebooks/perla_notebooks/ml-distribution-shift-case-study.html#2-feature-engineering","title":"2. Feature Engineering\u00b6","text":""},{"location":"notebooks/perla_notebooks/ml-distribution-shift-case-study.html#3-quantify-distribution-shift","title":"3. Quantify Distribution Shift\u00b6","text":""},{"location":"notebooks/perla_notebooks/ml-distribution-shift-case-study.html#4-prepare-traintest-splits","title":"4. Prepare Train/Test Splits\u00b6","text":""},{"location":"notebooks/perla_notebooks/ml-distribution-shift-case-study.html#5-learning-curves-averaged-over-5-runs","title":"5. Learning Curves (Averaged over 5 runs)\u00b6","text":""},{"location":"notebooks/perla_notebooks/ml-distribution-shift-case-study.html#6-final-model-comparison","title":"6. Final Model Comparison\u00b6","text":""},{"location":"notebooks/perla_notebooks/ml-distribution-shift-case-study.html#7-create-figure","title":"7. Create Figure\u00b6","text":""},{"location":"notebooks/perla_notebooks/ml-distribution-shift-case-study.html#8-export-results","title":"8. Export Results\u00b6","text":""},{"location":"notebooks/perla_notebooks/ml-distribution-shift-case-study.html#9-summary","title":"9. Summary\u00b6","text":""},{"location":"notebooks/perla_notebooks/performance-evolution.html","title":"Performance Evolution Analysis","text":"In\u00a0[1]: \"hide-cell\" Copied! <pre># ruff: noqa: E402\n</pre> # ruff: noqa: E402           Performance Evolution Analysis of Perovskite Solar Cells <p>     This notebook analyses some aspects of the performance evolution in time in the field with the data from the Perovskite database in NOMAD.   </p> <p> </p> In\u00a0[2]: Copied! <pre>from plotly_theme import register_template, set_defaults\n\nregister_template()\nset_defaults()\n</pre> from plotly_theme import register_template, set_defaults  register_template() set_defaults() In\u00a0[3]: Copied! <pre># Load the data from the parquet file into a DataFrame\nimport pandas as pd\n\ndf = pd.read_parquet('perovskite_solar_cell_database.parquet')\n</pre> # Load the data from the parquet file into a DataFrame import pandas as pd  df = pd.read_parquet('perovskite_solar_cell_database.parquet') In\u00a0[4]: Copied! <pre># Set a source_database column: if name_of_person_entering_the_data is 'LLM Extraction', use 'LLM Extracted', else 'Manual Entry'\ndf['source_database'] = df['data.ref.name_of_person_entering_the_data'].apply(\n    lambda x: 'LLM Extracted' if x == 'LLM Extraction' else 'Manual Entry'\n)\n</pre> # Set a source_database column: if name_of_person_entering_the_data is 'LLM Extraction', use 'LLM Extracted', else 'Manual Entry' df['source_database'] = df['data.ref.name_of_person_entering_the_data'].apply(     lambda x: 'LLM Extracted' if x == 'LLM Extraction' else 'Manual Entry' ) In\u00a0[5]: Copied! <pre># let's [;pt efficiency vs publication yera color coded by the source database]\n\nimport plotly.express as px\n\nfig = px.scatter(\n    df,\n    x='data.ref.publication_date',\n    y='results.properties.optoelectronic.solar_cell.efficiency',\n    color='source_database',\n    labels={\n        'data.ref.publication_date': 'Publication year',\n        'results.properties.optoelectronic.solar_cell.efficiency': 'Efficiency / %',\n        'source_database': '',\n    },\n    opacity=0.5,\n)\nfig.update_layout(\n    yaxis_title='Efficiency / %',\n    height=400,\n    width=700,\n)\n\n# x axis from year 2012 to 2026. Note the datetime format\n\nfig.update_xaxes(range=['2012-01-01', '2026-12-31'])\n\nfig.update_traces(\n    mode='markers', marker_line_width=0.5, marker_size=7, marker_line_color='white'\n)\n\nfig.show(renderer=\"notebook\")\n</pre> # let's [;pt efficiency vs publication yera color coded by the source database]  import plotly.express as px  fig = px.scatter(     df,     x='data.ref.publication_date',     y='results.properties.optoelectronic.solar_cell.efficiency',     color='source_database',     labels={         'data.ref.publication_date': 'Publication year',         'results.properties.optoelectronic.solar_cell.efficiency': 'Efficiency / %',         'source_database': '',     },     opacity=0.5, ) fig.update_layout(     yaxis_title='Efficiency / %',     height=400,     width=700, )  # x axis from year 2012 to 2026. Note the datetime format  fig.update_xaxes(range=['2012-01-01', '2026-12-31'])  fig.update_traces(     mode='markers', marker_line_width=0.5, marker_size=7, marker_line_color='white' )  fig.show(renderer=\"notebook\") <p>The newly extracted data show that the field keeps progressing steadily! \ud83d\ude80 \ud83d\udcc8</p> In\u00a0[6]: Copied! <pre># lets convert the bandgap values to eV from J\n\ndf['results.properties.electronic.band_gap.0.value'] = (\n    df['results.properties.electronic.band_gap.0.value'] / 1.60218e-19\n)\n</pre> # lets convert the bandgap values to eV from J  df['results.properties.electronic.band_gap.0.value'] = (     df['results.properties.electronic.band_gap.0.value'] / 1.60218e-19 ) In\u00a0[7]: Copied! <pre>df['sqvoc'] = -0.167 + 0.932 * df['results.properties.electronic.band_gap.0.value']\n</pre> df['sqvoc'] = -0.167 + 0.932 * df['results.properties.electronic.band_gap.0.value'] In\u00a0[8]: Copied! <pre># let's drop rows with missing values in any of the relevant columns\ndf.dropna(\n    subset=[\n        'results.properties.optoelectronic.solar_cell.open_circuit_voltage',\n        'data.ref.publication_date',\n        'results.properties.electronic.band_gap.0.value',\n        'sqvoc',\n    ],\n    inplace=True,\n)\n\n# remove rows where the diff is negative\ndf = df[\n    df['results.properties.optoelectronic.solar_cell.open_circuit_voltage']\n    &lt;= df['sqvoc']\n]\n\n# lets get some statistic of the yearly reported values for the sqvoc - voc, do the difference per row first and then do the stats\n\ndf['diff'] = (\n    df['sqvoc']\n    - df['results.properties.optoelectronic.solar_cell.open_circuit_voltage']\n)\n\n# lets get now the stats per year\n\ndf.groupby(df['data.ref.publication_date'].str[:4])['diff'].describe()\n\n# exclude data before 2013\n\ndf = df[df['data.ref.publication_date'].str[:4].astype(int) &gt;= 2013]\n</pre> # let's drop rows with missing values in any of the relevant columns df.dropna(     subset=[         'results.properties.optoelectronic.solar_cell.open_circuit_voltage',         'data.ref.publication_date',         'results.properties.electronic.band_gap.0.value',         'sqvoc',     ],     inplace=True, )  # remove rows where the diff is negative df = df[     df['results.properties.optoelectronic.solar_cell.open_circuit_voltage']     &lt;= df['sqvoc'] ]  # lets get some statistic of the yearly reported values for the sqvoc - voc, do the difference per row first and then do the stats  df['diff'] = (     df['sqvoc']     - df['results.properties.optoelectronic.solar_cell.open_circuit_voltage'] )  # lets get now the stats per year  df.groupby(df['data.ref.publication_date'].str[:4])['diff'].describe()  # exclude data before 2013  df = df[df['data.ref.publication_date'].str[:4].astype(int) &gt;= 2013] In\u00a0[9]: Copied! <pre># lets plot the mean and std dev per year with plotly\n\nimport plotly.express as px\n\nfig = px.scatter(\n    df.groupby(df['data.ref.publication_date'].str[:4])['diff']\n    .describe()\n    .reset_index(),\n    x='data.ref.publication_date',\n    y='mean',\n    error_y='std',\n    labels={\n        'data.ref.publication_date': 'Year',\n        'mean': 'Mean of &lt;i&gt;V&lt;/i&gt;&lt;sub&gt;OC&lt;/sub&gt;&lt;sup&gt;SQ&lt;/sup&gt; \u2212 &lt;i&gt;V&lt;/i&gt;&lt;sub&gt;OC&lt;/sub&gt;',\n        'std': 'Standard Deviation',\n    },\n)\nfig.update_layout(\n    yaxis_title='Yearly mean of &lt;i&gt;V&lt;/i&gt;&lt;sub&gt;OC&lt;/sub&gt;&lt;sup&gt;SQ&lt;/sup&gt; \u2212 &lt;i&gt;V&lt;/i&gt;&lt;sub&gt;OC&lt;/sub&gt; / V'\n)\n\nfig.update_traces(mode='markers', marker_line_width=0.5, marker_size=12)\n\n# add a fitted line to the plot and label the average decay per year. Do the fit first\n\nimport numpy as np\nfrom scipy import stats\n\nx = (\n    df.groupby(df['data.ref.publication_date'].str[:4])['diff']\n    .describe()\n    .reset_index()['data.ref.publication_date']\n    .astype(int)\n)\ny = (\n    df.groupby(df['data.ref.publication_date'].str[:4])['diff']\n    .describe()\n    .reset_index()['mean']\n)\n\nslope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\nprint(f'Slope: {slope}, Intercept: {intercept}, R-squared: {r_value**2}')\n\nfig.add_traces(\n    px.line(x=x, y=intercept + slope * x, labels={'x': 'Year', 'y': 'Fitted line'})\n    .update_traces(line_color='#ff0e5a')\n    .data\n)\n\nfig.update_layout(\n    height=400,\n    width=700,\n)\n\nfig.show(renderer=\"notebook\")\n</pre> # lets plot the mean and std dev per year with plotly  import plotly.express as px  fig = px.scatter(     df.groupby(df['data.ref.publication_date'].str[:4])['diff']     .describe()     .reset_index(),     x='data.ref.publication_date',     y='mean',     error_y='std',     labels={         'data.ref.publication_date': 'Year',         'mean': 'Mean of V<sub>OC</sub><sup>SQ</sup> \u2212 V<sub>OC</sub>',         'std': 'Standard Deviation',     }, ) fig.update_layout(     yaxis_title='Yearly mean of V<sub>OC</sub><sup>SQ</sup> \u2212 V<sub>OC</sub> / V' )  fig.update_traces(mode='markers', marker_line_width=0.5, marker_size=12)  # add a fitted line to the plot and label the average decay per year. Do the fit first  import numpy as np from scipy import stats  x = (     df.groupby(df['data.ref.publication_date'].str[:4])['diff']     .describe()     .reset_index()['data.ref.publication_date']     .astype(int) ) y = (     df.groupby(df['data.ref.publication_date'].str[:4])['diff']     .describe()     .reset_index()['mean'] )  slope, intercept, r_value, p_value, std_err = stats.linregress(x, y) print(f'Slope: {slope}, Intercept: {intercept}, R-squared: {r_value**2}')  fig.add_traces(     px.line(x=x, y=intercept + slope * x, labels={'x': 'Year', 'y': 'Fitted line'})     .update_traces(line_color='#ff0e5a')     .data )  fig.update_layout(     height=400,     width=700, )  fig.show(renderer=\"notebook\") <pre>Slope: -0.02185503259282026, Intercept: 44.47451936484294, R-squared: 0.9517990118698644\n</pre> <p>To highlight the spread in recombination performance over time, we also render violin plots per publication year. This view complements the summary statistics and makes outliers easy to spot.</p> In\u00a0[10]: Copied! <pre>import plotly.express as px\n\nfig = px.violin(\n    df,\n    x=df['data.ref.publication_date'].str[:4],\n    y='diff',\n    box=True,\n    points='all',\n    labels={\n        'x': 'Publication year',\n        'diff': '&lt;i&gt;V&lt;/i&gt;&lt;sub&gt;OC&lt;/sub&gt;&lt;sup&gt;SQ&lt;/sup&gt; \u2212 &lt;i&gt;V&lt;/i&gt;&lt;sub&gt;OC&lt;/sub&gt; / V',\n    },\n)\nfig.update_traces(marker=dict(opacity=0.1))\n\n# add a fitted trend line in red\nimport plotly.graph_objects as go\n\nyears = df['data.ref.publication_date'].str[:4].astype(int).unique()\nyears.sort()\nmean_diff = (\n    df.groupby(df['data.ref.publication_date'].str[:4])['diff']\n    .mean()\n    .reindex(years.astype(str))\n    .values\n)\nmeadian_diff = (\n    df.groupby(df['data.ref.publication_date'].str[:4])['diff']\n    .median()\n    .reindex(years.astype(str))\n    .values\n)\n# Fit a linear trend line\ncoeffs = np.polyfit(years, meadian_diff, 1)\ntrend_line = np.polyval(coeffs, years)\n\n# calculate the slope and intercept\nslope = coeffs[0]\nintercept = coeffs[1]\n\n# print r-squared value\ncorrelation_matrix = np.corrcoef(years, meadian_diff)\ncorrelation_xy = correlation_matrix[0, 1]\nr_squared = correlation_xy**2\nprint(f'R-squared: {r_squared}')\n\nfig.add_trace(\n    go.Scatter(\n        x=years,\n        y=trend_line,\n        mode='lines',\n        line=dict(color='#ff0e5a'),\n        name=f'{slope:.3f} V / year',\n    )\n)\n\nfig.update_layout(\n    height=400,\n    width=700,\n)\n\nfig.show(renderer=\"notebook\")\n</pre> import plotly.express as px  fig = px.violin(     df,     x=df['data.ref.publication_date'].str[:4],     y='diff',     box=True,     points='all',     labels={         'x': 'Publication year',         'diff': 'V<sub>OC</sub><sup>SQ</sup> \u2212 V<sub>OC</sub> / V',     }, ) fig.update_traces(marker=dict(opacity=0.1))  # add a fitted trend line in red import plotly.graph_objects as go  years = df['data.ref.publication_date'].str[:4].astype(int).unique() years.sort() mean_diff = (     df.groupby(df['data.ref.publication_date'].str[:4])['diff']     .mean()     .reindex(years.astype(str))     .values ) meadian_diff = (     df.groupby(df['data.ref.publication_date'].str[:4])['diff']     .median()     .reindex(years.astype(str))     .values ) # Fit a linear trend line coeffs = np.polyfit(years, meadian_diff, 1) trend_line = np.polyval(coeffs, years)  # calculate the slope and intercept slope = coeffs[0] intercept = coeffs[1]  # print r-squared value correlation_matrix = np.corrcoef(years, meadian_diff) correlation_xy = correlation_matrix[0, 1] r_squared = correlation_xy**2 print(f'R-squared: {r_squared}')  fig.add_trace(     go.Scatter(         x=years,         y=trend_line,         mode='lines',         line=dict(color='#ff0e5a'),         name=f'{slope:.3f} V / year',     ) )  fig.update_layout(     height=400,     width=700, )  fig.show(renderer=\"notebook\") <pre>R-squared: 0.9751363550492053\n</pre> <p>That completes this brief exploration. The near-linear efficiency gains emphasize how steadily the field has progressed, even as new transport layers and novel characterization techniques such as absolute photoluminescence have accelerated loss analysis.</p>"},{"location":"notebooks/perla_notebooks/performance-evolution.html#efficiency-evolution-over-time","title":"Efficiency evolution over time\u00b6","text":"<p>We start by inspecting how reported power-conversion efficiency has progressed over the publication year. The link below opens the filtered view in the NOMAD dashboard. You can use the scatter plot widget to look at the data from the figure directly inside NOMAD.</p> Dashboard YAML <pre>- type: scatter_plot\n  autorange: true\n  size: 10000\n  y:\n    search_quantity: results.properties.optoelectronic.solar_cell.efficiency\n  x:\n    search_quantity: data.ref.publication_date#perovskite_solar_cell_database.schema.PerovskiteSolarCell\n  layout:\n    xxl:\n      minH: 3\n      minW: 3\n      h: 6\n      w: 9\n      y: 0\n      x: Infinity\n    xl:\n      minH: 3\n      minW: 3\n      h: 6\n      w: 9\n      y: 0\n      x: 0\n    lg:\n      minH: 3\n      minW: 3\n      h: 6\n      w: 9\n      y: 0\n      x: Infinity\n    md:\n      minH: 3\n      minW: 3\n      h: 6\n      w: 9\n      y: 0\n      x: Infinity\n    sm:\n      minH: 3\n      minW: 3\n      h: 6\n      w: 9\n      y: 0\n      x: Infinity\n</pre>"},{"location":"notebooks/perla_notebooks/performance-evolution.html#recombination-loss-evolution","title":"Recombination loss evolution\u00b6","text":"<p>Next, we analyze the normalized reduction in recombination losses across the literature. Because this requires transforming the raw quantities, the plot cannot be reproduced directly inside the Perovskite Database dashboard, but the full workflow is captured here.</p> <p>We:</p> <ul> <li>retrieve the perovskite band gap (E<sub>g</sub>) from the archive and convert it from joules to electronvolts,</li> <li>estimate the Shockley\u2013Queisser limited open-circuit voltage (V<sub>OC,SQ</sub>) using the phenomenological relation valid for AM1.5G,</li> <li>compare the measured V<sub>OC</sub> against V<sub>OC,SQ</sub> to quantify the normalized loss.</li> </ul> <p>For reference (see SI in this paper), the SQ expression used here is:</p> <p>V<sub>OC,SQ</sub> = -0.167 + 0.932 \u00b7 E<sub>g</sub></p> <p>Several Python implementations are available if you want to explore the SQ limit further, for example: https://github.com/sidihamady/Shockley-Queisser</p>"},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html","title":"Perla evals analysis","text":"In\u00a0[\u00a0]: Copied! <pre># ruff: noqa: E402, F601, E741, I001, UP015\n</pre> # ruff: noqa: E402, F601, E741, I001, UP015           PERLA Evaluation Analysis <p>     This notebook evaluates the performance of the PERLA extraction pipeline by computing extraction metrics against ground truth data. </p> <p>The evaluation is done by comparing the extracted data to a ground truth dataset. Sometimes, the scoring will use an LLM to score the extracted data.</p> <p>For this reason, we need API keys for the LLMs we are using.</p> In\u00a0[1]: Copied! <pre># --- Imports ---\nimport json\nimport os\nfrom importlib.resources import files\nfrom math import pi\nfrom pathlib import Path\n\n\nimport dabest\nimport litellm\nfrom litellm.caching.caching import Cache\n\nlitellm.cache = Cache(type='disk')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom tqdm import tqdm\n\n# Third-party libraries\nfrom dotenv import load_dotenv\n\n\n# Internal modules\n# Ensure 'perovscribe' is accessible in the repo structure\nfrom perovscribe.pipeline import ExtractionPipeline\nfrom plotly_theme import register_template, set_defaults, MODEL_COLORS\n\n# --- Configuration &amp; Theme ---\nload_dotenv()  # Loads .env if present\nregister_template()\nset_defaults()\n\nplt.rcParams.update(\n    {\n        'font.family': 'Arial',\n        'font.size': 10,\n        'axes.labelsize': 11,\n        'axes.titlesize': 12,\n        'xtick.labelsize': 10,\n        'ytick.labelsize': 10,\n        'legend.fontsize': 10,\n        'figure.titlesize': 12,\n        'axes.linewidth': 1,\n        'axes.edgecolor': 'black',\n        'axes.facecolor': 'white',\n        'figure.facecolor': 'white',\n        'grid.color': 'lightgray',\n        'grid.linewidth': 0.5,\n        'axes.grid': False,\n        'axes.spines.top': True,\n        'axes.spines.right': True,\n        'savefig.dpi': 300,\n        'savefig.bbox': 'tight',\n        'savefig.facecolor': 'white',\n    }\n)\n# Define Paths (Use relative paths for reproducibility!)\nDATA_DIR = files('perovscribe').joinpath('data')\nEXTRACTIONS_DIR = DATA_DIR / 'extractions'\nGROUND_TRUTH_DIR = DATA_DIR / 'ground_truth' / 'test'\nDEV_DIR = DATA_DIR / 'ground_truth' / 'dev'\nEXPERTS_DIR = EXTRACTIONS_DIR / 'humans' / 'Consensus'\n# MODEL_COLORS is imported from plotly_theme (single source of truth for colors)\n</pre> # --- Imports --- import json import os from importlib.resources import files from math import pi from pathlib import Path   import dabest import litellm from litellm.caching.caching import Cache  litellm.cache = Cache(type='disk') import matplotlib.pyplot as plt import numpy as np import pandas as pd import plotly.graph_objects as go from tqdm import tqdm  # Third-party libraries from dotenv import load_dotenv   # Internal modules # Ensure 'perovscribe' is accessible in the repo structure from perovscribe.pipeline import ExtractionPipeline from plotly_theme import register_template, set_defaults, MODEL_COLORS  # --- Configuration &amp; Theme --- load_dotenv()  # Loads .env if present register_template() set_defaults()  plt.rcParams.update(     {         'font.family': 'Arial',         'font.size': 10,         'axes.labelsize': 11,         'axes.titlesize': 12,         'xtick.labelsize': 10,         'ytick.labelsize': 10,         'legend.fontsize': 10,         'figure.titlesize': 12,         'axes.linewidth': 1,         'axes.edgecolor': 'black',         'axes.facecolor': 'white',         'figure.facecolor': 'white',         'grid.color': 'lightgray',         'grid.linewidth': 0.5,         'axes.grid': False,         'axes.spines.top': True,         'axes.spines.right': True,         'savefig.dpi': 300,         'savefig.bbox': 'tight',         'savefig.facecolor': 'white',     } ) # Define Paths (Use relative paths for reproducibility!) DATA_DIR = files('perovscribe').joinpath('data') EXTRACTIONS_DIR = DATA_DIR / 'extractions' GROUND_TRUTH_DIR = DATA_DIR / 'ground_truth' / 'test' DEV_DIR = DATA_DIR / 'ground_truth' / 'dev' EXPERTS_DIR = EXTRACTIONS_DIR / 'humans' / 'Consensus' # MODEL_COLORS is imported from plotly_theme (single source of truth for colors) <pre>Pre-compiling numba functions for DABEST...\n</pre> <pre>Compiling numba functions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:00&lt;00:00, 61.77it/s]</pre> <pre>Numba compilation complete!\n</pre> <pre>22:39:14 - LiteLLM:ERROR: redis_cache.py:178 - Error connecting to Sync Redis client\n</pre> <pre>22:39:15 - LiteLLM:ERROR: redis_cache.py:1081 - LiteLLM Redis Cache PING: - Got exception from REDIS : Error 61 connecting to 127.0.0.1:6379. Connect call failed ('127.0.0.1', 6379).\n</pre> In\u00a0[2]: Copied! <pre># ============================================================================\n# COUNT DEVICES IN TEST AND VALIDATION SETS\n# ============================================================================\n\n\ndef count_devices_in_directory(ground_truth_dir):\n    \"\"\"\n    Count the number of devices in a ground truth directory.\n\n    Each JSON file contains a \"cells\" list, where each element represents a device.\n    This function counts the length of the \"cells\" list in each file.\n\n    Args:\n        ground_truth_dir: Path to ground truth directory\n\n    Returns:\n        dict with 'files' (number of JSON files), 'total_devices' (sum of len(cells)),\n        and 'papers' (number of unique papers/DOIs)\n    \"\"\"\n    if not ground_truth_dir.exists():\n        return {'files': 0, 'total_devices': 0, 'papers': 0}\n\n    json_files = list(ground_truth_dir.glob('*.json'))\n    num_files = len(json_files)\n\n    # Count devices from \"cells\" list in each JSON file\n    total_devices = 0\n    papers_with_devices = set()\n\n    for json_file in json_files:\n        try:\n            with open(json_file, 'r') as f:\n                data = json.load(f)\n\n            # Count devices: length of \"cells\" list\n            if isinstance(data, dict) and 'cells' in data:\n                devices_in_file = len(data['cells'])\n                total_devices += devices_in_file\n\n                # Extract DOI from filename (format: DOI.json)\n                # Filename format is like \"10.1002--adma.202305822.json\"\n                doi = json_file.stem.replace('--', '/')\n                papers_with_devices.add(doi)\n            else:\n                # Fallback: if structure is unexpected, count as 1 device\n                total_devices += 1\n\n        except (json.JSONDecodeError, KeyError, TypeError) as e:\n            # If file can't be parsed, skip it\n            print(f'Warning: Could not parse {json_file.name}: {e}')\n\n    return {\n        'files': num_files,\n        'total_devices': total_devices,\n        'papers': len(papers_with_devices) if papers_with_devices else num_files,\n    }\n\n\n# Count devices in test and validation sets\ntest_counts = count_devices_in_directory(GROUND_TRUTH_DIR)\ndev_counts = count_devices_in_directory(DEV_DIR)\n\nprint('=' * 60)\nprint('DEVICE COUNTS IN DATASETS')\nprint('=' * 60)\nprint('\\nTest Set:')\nprint(f'  Number of files: {test_counts[\"files\"]}')\nprint(f'  Estimated devices: {test_counts[\"total_devices\"]}')\nprint(f'  Unique papers: {test_counts[\"papers\"]}')\n\nprint('\\nValidation Set (dev):')\nprint(f'  Number of files: {dev_counts[\"files\"]}')\nprint(f'  Estimated devices: {dev_counts[\"total_devices\"]}')\nprint(f'  Unique papers: {dev_counts[\"papers\"]}')\n\nprint('\\nTotal:')\nprint(f'  Files: {test_counts[\"files\"] + dev_counts[\"files\"]}')\nprint(f'  Devices: {test_counts[\"total_devices\"] + dev_counts[\"total_devices\"]}')\nprint(f'  Papers: {test_counts[\"papers\"] + dev_counts[\"papers\"]}')\nprint('=' * 60)\n</pre> # ============================================================================ # COUNT DEVICES IN TEST AND VALIDATION SETS # ============================================================================   def count_devices_in_directory(ground_truth_dir):     \"\"\"     Count the number of devices in a ground truth directory.      Each JSON file contains a \"cells\" list, where each element represents a device.     This function counts the length of the \"cells\" list in each file.      Args:         ground_truth_dir: Path to ground truth directory      Returns:         dict with 'files' (number of JSON files), 'total_devices' (sum of len(cells)),         and 'papers' (number of unique papers/DOIs)     \"\"\"     if not ground_truth_dir.exists():         return {'files': 0, 'total_devices': 0, 'papers': 0}      json_files = list(ground_truth_dir.glob('*.json'))     num_files = len(json_files)      # Count devices from \"cells\" list in each JSON file     total_devices = 0     papers_with_devices = set()      for json_file in json_files:         try:             with open(json_file, 'r') as f:                 data = json.load(f)              # Count devices: length of \"cells\" list             if isinstance(data, dict) and 'cells' in data:                 devices_in_file = len(data['cells'])                 total_devices += devices_in_file                  # Extract DOI from filename (format: DOI.json)                 # Filename format is like \"10.1002--adma.202305822.json\"                 doi = json_file.stem.replace('--', '/')                 papers_with_devices.add(doi)             else:                 # Fallback: if structure is unexpected, count as 1 device                 total_devices += 1          except (json.JSONDecodeError, KeyError, TypeError) as e:             # If file can't be parsed, skip it             print(f'Warning: Could not parse {json_file.name}: {e}')      return {         'files': num_files,         'total_devices': total_devices,         'papers': len(papers_with_devices) if papers_with_devices else num_files,     }   # Count devices in test and validation sets test_counts = count_devices_in_directory(GROUND_TRUTH_DIR) dev_counts = count_devices_in_directory(DEV_DIR)  print('=' * 60) print('DEVICE COUNTS IN DATASETS') print('=' * 60) print('\\nTest Set:') print(f'  Number of files: {test_counts[\"files\"]}') print(f'  Estimated devices: {test_counts[\"total_devices\"]}') print(f'  Unique papers: {test_counts[\"papers\"]}')  print('\\nValidation Set (dev):') print(f'  Number of files: {dev_counts[\"files\"]}') print(f'  Estimated devices: {dev_counts[\"total_devices\"]}') print(f'  Unique papers: {dev_counts[\"papers\"]}')  print('\\nTotal:') print(f'  Files: {test_counts[\"files\"] + dev_counts[\"files\"]}') print(f'  Devices: {test_counts[\"total_devices\"] + dev_counts[\"total_devices\"]}') print(f'  Papers: {test_counts[\"papers\"] + dev_counts[\"papers\"]}') print('=' * 60) <pre>============================================================\nDEVICE COUNTS IN DATASETS\n============================================================\n\nTest Set:\n  Number of files: 20\n  Estimated devices: 101\n  Unique papers: 20\n\nValidation Set (dev):\n  Number of files: 10\n  Estimated devices: 35\n  Unique papers: 10\n\nTotal:\n  Files: 30\n  Devices: 136\n  Papers: 30\n============================================================\n</pre> In\u00a0[3]: Copied! <pre># Define model metadata: Display Names, Colors, and Token Costs (per 1M tokens)\n# Prices are examples; verify current API pricing.\n# Colors are obtained from MODEL_COLORS (imported from plotly_theme)\nMODEL_CONFIG = {\n    'gpt-5-2025-08-07': {\n        'name': 'GPT-5',\n        'color': MODEL_COLORS['GPT-5'],\n    },\n    'gpt-5-mini-2025-08-07': {\n        'name': 'GPT-5 Mini',\n        'color': MODEL_COLORS['GPT-5 Mini'],\n    },\n    'claude-opus-4-20250514': {\n        'name': 'Claude Opus 4',\n        'color': MODEL_COLORS['Claude Opus 4'],\n    },\n    'claude-sonnet-4-20250514': {\n        'name': 'Claude Sonnet 4',\n        'color': MODEL_COLORS['Claude Sonnet 4'],\n    },\n    'claude-opus-4-1-20250805': {\n        'name': 'Claude Opus 4.1',\n        'color': MODEL_COLORS['Claude Opus 4.1'],\n    },\n    'gpt-4.1-2025-04-14': {\n        'name': 'GPT-4.1',\n        'color': MODEL_COLORS['GPT-4.1'],\n    },\n    'gpt-4o-2024-08-06': {\n        'name': 'GPT-4o',\n        'color': MODEL_COLORS['GPT-4o'],\n    },\n}\n</pre> # Define model metadata: Display Names, Colors, and Token Costs (per 1M tokens) # Prices are examples; verify current API pricing. # Colors are obtained from MODEL_COLORS (imported from plotly_theme) MODEL_CONFIG = {     'gpt-5-2025-08-07': {         'name': 'GPT-5',         'color': MODEL_COLORS['GPT-5'],     },     'gpt-5-mini-2025-08-07': {         'name': 'GPT-5 Mini',         'color': MODEL_COLORS['GPT-5 Mini'],     },     'claude-opus-4-20250514': {         'name': 'Claude Opus 4',         'color': MODEL_COLORS['Claude Opus 4'],     },     'claude-sonnet-4-20250514': {         'name': 'Claude Sonnet 4',         'color': MODEL_COLORS['Claude Sonnet 4'],     },     'claude-opus-4-1-20250805': {         'name': 'Claude Opus 4.1',         'color': MODEL_COLORS['Claude Opus 4.1'],     },     'gpt-4.1-2025-04-14': {         'name': 'GPT-4.1',         'color': MODEL_COLORS['GPT-4.1'],     },     'gpt-4o-2024-08-06': {         'name': 'GPT-4o',         'color': MODEL_COLORS['GPT-4o'],     }, } In\u00a0[\u00a0]: Copied! <pre># ============================================================================\n# DATA LOADING AND MODEL EVALUATION\n# ============================================================================\n\nall_metrics = {}  # model_name -&gt; paper_doi -&gt; {field: score}\nall_precs_and_recalls = {}\n\n# Evaluate all models\nfor model_dir in tqdm(\n    EXTRACTIONS_DIR.iterdir(), total=len([d for d in EXTRACTIONS_DIR.iterdir()])\n):\n    if not model_dir.is_dir() or model_dir == 'humans':\n        continue\n\n    model_name = model_dir.name\n    print(f'Evaluating model: {model_name}')\n\n    pipeline = ExtractionPipeline(\n        model_name=model_name,\n        preprocessor='pymupdf',\n        postprocessor='NONE',\n        cache_dir='',\n        use_cache=True,\n    )\n    model_metrics, avg_recalls, avg_precisions = pipeline._evaluate_multiple(\n        model_dir, GROUND_TRUTH_DIR\n    )\n\n    all_precs_and_recalls[model_name] = {\n        'precision': avg_precisions,\n        'recall': avg_recalls,\n    }\n    all_metrics[model_name] = model_metrics\n\n# Rename models to readable names\nmodel_name_map = {\n    'claude-opus-4-1-20250805': 'Claude Opus 4.1',\n    'claude-opus-4-20250514': 'Claude Opus 4',\n    'claude-sonnet-4-20250514': 'Claude Sonnet 4',\n    'gpt-4.1-2025-04-14': 'GPT-4.1',\n    'gpt-4o-2024-08-06': 'GPT-4o',\n    'gpt-5-2025-08-07': 'GPT-5',\n    'gpt-5-mini-2025-08-07': 'GPT-5 Mini',\n}\n\nall_metrics = {model_name_map.get(k, k): v for k, v in all_metrics.items()}\n</pre> # ============================================================================ # DATA LOADING AND MODEL EVALUATION # ============================================================================  all_metrics = {}  # model_name -&gt; paper_doi -&gt; {field: score} all_precs_and_recalls = {}  # Evaluate all models for model_dir in tqdm(     EXTRACTIONS_DIR.iterdir(), total=len([d for d in EXTRACTIONS_DIR.iterdir()]) ):     if not model_dir.is_dir() or model_dir == 'humans':         continue      model_name = model_dir.name     print(f'Evaluating model: {model_name}')      pipeline = ExtractionPipeline(         model_name=model_name,         preprocessor='pymupdf',         postprocessor='NONE',         cache_dir='',         use_cache=True,     )     model_metrics, avg_recalls, avg_precisions = pipeline._evaluate_multiple(         model_dir, GROUND_TRUTH_DIR     )      all_precs_and_recalls[model_name] = {         'precision': avg_precisions,         'recall': avg_recalls,     }     all_metrics[model_name] = model_metrics  # Rename models to readable names model_name_map = {     'claude-opus-4-1-20250805': 'Claude Opus 4.1',     'claude-opus-4-20250514': 'Claude Opus 4',     'claude-sonnet-4-20250514': 'Claude Sonnet 4',     'gpt-4.1-2025-04-14': 'GPT-4.1',     'gpt-4o-2024-08-06': 'GPT-4o',     'gpt-5-2025-08-07': 'GPT-5',     'gpt-5-mini-2025-08-07': 'GPT-5 Mini', }  all_metrics = {model_name_map.get(k, k): v for k, v in all_metrics.items()} In\u00a0[32]: Copied! <pre># ============================================================================\n# HELPER FUNCTIONS (DATAFRAME VERSION)\n# ============================================================================\n\n\ndef metrics_to_dataframe(metrics_dict):\n    \"\"\"\n    Convert nested metrics dictionary to a flat DataFrame.\n\n    Returns:\n        DataFrame with columns: model, paper, field, TP, FP, FN\n    \"\"\"\n    rows = []\n    for model, papers in metrics_dict.items():\n        for paper, fields in papers.items():\n            for field, values in fields.items():\n                if isinstance(values, dict):\n                    rows.append(\n                        {\n                            'model': model,\n                            'paper': paper,\n                            'field': field,\n                            'TP': values.get('TP', 0.0),\n                            'FP': values.get('FP', 0.0),\n                            'FN': values.get('FN', 0.0),\n                        }\n                    )\n    return pd.DataFrame(rows)\n\n\ndef add_field_categories(df):\n    \"\"\"Add aggregation category for each field.\"\"\"\n\n    def categorize(field):  # noqa: PLR0911\n        if field.endswith(':unit'):\n            return 'units'\n        field_lower = field.lower()\n        if 'composition' in field_lower:\n            return 'composition'\n        if 'stability' in field_lower:\n            return 'stability'\n        if 'deposition' in field_lower:\n            return 'deposition'\n        if 'layers' in field_lower:\n            return 'layers'\n        if 'light' in field_lower:\n            return 'light'\n        # Clean up individual fields\n        if any(\n            x in field\n            for x in ['averaged_quantities', 'number_devices', 'encapsulated']\n        ):\n            return None\n        return field.replace('_', ' ').split(':value')[0]\n\n    df['category'] = df['field'].apply(categorize)\n    return df[df['category'].notna()]\n\n\ndef calculate_metrics(df, metric_type='recall'):\n    \"\"\"\n    Calculate precision or recall for each row.\n\n    Args:\n        df: DataFrame with TP, FP, FN columns\n        metric_type: 'recall' or 'precision'\n    \"\"\"\n    if metric_type == 'recall':\n        df['score'] = df.apply(\n            lambda row: row['TP'] / (row['TP'] + row['FN'])\n            if (row['TP'] + row['FN']) &gt; 0\n            else np.nan,\n            axis=1,\n        )\n    else:  # precision\n        df['score'] = df.apply(\n            lambda row: row['TP'] / (row['TP'] + row['FP'])\n            if (row['TP'] + row['FP']) &gt; 0\n            else np.nan,\n            axis=1,\n        )\n    return df\n</pre> # ============================================================================ # HELPER FUNCTIONS (DATAFRAME VERSION) # ============================================================================   def metrics_to_dataframe(metrics_dict):     \"\"\"     Convert nested metrics dictionary to a flat DataFrame.      Returns:         DataFrame with columns: model, paper, field, TP, FP, FN     \"\"\"     rows = []     for model, papers in metrics_dict.items():         for paper, fields in papers.items():             for field, values in fields.items():                 if isinstance(values, dict):                     rows.append(                         {                             'model': model,                             'paper': paper,                             'field': field,                             'TP': values.get('TP', 0.0),                             'FP': values.get('FP', 0.0),                             'FN': values.get('FN', 0.0),                         }                     )     return pd.DataFrame(rows)   def add_field_categories(df):     \"\"\"Add aggregation category for each field.\"\"\"      def categorize(field):  # noqa: PLR0911         if field.endswith(':unit'):             return 'units'         field_lower = field.lower()         if 'composition' in field_lower:             return 'composition'         if 'stability' in field_lower:             return 'stability'         if 'deposition' in field_lower:             return 'deposition'         if 'layers' in field_lower:             return 'layers'         if 'light' in field_lower:             return 'light'         # Clean up individual fields         if any(             x in field             for x in ['averaged_quantities', 'number_devices', 'encapsulated']         ):             return None         return field.replace('_', ' ').split(':value')[0]      df['category'] = df['field'].apply(categorize)     return df[df['category'].notna()]   def calculate_metrics(df, metric_type='recall'):     \"\"\"     Calculate precision or recall for each row.      Args:         df: DataFrame with TP, FP, FN columns         metric_type: 'recall' or 'precision'     \"\"\"     if metric_type == 'recall':         df['score'] = df.apply(             lambda row: row['TP'] / (row['TP'] + row['FN'])             if (row['TP'] + row['FN']) &gt; 0             else np.nan,             axis=1,         )     else:  # precision         df['score'] = df.apply(             lambda row: row['TP'] / (row['TP'] + row['FP'])             if (row['TP'] + row['FP']) &gt; 0             else np.nan,             axis=1,         )     return df In\u00a0[33]: Copied! <pre># ============================================================================\n# BAR CHART: OVERALL MODEL PERFORMANCE (DATAFRAME VERSION)\n# ============================================================================\n\n# Calculate overall metrics per model\ndf = metrics_to_dataframe(all_metrics)\n\ndf_doi = (\n    df.groupby(['model', 'paper'])\n    .agg({'TP': 'sum', 'FP': 'sum', 'FN': 'sum'})\n    .reset_index()\n)\n\noverall = df_doi.groupby('model').sum().reset_index()\noverall['precision'] = overall['TP'] / (overall['TP'] + overall['FP'])\noverall['recall'] = overall['TP'] / (overall['TP'] + overall['FN'])\n\n\n# Plot\nx = np.arange(len(overall))\nwidth = 0.35\n\noverall_performance_fig, ax = plt.subplots(figsize=(7.2, 4))\nrects1 = ax.bar(x - width / 2, overall['precision'], width, label='Precision')\nrects2 = ax.bar(x + width / 2, overall['recall'], width, label='Recall')\n\nax.set_ylabel('Score')\nax.set_title('Model Performances')\nax.set_xticks(x)\nax.set_xticklabels(overall['model'], rotation=45)\nax.set_yticks(np.arange(0, 1.1, 0.4))\nax.set_yticklabels([f'{y:.1f}' for y in np.arange(0, 1.1, 0.4)])\nax.set_ylim(0, 1.2)\nax.legend(loc='upper right', ncol=2, frameon=False)\n\n# Add value labels\nfor rects, values in [(rects1, overall['precision']), (rects2, overall['recall'])]:\n    for rect, val in zip(rects, values):\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            height + 0.02,\n            f'{val:.2f}',\n            ha='center',\n            va='bottom',\n        )\n\nplt.tight_layout()\noverall_performance_fig.savefig('overall_performance_bars.pdf')\n</pre> # ============================================================================ # BAR CHART: OVERALL MODEL PERFORMANCE (DATAFRAME VERSION) # ============================================================================  # Calculate overall metrics per model df = metrics_to_dataframe(all_metrics)  df_doi = (     df.groupby(['model', 'paper'])     .agg({'TP': 'sum', 'FP': 'sum', 'FN': 'sum'})     .reset_index() )  overall = df_doi.groupby('model').sum().reset_index() overall['precision'] = overall['TP'] / (overall['TP'] + overall['FP']) overall['recall'] = overall['TP'] / (overall['TP'] + overall['FN'])   # Plot x = np.arange(len(overall)) width = 0.35  overall_performance_fig, ax = plt.subplots(figsize=(7.2, 4)) rects1 = ax.bar(x - width / 2, overall['precision'], width, label='Precision') rects2 = ax.bar(x + width / 2, overall['recall'], width, label='Recall')  ax.set_ylabel('Score') ax.set_title('Model Performances') ax.set_xticks(x) ax.set_xticklabels(overall['model'], rotation=45) ax.set_yticks(np.arange(0, 1.1, 0.4)) ax.set_yticklabels([f'{y:.1f}' for y in np.arange(0, 1.1, 0.4)]) ax.set_ylim(0, 1.2) ax.legend(loc='upper right', ncol=2, frameon=False)  # Add value labels for rects, values in [(rects1, overall['precision']), (rects2, overall['recall'])]:     for rect, val in zip(rects, values):         height = rect.get_height()         ax.text(             rect.get_x() + rect.get_width() / 2.0,             height + 0.02,             f'{val:.2f}',             ha='center',             va='bottom',         )  plt.tight_layout() overall_performance_fig.savefig('overall_performance_bars.pdf') In\u00a0[50]: Copied! <pre># Convert to DataFrame and calculate precisions\ndf = metrics_to_dataframe(all_metrics)\ndf = add_field_categories(df)\ndf_recall = df.copy()\ndf_precision = df.copy()\ndf_precision = calculate_metrics(df_precision, metric_type='precision')\ndf_recall = calculate_metrics(df_recall, metric_type='recall')\n\n# Aggregate by model and category\naggregated_precision = (\n    df_precision.groupby(['model', 'category'])['score'].mean().reset_index()\n)\naggregated_recall = (\n    df_recall.groupby(['model', 'category'])['score'].mean().reset_index()\n)\n\n# Pivot for radar plot\npivot_precision = aggregated_precision.pivot(\n    index='model', columns='category', values='score'\n).fillna(0)\npivot_recall = aggregated_recall.pivot(\n    index='model', columns='category', values='score'\n).fillna(0)\n\n\n# Prepare Fields\nfields = sorted(pivot_precision.columns)\nnum_fields = len(fields)\nangles = [n / float(num_fields) * 2 * pi for n in range(num_fields)]\nangles += angles[:1]  # Close the loop\n\n\n# Formatter\ndef format_field_name(field):\n    field_map = {\n        'pce': 'PCE',\n        'jsc': r'$J_\\mathrm{sc}$',\n        'ff': 'FF',\n        'voc': r'$V_\\mathrm{oc}$',\n        'active area': 'Active Area',\n        'layers': 'Layers',\n        'device architecture': 'Architecture',\n        'composition': 'Composition',\n        'deposition': 'Deposition',\n        'stability': 'Stability',\n        'units': 'Units',\n        'light': 'Light',\n    }\n    return field_map.get(field.lower(), field)\n\n\nfield_labels = [format_field_name(f) for f in fields]\n\n# ============================================================================\n# 3. PLOTTING FUNCTION\n# ============================================================================\n\n\ndef draw_radar(ax, pivot_df, title, letter_label, ylim=(0, 1)):\n    # A. Setup Grid &amp; Spines\n    ax.set_theta_offset(pi / 2)\n    ax.set_theta_direction(-1)\n    ax.spines['polar'].set_visible(False)\n    # Very light, thin grid\n    ax.grid(color='#D9D9D9', linestyle='--', linewidth=0.5)\n\n    # B. Plot Data (Thinner lines for clarity)\n    for model_name in pivot_df.index:\n        if model_name not in MODEL_COLORS:\n            print(f'Warning: Unknown model {model_name}, skipping.')\n            continue  # Skip unknown models\n        scores = pivot_df.loc[model_name, fields].tolist()\n        values = scores + [scores[0]]\n        color = MODEL_COLORS.get(model_name, '#333333')\n\n        ax.plot(\n            angles,\n            values,\n            linewidth=1.0,  # Nature standard line width\n            linestyle='-',\n            marker='o',\n            markersize=4.5,  # Small, crisp markers\n            markeredgewidth=0,  # No border on markers for cleanliness\n            label=model_name,\n            color=color,\n        )\n\n        # Extremely subtle fill to keep grid visible\n        ax.fill(angles, values, color=color, alpha=0.01)\n\n    # C. Smart Labels (Compact &amp; Aligned)\n    ax.set_xticks(angles[:-1])\n    ax.set_xticklabels([])\n\n    # Distance: Closer to center because fonts are smaller (1.15 is sufficient now)\n    label_distance = 1.15\n\n    for angle, label in zip(angles[:-1], field_labels):\n        angle_norm = angle % (2 * np.pi)\n\n        if np.isclose(angle_norm, 0):  # 12 o'clock\n            ha, va = 'center', 'bottom'\n        elif 0 &lt; angle_norm &lt; np.pi:  # Right\n            ha, va = 'left', 'center'\n        elif np.isclose(angle_norm, np.pi):  # 6 o'clock\n            ha, va = 'center', 'top'\n        else:  # Left\n            ha, va = 'right', 'center'\n\n        # Font size 7pt (standard text)\n        ax.text(\n            angle,\n            label_distance,\n            label,\n            horizontalalignment=ha,\n            verticalalignment=va,\n            color='black',\n        )\n\n    # D. Y-Axis (Radial) - Minimalist\n    ax.set_ylim(ylim)\n    ax.set_yticklabels([])\n\n    # Minimal ticks: Only show max and mid-point to reduce clutter\n    mid_val = (ylim[0] + ylim[1]) / 2\n    ticks = [mid_val, ylim[1]]\n    r_angle = np.deg2rad(22.5)  # Draw ticks at 22.5 deg to avoid crossing vertical axis\n\n    for y in ticks:\n        ax.text(\n            r_angle,\n            y,\n            f'{y:.1f}',\n            color='#404040',\n            size=6,\n            ha='center',\n            va='center',\n            bbox=dict(facecolor='white', edgecolor='none', alpha=0.5, pad=0.5),\n        )\n\n    ax.set_title(title, y=1.2, fontweight='bold', color='black')\n\n    ax.text(\n        -0.3,\n        1.3,\n        letter_label,\n        transform=ax.transAxes,\n        fontsize=14,\n        fontweight='bold',\n        va='top',\n        ha='right',\n        color='black',\n    )\n\n\n# ============================================================================\n# 4. GENERATE FIGURE\n# ============================================================================\n\n# Width: 7.2 inches (183mm) exactly\n# Height: 3.5 inches (approx 89mm) - compact aspect ratio\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.2, 3.5), subplot_kw={'polar': True})\n\n# Draw Plots\ndraw_radar(ax1, pivot_precision, 'Precision', 'a', ylim=(0.0, 1.0))\ndraw_radar(ax2, pivot_recall, 'Recall', 'b', ylim=(0.0, 1.0))\n\n# Shared Legend\nhandles, labels = ax1.get_legend_handles_labels()\n# Very compact legend\nfig.legend(\n    handles,\n    labels,\n    loc='lower center',\n    bbox_to_anchor=(0.5, -0.1),\n    ncol=4,  # 4 columns to be wider/flatter\n    frameon=False,\n    columnspacing=1.5,\n    handletextpad=0.5,\n)\n\n# Tight layout with manual adjustments for the legend space\nplt.subplots_adjust(\n    top=0.82,\n    bottom=0.20,  # Space for legend\n    left=0.08,\n    right=0.92,\n    wspace=0.85,\n)\n\n# Save as PDF (Vector format required by Nature)\nplt.savefig('performance_radar_plot.pdf', dpi=300, format='pdf')\n</pre> # Convert to DataFrame and calculate precisions df = metrics_to_dataframe(all_metrics) df = add_field_categories(df) df_recall = df.copy() df_precision = df.copy() df_precision = calculate_metrics(df_precision, metric_type='precision') df_recall = calculate_metrics(df_recall, metric_type='recall')  # Aggregate by model and category aggregated_precision = (     df_precision.groupby(['model', 'category'])['score'].mean().reset_index() ) aggregated_recall = (     df_recall.groupby(['model', 'category'])['score'].mean().reset_index() )  # Pivot for radar plot pivot_precision = aggregated_precision.pivot(     index='model', columns='category', values='score' ).fillna(0) pivot_recall = aggregated_recall.pivot(     index='model', columns='category', values='score' ).fillna(0)   # Prepare Fields fields = sorted(pivot_precision.columns) num_fields = len(fields) angles = [n / float(num_fields) * 2 * pi for n in range(num_fields)] angles += angles[:1]  # Close the loop   # Formatter def format_field_name(field):     field_map = {         'pce': 'PCE',         'jsc': r'$J_\\mathrm{sc}$',         'ff': 'FF',         'voc': r'$V_\\mathrm{oc}$',         'active area': 'Active Area',         'layers': 'Layers',         'device architecture': 'Architecture',         'composition': 'Composition',         'deposition': 'Deposition',         'stability': 'Stability',         'units': 'Units',         'light': 'Light',     }     return field_map.get(field.lower(), field)   field_labels = [format_field_name(f) for f in fields]  # ============================================================================ # 3. PLOTTING FUNCTION # ============================================================================   def draw_radar(ax, pivot_df, title, letter_label, ylim=(0, 1)):     # A. Setup Grid &amp; Spines     ax.set_theta_offset(pi / 2)     ax.set_theta_direction(-1)     ax.spines['polar'].set_visible(False)     # Very light, thin grid     ax.grid(color='#D9D9D9', linestyle='--', linewidth=0.5)      # B. Plot Data (Thinner lines for clarity)     for model_name in pivot_df.index:         if model_name not in MODEL_COLORS:             print(f'Warning: Unknown model {model_name}, skipping.')             continue  # Skip unknown models         scores = pivot_df.loc[model_name, fields].tolist()         values = scores + [scores[0]]         color = MODEL_COLORS.get(model_name, '#333333')          ax.plot(             angles,             values,             linewidth=1.0,  # Nature standard line width             linestyle='-',             marker='o',             markersize=4.5,  # Small, crisp markers             markeredgewidth=0,  # No border on markers for cleanliness             label=model_name,             color=color,         )          # Extremely subtle fill to keep grid visible         ax.fill(angles, values, color=color, alpha=0.01)      # C. Smart Labels (Compact &amp; Aligned)     ax.set_xticks(angles[:-1])     ax.set_xticklabels([])      # Distance: Closer to center because fonts are smaller (1.15 is sufficient now)     label_distance = 1.15      for angle, label in zip(angles[:-1], field_labels):         angle_norm = angle % (2 * np.pi)          if np.isclose(angle_norm, 0):  # 12 o'clock             ha, va = 'center', 'bottom'         elif 0 &lt; angle_norm &lt; np.pi:  # Right             ha, va = 'left', 'center'         elif np.isclose(angle_norm, np.pi):  # 6 o'clock             ha, va = 'center', 'top'         else:  # Left             ha, va = 'right', 'center'          # Font size 7pt (standard text)         ax.text(             angle,             label_distance,             label,             horizontalalignment=ha,             verticalalignment=va,             color='black',         )      # D. Y-Axis (Radial) - Minimalist     ax.set_ylim(ylim)     ax.set_yticklabels([])      # Minimal ticks: Only show max and mid-point to reduce clutter     mid_val = (ylim[0] + ylim[1]) / 2     ticks = [mid_val, ylim[1]]     r_angle = np.deg2rad(22.5)  # Draw ticks at 22.5 deg to avoid crossing vertical axis      for y in ticks:         ax.text(             r_angle,             y,             f'{y:.1f}',             color='#404040',             size=6,             ha='center',             va='center',             bbox=dict(facecolor='white', edgecolor='none', alpha=0.5, pad=0.5),         )      ax.set_title(title, y=1.2, fontweight='bold', color='black')      ax.text(         -0.3,         1.3,         letter_label,         transform=ax.transAxes,         fontsize=14,         fontweight='bold',         va='top',         ha='right',         color='black',     )   # ============================================================================ # 4. GENERATE FIGURE # ============================================================================  # Width: 7.2 inches (183mm) exactly # Height: 3.5 inches (approx 89mm) - compact aspect ratio fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.2, 3.5), subplot_kw={'polar': True})  # Draw Plots draw_radar(ax1, pivot_precision, 'Precision', 'a', ylim=(0.0, 1.0)) draw_radar(ax2, pivot_recall, 'Recall', 'b', ylim=(0.0, 1.0))  # Shared Legend handles, labels = ax1.get_legend_handles_labels() # Very compact legend fig.legend(     handles,     labels,     loc='lower center',     bbox_to_anchor=(0.5, -0.1),     ncol=4,  # 4 columns to be wider/flatter     frameon=False,     columnspacing=1.5,     handletextpad=0.5, )  # Tight layout with manual adjustments for the legend space plt.subplots_adjust(     top=0.82,     bottom=0.20,  # Space for legend     left=0.08,     right=0.92,     wspace=0.85, )  # Save as PDF (Vector format required by Nature) plt.savefig('performance_radar_plot.pdf', dpi=300, format='pdf') In\u00a0[35]: Copied! <pre>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.2, 2.5), sharey=True)\n\nmodels = [\n    'Claude Sonnet 4',\n    'Claude Opus 4.1',\n    'Claude Opus 4',\n    'GPT-5',\n    'GPT-5 Mini',\n    'GPT-4.1',\n    'GPT-4o',\n]\nmodels = [m for m in models if m in pivot_precision.index]\n\nx = np.arange(len(models))\nwidth = 0.6\n\n# Precision\nmeans_p = [pivot_precision.loc[m].mean() for m in models]\nstds_p = [pivot_precision.loc[m].std() for m in models]\nax1.barh(\n    x,\n    means_p,\n    xerr=stds_p,\n    height=width,\n    color='#4DBBD5',\n    capsize=2,\n    error_kw={'linewidth': 0.8},\n)\nax1.set_xlim(0, 1.1)\nax1.set_xlabel('Precision', fontsize=9)\nax1.set_yticks(x)\nax1.set_yticklabels(models, fontsize=8)\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.text(-0.25, 1.05, 'a', transform=ax1.transAxes, fontsize=12, fontweight='bold')\n\n# Recall\nmeans_r = [pivot_recall.loc[m].mean() for m in models]\nstds_r = [pivot_recall.loc[m].std() for m in models]\nax2.barh(\n    x,\n    means_r,\n    xerr=stds_r,\n    height=width,\n    color='#E64B35',\n    capsize=2,\n    error_kw={'linewidth': 0.8},\n)\nax2.set_xlim(0, 1.1)\nax2.set_xlabel('Recall', fontsize=9)\nax2.spines['top'].set_visible(False)\nax2.spines['right'].set_visible(False)\nax2.text(-0.1, 1.05, 'b', transform=ax2.transAxes, fontsize=12, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('performance_bars.pdf', dpi=300)\n</pre> fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.2, 2.5), sharey=True)  models = [     'Claude Sonnet 4',     'Claude Opus 4.1',     'Claude Opus 4',     'GPT-5',     'GPT-5 Mini',     'GPT-4.1',     'GPT-4o', ] models = [m for m in models if m in pivot_precision.index]  x = np.arange(len(models)) width = 0.6  # Precision means_p = [pivot_precision.loc[m].mean() for m in models] stds_p = [pivot_precision.loc[m].std() for m in models] ax1.barh(     x,     means_p,     xerr=stds_p,     height=width,     color='#4DBBD5',     capsize=2,     error_kw={'linewidth': 0.8}, ) ax1.set_xlim(0, 1.1) ax1.set_xlabel('Precision', fontsize=9) ax1.set_yticks(x) ax1.set_yticklabels(models, fontsize=8) ax1.spines['top'].set_visible(False) ax1.spines['right'].set_visible(False) ax1.text(-0.25, 1.05, 'a', transform=ax1.transAxes, fontsize=12, fontweight='bold')  # Recall means_r = [pivot_recall.loc[m].mean() for m in models] stds_r = [pivot_recall.loc[m].std() for m in models] ax2.barh(     x,     means_r,     xerr=stds_r,     height=width,     color='#E64B35',     capsize=2,     error_kw={'linewidth': 0.8}, ) ax2.set_xlim(0, 1.1) ax2.set_xlabel('Recall', fontsize=9) ax2.spines['top'].set_visible(False) ax2.spines['right'].set_visible(False) ax2.text(-0.1, 1.05, 'b', transform=ax2.transAxes, fontsize=12, fontweight='bold')  plt.tight_layout() plt.savefig('performance_bars.pdf', dpi=300) In\u00a0[\u00a0]: Copied! <pre>pipeline = ExtractionPipeline(\n    model_name='Consensus',\n    preprocessor='pymupdf',\n    postprocessor='NONE',\n    cache_dir='',\n    use_cache=True,\n)\nauthors_metrics, authors_recalls, authors_precisions = pipeline._evaluate_multiple(\n    EXPERTS_DIR, GROUND_TRUTH_DIR\n)\n</pre> pipeline = ExtractionPipeline(     model_name='Consensus',     preprocessor='pymupdf',     postprocessor='NONE',     cache_dir='',     use_cache=True, ) authors_metrics, authors_recalls, authors_precisions = pipeline._evaluate_multiple(     EXPERTS_DIR, GROUND_TRUTH_DIR ) In\u00a0[37]: Copied! <pre>all_metrics['Consensus'] = authors_metrics\nexperts_included_df = metrics_to_dataframe(all_metrics)\n\n# 1. Get the set of papers that appear with model == \"Consensus\"\nexpert_papers = set(\n    experts_included_df.loc[experts_included_df['model'] == 'Consensus', 'paper']\n)\n\n# 2. Filter the DataFrame\nfiltered_df = experts_included_df[\n    (experts_included_df['model'] == 'Consensus')\n    | (\n        (experts_included_df['model'] != 'Consensus')\n        &amp; (experts_included_df['paper'].isin(expert_papers))\n    )\n]\n</pre> all_metrics['Consensus'] = authors_metrics experts_included_df = metrics_to_dataframe(all_metrics)  # 1. Get the set of papers that appear with model == \"Consensus\" expert_papers = set(     experts_included_df.loc[experts_included_df['model'] == 'Consensus', 'paper'] )  # 2. Filter the DataFrame filtered_df = experts_included_df[     (experts_included_df['model'] == 'Consensus')     | (         (experts_included_df['model'] != 'Consensus')         &amp; (experts_included_df['paper'].isin(expert_papers))     ) ] In\u00a0[38]: Copied! <pre># Group by paper and model, sum TP and FP\nmicro_precision_df = (\n    filtered_df.groupby(['paper', 'model'])[['TP', 'FP']].sum().reset_index()\n)\n\n# Compute micro-precision\nmicro_precision_df['precision'] = micro_precision_df['TP'] / (\n    micro_precision_df['TP'] + micro_precision_df['FP']\n)\n</pre> # Group by paper and model, sum TP and FP micro_precision_df = (     filtered_df.groupby(['paper', 'model'])[['TP', 'FP']].sum().reset_index() )  # Compute micro-precision micro_precision_df['precision'] = micro_precision_df['TP'] / (     micro_precision_df['TP'] + micro_precision_df['FP'] ) In\u00a0[39]: Copied! <pre># Only select the papers where both LLMs and experts exist\npapers_with_both = micro_precision_df['paper'].value_counts()\npapers_with_both = papers_with_both[papers_with_both &gt; 1].index\ndf_plot = micro_precision_df[micro_precision_df['paper'].isin(papers_with_both)]\n\n# Pivot data so each row is a DOI and each column is a model\ndf_pivot = df_plot.pivot(\n    index='paper', columns='model', values='precision'\n).reset_index()\n\n# Melt data for dabest\ndf_melt = df_pivot.melt(id_vars='paper', var_name='model', value_name='precision')\n\n# Create a dabest object using authors as the control\ndabest_data = dabest.load(\n    data=df_melt,\n    x='model',\n    y='precision',\n    idx=(\n        'Consensus',\n        'GPT-4.1',\n        'Claude Opus 4',\n        'GPT-4o',\n        'GPT-5',\n        'Claude Sonnet 4',\n        'Claude Opus 4.1',\n        'GPT-5 Mini',\n    ),\n)\n\n# Plot mean difference against authors\nplt.figure()\nmean_fig = dabest_data.mean_diff.plot(\n    raw_marker_size=4,\n    custom_palette=MODEL_COLORS,\n)\n</pre> # Only select the papers where both LLMs and experts exist papers_with_both = micro_precision_df['paper'].value_counts() papers_with_both = papers_with_both[papers_with_both &gt; 1].index df_plot = micro_precision_df[micro_precision_df['paper'].isin(papers_with_both)]  # Pivot data so each row is a DOI and each column is a model df_pivot = df_plot.pivot(     index='paper', columns='model', values='precision' ).reset_index()  # Melt data for dabest df_melt = df_pivot.melt(id_vars='paper', var_name='model', value_name='precision')  # Create a dabest object using authors as the control dabest_data = dabest.load(     data=df_melt,     x='model',     y='precision',     idx=(         'Consensus',         'GPT-4.1',         'Claude Opus 4',         'GPT-4o',         'GPT-5',         'Claude Sonnet 4',         'Claude Opus 4.1',         'GPT-5 Mini',     ), )  # Plot mean difference against authors plt.figure() mean_fig = dabest_data.mean_diff.plot(     raw_marker_size=4,     custom_palette=MODEL_COLORS, ) <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>human_metrics = {}  # model_name -&gt; paper_doi -&gt; {field: score}\nhuman_precs_and_recalls = {}\n\nHUMANS_DIR = EXTRACTIONS_DIR / 'humans'\nDEV_DIR = DATA_DIR / 'ground_truth' / 'dev'\n# Evaluate all models\nfor model_dir in HUMANS_DIR.iterdir():\n    if not model_dir.is_dir():\n        continue\n\n    model_name = model_dir.name\n    print(f'Evaluating model: {model_name}')\n\n    pipeline = ExtractionPipeline(\n        model_name=model_name,\n        preprocessor='pymupdf',\n        postprocessor='NONE',\n        cache_dir='',\n        use_cache=True,\n    )\n    model_metrics, avg_recalls, avg_precisions = pipeline._evaluate_multiple(\n        model_dir, DEV_DIR\n    )\n\n    human_precs_and_recalls[model_name] = {\n        'precision': avg_precisions,\n        'recall': avg_recalls,\n    }\n    human_metrics[model_name] = model_metrics\n\nsonnet_4_metrics, s_rec, s_prec = pipeline._evaluate_multiple(\n    EXTRACTIONS_DIR / 'claude-sonnet-4-20250514/', DEV_DIR\n)\nhuman_metrics['Claude Sonnet 4'] = sonnet_4_metrics\n</pre> human_metrics = {}  # model_name -&gt; paper_doi -&gt; {field: score} human_precs_and_recalls = {}  HUMANS_DIR = EXTRACTIONS_DIR / 'humans' DEV_DIR = DATA_DIR / 'ground_truth' / 'dev' # Evaluate all models for model_dir in HUMANS_DIR.iterdir():     if not model_dir.is_dir():         continue      model_name = model_dir.name     print(f'Evaluating model: {model_name}')      pipeline = ExtractionPipeline(         model_name=model_name,         preprocessor='pymupdf',         postprocessor='NONE',         cache_dir='',         use_cache=True,     )     model_metrics, avg_recalls, avg_precisions = pipeline._evaluate_multiple(         model_dir, DEV_DIR     )      human_precs_and_recalls[model_name] = {         'precision': avg_precisions,         'recall': avg_recalls,     }     human_metrics[model_name] = model_metrics  sonnet_4_metrics, s_rec, s_prec = pipeline._evaluate_multiple(     EXTRACTIONS_DIR / 'claude-sonnet-4-20250514/', DEV_DIR ) human_metrics['Claude Sonnet 4'] = sonnet_4_metrics In\u00a0[41]: Copied! <pre>from collections import defaultdict\n\ndoi_to_groups = defaultdict(list)\n\nfor group, dois in human_metrics.items():\n    for doi in dois:\n        doi_to_groups[doi].append(group)\n\nprint('DOI matches across groups:\\n')\nfor doi, groups in doi_to_groups.items():\n    if len(groups) &gt; 1:\n        print(f'{doi} -&gt; {\", \".join(groups)}')\n</pre> from collections import defaultdict  doi_to_groups = defaultdict(list)  for group, dois in human_metrics.items():     for doi in dois:         doi_to_groups[doi].append(group)  print('DOI matches across groups:\\n') for doi, groups in doi_to_groups.items():     if len(groups) &gt; 1:         print(f'{doi} -&gt; {\", \".join(groups)}') <pre>DOI matches across groups:\n\n10.1021--acs.chemmater.8b01521.json -&gt; Consensus, Kit, Seal, Robin, Claude Sonnet 4\n10.1021--acsaem.9b01928.json -&gt; Consensus, Bee, Panda, Seal, Dove, Claude Sonnet 4\n10.1002--adfm.201904856.json -&gt; Consensus, Bee, Fox, Kit, Claude Sonnet 4\n10.1039--c7nr04692h.json -&gt; Consensus, Bear, Kit, Dove, Claude Sonnet 4\n10.1002--adma.202302143.json -&gt; Consensus, Bee, Robin, Claude Sonnet 4\n10.1002--solr.201900370.json -&gt; Consensus, Bear, Lark, Kit, Claude Sonnet 4\n10.1039--c5dt02388b.json -&gt; Consensus, Bear, Kit, Otto, Claude Sonnet 4\n10.1016--j.nanoen.2016.05.023.json -&gt; Consensus, Bear, Panda, Lark, Seal, Dove, Hawk, Claude Sonnet 4\n10.1002--adfm.201500335.json -&gt; Consensus, Bee, Panda, Dove, Claude Sonnet 4\n10.1016--j.matlet.2016.07.004.json -&gt; Consensus, Bear, Lark, Dove, Claude Sonnet 4\n</pre> In\u00a0[48]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom math import pi\nfrom matplotlib.lines import Line2D\n\n# ---------------------------------------------------------------------\n# 1) DATA PREP\n# ---------------------------------------------------------------------\ndf = metrics_to_dataframe(human_metrics)\ndf = add_field_categories(df)\ndf = calculate_metrics(df, metric_type='precision')\n\naggregated = df.groupby(['model', 'category'])['score'].mean().reset_index()\npivot_df = aggregated.pivot(index='model', columns='category', values='score').fillna(0)\n\nfields = sorted(pivot_df.columns)\nnum_fields = len(fields)\nangles = [n / float(num_fields) * 2 * pi for n in range(num_fields)]\nangles += angles[:1]\n\nfield_labels = [format_field_name(f) for f in fields]\n\n# Separate human annotators from models\nhighlight_models = ['Consensus', 'Claude Sonnet 4']\nhuman_names = [m for m in pivot_df.index if m not in highlight_models]\n\n\n# ---------------------------------------------------------------------\n# 2) PLOTTING\n# ---------------------------------------------------------------------\ndef draw_radar_with_humans(ax, pivot_df, title='Precision', ylim=(0.3, 1.0)):\n    # Grid &amp; spines\n    ax.set_theta_offset(pi / 2)\n    ax.set_theta_direction(-1)\n    ax.spines['polar'].set_visible(False)\n    ax.grid(color='#D9D9D9', linestyle='--', linewidth=0.5)\n\n    # A) Plot individual humans (light gray, visible but background)\n    for human in human_names:\n        scores = pivot_df.loc[human, fields].tolist()\n        values = scores + [scores[0]]\n        ax.plot(\n            angles,\n            values,\n            color='#AAAAAA',\n            linewidth=1.0,\n            alpha=0.8,\n            linestyle='-',\n            zorder=1,\n        )\n\n    # B) Plot highlighted models\n    model_styles = {\n        'Claude Sonnet 4': {'color': '#E67E22', 'lw': 2.0, 'ms': 3.5, 'zorder': 10},\n        'Consensus': {'color': '#1A5276', 'lw': 1.8, 'ms': 3.0, 'zorder': 9},\n    }\n\n    for model_name in highlight_models:\n        if model_name not in pivot_df.index:\n            continue\n        scores = pivot_df.loc[model_name, fields].tolist()\n        values = scores + [scores[0]]\n        style = model_styles.get(model_name)\n\n        ax.plot(\n            angles,\n            values,\n            linewidth=style['lw'],\n            linestyle='-',\n            marker='o',\n            markersize=style['ms'],\n            markeredgewidth=0,\n            color=style['color'],\n            zorder=style['zorder'],\n        )\n\n    # C) Axis limits\n    ax.set_ylim(ylim)\n\n    # D) X-axis labels (manual placement)\n    ax.set_xticks(angles[:-1])\n    ax.set_xticklabels([])\n\n    label_distance = 1.12\n    for angle, label in zip(angles[:-1], field_labels):\n        angle_norm = angle % (2 * np.pi)\n        if np.isclose(angle_norm, 0):\n            ha, va = 'center', 'bottom'\n        elif 0 &lt; angle_norm &lt; np.pi:\n            ha, va = 'left', 'center'\n        elif np.isclose(angle_norm, np.pi):\n            ha, va = 'center', 'top'\n        else:\n            ha, va = 'right', 'center'\n\n        ax.text(\n            angle,\n            label_distance,\n            label,\n            ha=ha,\n            va=va,\n            color='black',\n            fontsize=7,\n        )\n\n    # E) Radial tick labels\n    ax.set_yticklabels([])\n    mid_val = (ylim[0] + ylim[1]) / 2\n    for y in [mid_val, ylim[1]]:\n        ax.text(\n            np.deg2rad(22.5),\n            y,\n            f'{y:.1f}',\n            color='#404040',\n            size=6,\n            ha='center',\n            va='center',\n            bbox=dict(facecolor='white', edgecolor='none', alpha=0.8, pad=0.5),\n        )\n\n    # F) Title\n    ax.set_title(title, y=1.12, fontweight='bold', fontsize=10)\n\n\n# ---------------------------------------------------------------------\n# 3) CREATE FIGURE\n# ---------------------------------------------------------------------\nfig = plt.figure(figsize=(4, 4))\nax = plt.subplot(111, polar=True)\n\ndraw_radar_with_humans(ax, pivot_df, title='Precision', ylim=(0.3, 1.0))\n\n# Custom legend\nlegend_elements = [\n    Line2D(\n        [0],\n        [0],\n        color='#E67E22',\n        linewidth=2,\n        marker='o',\n        markersize=3.5,\n        markeredgewidth=0,\n        label='Claude Sonnet 4',\n    ),\n    Line2D(\n        [0],\n        [0],\n        color='#1A5276',\n        linewidth=1.8,\n        marker='o',\n        markersize=3,\n        markeredgewidth=0,\n        label='Consensus',\n    ),\n    Line2D(\n        [0], [0], color='#AAAAAA', linewidth=1.0, alpha=0.6, label='Individual humans'\n    ),\n]\n\nfig.legend(\n    handles=legend_elements,\n    loc='lower center',\n    bbox_to_anchor=(0.5, -0.02),\n    ncol=3,\n    frameon=False,\n    fontsize=7,\n    handlelength=1.5,\n    columnspacing=1.2,\n)\n\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.12)\nplt.savefig('human_precision_radar_v3.pdf', dpi=300, bbox_inches='tight')\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from math import pi from matplotlib.lines import Line2D  # --------------------------------------------------------------------- # 1) DATA PREP # --------------------------------------------------------------------- df = metrics_to_dataframe(human_metrics) df = add_field_categories(df) df = calculate_metrics(df, metric_type='precision')  aggregated = df.groupby(['model', 'category'])['score'].mean().reset_index() pivot_df = aggregated.pivot(index='model', columns='category', values='score').fillna(0)  fields = sorted(pivot_df.columns) num_fields = len(fields) angles = [n / float(num_fields) * 2 * pi for n in range(num_fields)] angles += angles[:1]  field_labels = [format_field_name(f) for f in fields]  # Separate human annotators from models highlight_models = ['Consensus', 'Claude Sonnet 4'] human_names = [m for m in pivot_df.index if m not in highlight_models]   # --------------------------------------------------------------------- # 2) PLOTTING # --------------------------------------------------------------------- def draw_radar_with_humans(ax, pivot_df, title='Precision', ylim=(0.3, 1.0)):     # Grid &amp; spines     ax.set_theta_offset(pi / 2)     ax.set_theta_direction(-1)     ax.spines['polar'].set_visible(False)     ax.grid(color='#D9D9D9', linestyle='--', linewidth=0.5)      # A) Plot individual humans (light gray, visible but background)     for human in human_names:         scores = pivot_df.loc[human, fields].tolist()         values = scores + [scores[0]]         ax.plot(             angles,             values,             color='#AAAAAA',             linewidth=1.0,             alpha=0.8,             linestyle='-',             zorder=1,         )      # B) Plot highlighted models     model_styles = {         'Claude Sonnet 4': {'color': '#E67E22', 'lw': 2.0, 'ms': 3.5, 'zorder': 10},         'Consensus': {'color': '#1A5276', 'lw': 1.8, 'ms': 3.0, 'zorder': 9},     }      for model_name in highlight_models:         if model_name not in pivot_df.index:             continue         scores = pivot_df.loc[model_name, fields].tolist()         values = scores + [scores[0]]         style = model_styles.get(model_name)          ax.plot(             angles,             values,             linewidth=style['lw'],             linestyle='-',             marker='o',             markersize=style['ms'],             markeredgewidth=0,             color=style['color'],             zorder=style['zorder'],         )      # C) Axis limits     ax.set_ylim(ylim)      # D) X-axis labels (manual placement)     ax.set_xticks(angles[:-1])     ax.set_xticklabels([])      label_distance = 1.12     for angle, label in zip(angles[:-1], field_labels):         angle_norm = angle % (2 * np.pi)         if np.isclose(angle_norm, 0):             ha, va = 'center', 'bottom'         elif 0 &lt; angle_norm &lt; np.pi:             ha, va = 'left', 'center'         elif np.isclose(angle_norm, np.pi):             ha, va = 'center', 'top'         else:             ha, va = 'right', 'center'          ax.text(             angle,             label_distance,             label,             ha=ha,             va=va,             color='black',             fontsize=7,         )      # E) Radial tick labels     ax.set_yticklabels([])     mid_val = (ylim[0] + ylim[1]) / 2     for y in [mid_val, ylim[1]]:         ax.text(             np.deg2rad(22.5),             y,             f'{y:.1f}',             color='#404040',             size=6,             ha='center',             va='center',             bbox=dict(facecolor='white', edgecolor='none', alpha=0.8, pad=0.5),         )      # F) Title     ax.set_title(title, y=1.12, fontweight='bold', fontsize=10)   # --------------------------------------------------------------------- # 3) CREATE FIGURE # --------------------------------------------------------------------- fig = plt.figure(figsize=(4, 4)) ax = plt.subplot(111, polar=True)  draw_radar_with_humans(ax, pivot_df, title='Precision', ylim=(0.3, 1.0))  # Custom legend legend_elements = [     Line2D(         [0],         [0],         color='#E67E22',         linewidth=2,         marker='o',         markersize=3.5,         markeredgewidth=0,         label='Claude Sonnet 4',     ),     Line2D(         [0],         [0],         color='#1A5276',         linewidth=1.8,         marker='o',         markersize=3,         markeredgewidth=0,         label='Consensus',     ),     Line2D(         [0], [0], color='#AAAAAA', linewidth=1.0, alpha=0.6, label='Individual humans'     ), ]  fig.legend(     handles=legend_elements,     loc='lower center',     bbox_to_anchor=(0.5, -0.02),     ncol=3,     frameon=False,     fontsize=7,     handlelength=1.5,     columnspacing=1.2, )  plt.tight_layout() plt.subplots_adjust(bottom=0.12) plt.savefig('human_precision_radar_v3.pdf', dpi=300, bbox_inches='tight') plt.show()"},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#overview","title":"Overview\u00b6","text":"<p>This notebook evaluates the performance of the PERLA (Perovskite Extraction and Research Literature Analysis) extraction pipeline by comparing extracted data against a ground truth dataset. The evaluation covers multiple Large Language Models (LLMs) and compares their extraction performance across different data fields.</p>"},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#evaluation-methodology","title":"Evaluation Methodology\u00b6","text":"<p>The evaluation uses a confusion matrix approach:</p> <ul> <li>True Positives (TP): Fields correctly extracted and matching ground truth</li> <li>False Positives (FP): Fields extracted but not present in ground truth</li> <li>False Negatives (FN): Fields in ground truth but not extracted</li> </ul> <p>Metrics calculated:</p> <ul> <li>Precision = TP / (TP + FP) \u2014 Measures extraction accuracy</li> <li>Recall = TP / (TP + FN) \u2014 Measures extraction completeness</li> <li>F1 Score = 2 \u00d7 (Precision \u00d7 Recall) / (Precision + Recall) \u2014 Harmonic mean of precision and recall</li> </ul>"},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#setup-and-evaluations","title":"Setup and Evaluations\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#imports-setup","title":"Imports &amp; Setup\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#model-configurations","title":"Model Configurations\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#evaluations","title":"Evaluations\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#evals-code","title":"Evals Code\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#helper-functions","title":"Helper Functions\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#visualize","title":"Visualize\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#overall-performance","title":"Overall Performance\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#radar-plot-recallprecision-per-field","title":"Radar Plot: Recall/Precision per field\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#comparison-with-experts","title":"Comparison with Experts\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#evaluation-code","title":"Evaluation Code\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#overlapping-extractions-spider-plots","title":"Overlapping extractions spider plots\u00b6","text":""},{"location":"notebooks/perla_notebooks/perla-evals-analysis.html#what-dois-match-across-our-extractions","title":"What dois match across our extractions\u00b6","text":""},{"location":"notebooks/perla_notebooks/perovskite-paperbot-plot.html","title":"Perovskite Paperbot Plotting","text":"In\u00a0[\u00a0]: Copied! <pre># ruff: noqa: E402\n</pre> # ruff: noqa: E402          Perovskite Papersbot Analysis <p>     This notebook retrieves data from the Perovskite Papersbot and plots the filtering steps. </p> <p> </p> In\u00a0[1]: Copied! <pre>import time\n\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom huggingface_hub import HfApi, snapshot_download\nfrom plotly_theme import register_template, set_defaults  # type: ignore\n\n# Register and set default Plotly theme for consistent styling\nregister_template()\nset_defaults()\n</pre> import time  import pandas as pd import plotly.graph_objects as go from huggingface_hub import HfApi, snapshot_download from plotly_theme import register_template, set_defaults  # type: ignore  # Register and set default Plotly theme for consistent styling register_template() set_defaults() <pre>/Users/pepemarquez/git/Pepe-Marquez/nomad-distro-dev/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre># Initialize Hugging Face API token and repository ID\nrepo_id = 'pilar12/perovskite-papersbot'\napi = HfApi()\nlocal_dir = 'paperbot_runs'\n\n\n# Function to download files from the Hugging Face repository\ndef download_files():\n    snapshot_path = snapshot_download(\n        repo_id=repo_id,\n        local_dir=local_dir,  # Local directory to save the downloaded files\n        repo_type='dataset',\n        revision='comb_regex',\n        force_download=True,\n    )\n    return snapshot_path\n</pre> # Initialize Hugging Face API token and repository ID repo_id = 'pilar12/perovskite-papersbot' api = HfApi() local_dir = 'paperbot_runs'   # Function to download files from the Hugging Face repository def download_files():     snapshot_path = snapshot_download(         repo_id=repo_id,         local_dir=local_dir,  # Local directory to save the downloaded files         repo_type='dataset',         revision='comb_regex',         force_download=True,     )     return snapshot_path In\u00a0[3]: Copied! <pre>download_files()\n</pre> download_files() <pre>Fetching 14 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:03&lt;00:00,  3.89it/s]\n</pre> Out[3]: <pre>'/Users/pepemarquez/git/Pepe-Marquez/nomad-distro-dev/packages/nomad-perovskite-solar-cells-database/src/perovskite_solar_cell_database/example_uploads/perla_notebooks/paperbot_runs'</pre> In\u00a0[4]: Copied! <pre>def get_stats():\n    # Load post-processed data and full entry statistics\n    post_proc_df = pd.read_csv(f'{local_dir}/post_proc.csv').replace({float('nan'): ''})\n    post_proc_df['pdf_available'] = post_proc_df['pdf_available'].apply(\n        lambda x: False if x == '' else x\n    )\n    full_df = pd.read_csv(f'{local_dir}/entry_stats.csv').replace({float('nan'): ''})\n\n    # Filter data based on different matching criteria\n    full_rss = full_df[full_df['match'] == 1]\n    full_strict_rss = full_df[full_df['strict_regex'] &gt; 2]\n    strict_rss_with_doi = post_proc_df[post_proc_df['strict_regex'] &gt; 2]\n    full_relaxed_rss = full_df[(full_df['match'] == 1) &amp; (full_df['strict_regex'] &lt;= 2)]\n    relaxed_rss_with_doi = post_proc_df[\n        (post_proc_df['match'] == 1) &amp; (post_proc_df['strict_regex'] &lt;= 2)\n    ]\n\n    # Store dataframes in a dictionary for easier access\n    dfs = {\n        'full': (full_rss, post_proc_df),\n        'strict_rss': (full_strict_rss, strict_rss_with_doi),\n        'relaxed_rss': (full_relaxed_rss, relaxed_rss_with_doi),\n    }\n\n    # Initialize statistics dictionary\n    match_df = post_proc_df[\n        (post_proc_df['abstract_match']) &amp; (post_proc_df['doi_good_to_go'])\n    ]\n    stats = {\n        'total': len(full_df),\n        'oa': len(match_df[match_df['pdf_available']]),\n        'non_oa': len(\n            match_df[match_df['pdf_url'] == '']\n        ),  # Non-OA with abstract match\n        'oa_no_info': len(match_df[match_df['pdf_url'].apply(lambda x: 'Error' in x)]),\n    }  # OA with errors in URL\n\n    # Calculate statistics for each category (full, strict_rss, relaxed_rss)\n    for key, (fdf, doi_df) in dfs.items():\n        abs_df = doi_df[doi_df['abstract_found']]\n        stats[f'{key}_match'] = len(fdf)\n        stats[f'{key}_match_with_doi'] = len(doi_df)\n        stats[f'{key}_match_missing_doi'] = len(fdf) - len(doi_df)\n        stats[f'{key}_abstracts_found'] = len(abs_df)\n        stats[f'{key}_missing_abstracts'] = len(doi_df) - len(abs_df)\n        stats[f'{key}_strict_matches_with_abstract_found'] = len(\n            abs_df[abs_df['abstract_match']]\n        )\n        stats[f'{key}_strict_matches_without_abstract'] = len(\n            doi_df[doi_df['abstract_match']]\n        ) - len(abs_df[abs_df['abstract_match']])\n        stats[f'{key}_total_strict_matches'] = len(doi_df[doi_df['abstract_match']])\n        stats[f'{key}_doi_good_matches_with_abstract_found'] = len(\n            abs_df[(abs_df['abstract_match']) &amp; (abs_df['doi_good_to_go'])]\n        )\n        stats[f'{key}_doi_good_matches_without_abstract'] = len(\n            doi_df[(doi_df['abstract_match']) &amp; (doi_df['doi_good_to_go'])]\n        ) - len(abs_df[(abs_df['abstract_match']) &amp; (abs_df['doi_good_to_go'])])\n        stats[f'{key}_total_doi_good_matches'] = len(\n            doi_df[(doi_df['abstract_match']) &amp; (doi_df['doi_good_to_go'])]\n        )\n\n    # Ensure all stats are integers\n    stats = {k: int(v) for k, v in stats.items()}\n\n    # Get the time range of parsed data\n    parsed_times = full_df['parsed_time'].values\n    start = min(parsed_times)\n    end = max(parsed_times)\n    stats['period'] = end - start\n    stats['start'] = time.strftime('%d-%m-%Y', time.gmtime(start))\n    stats['end'] = time.strftime('%d-%m-%Y', time.gmtime(end))\n    return stats\n</pre> def get_stats():     # Load post-processed data and full entry statistics     post_proc_df = pd.read_csv(f'{local_dir}/post_proc.csv').replace({float('nan'): ''})     post_proc_df['pdf_available'] = post_proc_df['pdf_available'].apply(         lambda x: False if x == '' else x     )     full_df = pd.read_csv(f'{local_dir}/entry_stats.csv').replace({float('nan'): ''})      # Filter data based on different matching criteria     full_rss = full_df[full_df['match'] == 1]     full_strict_rss = full_df[full_df['strict_regex'] &gt; 2]     strict_rss_with_doi = post_proc_df[post_proc_df['strict_regex'] &gt; 2]     full_relaxed_rss = full_df[(full_df['match'] == 1) &amp; (full_df['strict_regex'] &lt;= 2)]     relaxed_rss_with_doi = post_proc_df[         (post_proc_df['match'] == 1) &amp; (post_proc_df['strict_regex'] &lt;= 2)     ]      # Store dataframes in a dictionary for easier access     dfs = {         'full': (full_rss, post_proc_df),         'strict_rss': (full_strict_rss, strict_rss_with_doi),         'relaxed_rss': (full_relaxed_rss, relaxed_rss_with_doi),     }      # Initialize statistics dictionary     match_df = post_proc_df[         (post_proc_df['abstract_match']) &amp; (post_proc_df['doi_good_to_go'])     ]     stats = {         'total': len(full_df),         'oa': len(match_df[match_df['pdf_available']]),         'non_oa': len(             match_df[match_df['pdf_url'] == '']         ),  # Non-OA with abstract match         'oa_no_info': len(match_df[match_df['pdf_url'].apply(lambda x: 'Error' in x)]),     }  # OA with errors in URL      # Calculate statistics for each category (full, strict_rss, relaxed_rss)     for key, (fdf, doi_df) in dfs.items():         abs_df = doi_df[doi_df['abstract_found']]         stats[f'{key}_match'] = len(fdf)         stats[f'{key}_match_with_doi'] = len(doi_df)         stats[f'{key}_match_missing_doi'] = len(fdf) - len(doi_df)         stats[f'{key}_abstracts_found'] = len(abs_df)         stats[f'{key}_missing_abstracts'] = len(doi_df) - len(abs_df)         stats[f'{key}_strict_matches_with_abstract_found'] = len(             abs_df[abs_df['abstract_match']]         )         stats[f'{key}_strict_matches_without_abstract'] = len(             doi_df[doi_df['abstract_match']]         ) - len(abs_df[abs_df['abstract_match']])         stats[f'{key}_total_strict_matches'] = len(doi_df[doi_df['abstract_match']])         stats[f'{key}_doi_good_matches_with_abstract_found'] = len(             abs_df[(abs_df['abstract_match']) &amp; (abs_df['doi_good_to_go'])]         )         stats[f'{key}_doi_good_matches_without_abstract'] = len(             doi_df[(doi_df['abstract_match']) &amp; (doi_df['doi_good_to_go'])]         ) - len(abs_df[(abs_df['abstract_match']) &amp; (abs_df['doi_good_to_go'])])         stats[f'{key}_total_doi_good_matches'] = len(             doi_df[(doi_df['abstract_match']) &amp; (doi_df['doi_good_to_go'])]         )      # Ensure all stats are integers     stats = {k: int(v) for k, v in stats.items()}      # Get the time range of parsed data     parsed_times = full_df['parsed_time'].values     start = min(parsed_times)     end = max(parsed_times)     stats['period'] = end - start     stats['start'] = time.strftime('%d-%m-%Y', time.gmtime(start))     stats['end'] = time.strftime('%d-%m-%Y', time.gmtime(end))     return stats In\u00a0[5]: Copied! <pre>def complex_sankey():\n    # Get statistics from the data\n    stats = get_stats()\n\n    # Define labels for the Sankey diagram nodes\n    labels = [\n        'RSS Summary &lt;br&gt;Keyword Match',\n        'Un-Resolvable Reference',\n        'Abstract Found',\n        'Abstract Missing',\n        'Filtered Papers',\n        'Open-Access',\n        'Non Open-Access',\n        'Failed Retrieval',\n    ]\n\n    # Define source and target nodes for the links\n    sources = [0, 0, 0, 2, 3, 4, 4, 4]\n    targets = [1, 2, 3, 4, 4, 5, 6, 7]\n\n    # Define the values (thickness) for the links based on calculated statistics\n    values = [\n        stats['full_match_missing_doi'],  # Un-Resolvable Reference\n        stats['full_abstracts_found'],  # Abstract Found\n        stats['full_missing_abstracts'],  # Abstract Missing\n        stats['full_doi_good_matches_with_abstract_found'],  # Keyword Match,\n        stats['full_doi_good_matches_without_abstract'],  # Keyword Match\n        stats['oa'],  # Open-Access\n        stats['non_oa'],  # Non Open-Acess\n        stats['oa_no_info'],  # Failed Retrieval\n    ]\n\n    # Calculate the total value for each node to display in the label\n    node_values = []\n    for i in range(len(labels)):\n        v = 0\n        node_list = targets\n        if i not in targets:\n            node_list = sources\n        for j in range(len(node_list)):\n            v += values[j] if node_list[j] == i else 0\n        node_values.append(v)\n\n    # Format labels to include node values\n    labels = [f'&lt;b&gt;{i}&lt;br&gt;{v}&lt;/b&gt;' for i, v in zip(labels, node_values)]\n\n    # Create the Sankey diagram figure\n    fig = go.Figure(\n        data=[\n            go.Sankey(\n                valueformat='.0f',\n                arrangement='snap',\n                node=dict(\n                    pad=8,\n                    thickness=10,\n                    line=dict(color='black', width=0.5),\n                    label=labels,\n                    align='left',\n                ),\n                link=dict(\n                    source=sources,\n                    target=targets,\n                    value=values,\n                    color='rgba(0,0,255,0.2)',\n                ),\n            )\n        ]\n    )\n\n    # Set a title for the plot including total papers and date range\n    fig.update_layout(\n        title_text=f'&lt;b&gt;{stats[\"total\"]} papers parsed  from {stats[\"start\"]} to {stats[\"end\"]}&lt;/b&gt;',\n        font_size=12,\n        width=600,\n    )\n\n    # Display the plot\n    fig.show(renderer=\"notebook\")\n    return fig, stats\n</pre> def complex_sankey():     # Get statistics from the data     stats = get_stats()      # Define labels for the Sankey diagram nodes     labels = [         'RSS Summary Keyword Match',         'Un-Resolvable Reference',         'Abstract Found',         'Abstract Missing',         'Filtered Papers',         'Open-Access',         'Non Open-Access',         'Failed Retrieval',     ]      # Define source and target nodes for the links     sources = [0, 0, 0, 2, 3, 4, 4, 4]     targets = [1, 2, 3, 4, 4, 5, 6, 7]      # Define the values (thickness) for the links based on calculated statistics     values = [         stats['full_match_missing_doi'],  # Un-Resolvable Reference         stats['full_abstracts_found'],  # Abstract Found         stats['full_missing_abstracts'],  # Abstract Missing         stats['full_doi_good_matches_with_abstract_found'],  # Keyword Match,         stats['full_doi_good_matches_without_abstract'],  # Keyword Match         stats['oa'],  # Open-Access         stats['non_oa'],  # Non Open-Acess         stats['oa_no_info'],  # Failed Retrieval     ]      # Calculate the total value for each node to display in the label     node_values = []     for i in range(len(labels)):         v = 0         node_list = targets         if i not in targets:             node_list = sources         for j in range(len(node_list)):             v += values[j] if node_list[j] == i else 0         node_values.append(v)      # Format labels to include node values     labels = [f'{i}{v}' for i, v in zip(labels, node_values)]      # Create the Sankey diagram figure     fig = go.Figure(         data=[             go.Sankey(                 valueformat='.0f',                 arrangement='snap',                 node=dict(                     pad=8,                     thickness=10,                     line=dict(color='black', width=0.5),                     label=labels,                     align='left',                 ),                 link=dict(                     source=sources,                     target=targets,                     value=values,                     color='rgba(0,0,255,0.2)',                 ),             )         ]     )      # Set a title for the plot including total papers and date range     fig.update_layout(         title_text=f'{stats[\"total\"]} papers parsed  from {stats[\"start\"]} to {stats[\"end\"]}',         font_size=12,         width=600,     )      # Display the plot     fig.show(renderer=\"notebook\")     return fig, stats In\u00a0[6]: Copied! <pre>fig, stats = complex_sankey()\n</pre> fig, stats = complex_sankey() In\u00a0[\u00a0]: Copied! <pre>f'The multi-stage filtering pipeline for identifying new perovskite solar cell papers. \\\nOver a {int(stats[\"period\"] / 86400)}-day period ({stats[\"start\"]} to {stats[\"end\"]}), {stats[\"total\"]} papers were parsed from Journal RSS feeds. \\\nAn initial match against RSS summaries identified {stats[\"full_match\"]} candidates. Subsequent steps remove papers with unresolvable DOIs (n = {stats[\"full_match_missing_doi\"]}),\\\nand failing a secondary strict match (n = {stats[\"full_match\"] - stats[\"full_total_strict_matches\"] - stats[\"full_match_missing_doi\"]}). \\\nFurther filtering is done to exclude theoretical, computational and review works (n = {stats[\"full_total_strict_matches\"] - stats[\"full_total_doi_good_matches\"]}), yielding a final set of {stats[\"full_total_doi_good_matches\"]} relevant papers of which {stats[\"oa\"]} were open-access papers.'\n</pre> f'The multi-stage filtering pipeline for identifying new perovskite solar cell papers. \\ Over a {int(stats[\"period\"] / 86400)}-day period ({stats[\"start\"]} to {stats[\"end\"]}), {stats[\"total\"]} papers were parsed from Journal RSS feeds. \\ An initial match against RSS summaries identified {stats[\"full_match\"]} candidates. Subsequent steps remove papers with unresolvable DOIs (n = {stats[\"full_match_missing_doi\"]}),\\ and failing a secondary strict match (n = {stats[\"full_match\"] - stats[\"full_total_strict_matches\"] - stats[\"full_match_missing_doi\"]}). \\ Further filtering is done to exclude theoretical, computational and review works (n = {stats[\"full_total_strict_matches\"] - stats[\"full_total_doi_good_matches\"]}), yielding a final set of {stats[\"full_total_doi_good_matches\"]} relevant papers of which {stats[\"oa\"]} were open-access papers.'"},{"location":"notebooks/perla_notebooks/perovskite-paperbot-plot.html#overview","title":"Overview\u00b6","text":"<ol> <li>Data Retrieval: Downloads Perovskite Papersbot run log files stored in Hugging Face repo <code>pilar12/perovskite-papersbot</code>.</li> <li>Data Processing: Loads and processes the downloaded CSV files to extract statistics related to paper matching, abstract availability, and open-access status.</li> <li>Visualisation: Generates a Sankey diagram to visually represent the multi-stage filtering pipeline for identifying new perovskite solar cell papers, showing the flow of papers through different stages of filtering.</li> <li>Summary: Provides a textual summary of the filtering process, including the total number of papers parsed, initial matches, papers filtered out, and the final count of relevant and open-access papers.</li> </ol>"},{"location":"notebooks/perla_notebooks/perovskite-paperbot-plot.html#1-data-retrieval","title":"1. Data Retrieval\u00b6","text":""},{"location":"notebooks/perla_notebooks/perovskite-paperbot-plot.html#2-data-processing","title":"2. Data Processing\u00b6","text":""},{"location":"notebooks/perla_notebooks/perovskite-paperbot-plot.html#3-visualisation","title":"3. Visualisation\u00b6","text":""},{"location":"notebooks/perla_notebooks/perovskite-paperbot-plot.html#4-summary","title":"4. Summary\u00b6","text":""},{"location":"notebooks/perla_notebooks/physics_filter.html","title":"Physics filter","text":"In\u00a0[\u00a0]: \"hide-cell\" Copied! <pre># ruff: noqa: E402, I001\n</pre> # ruff: noqa: E402, I001           Physics Consistency Filter: Legacy Database vs. PERLA Pipeline <p>     This notebook evaluates data quality by testing the fundamental physics relationship for solar cell power conversion efficiency: PCE = (FF \u00d7 V<sub>OC</sub> \u00d7 J<sub>SC</sub>) / P<sub>in</sub>. We compare two datasets from the Perovskite Solar Cell Database in NOMAD: the legacy human-curated entries and the new PERLA LLM-extracted entries. </p> <p>     The analysis reveals the fraction of legacy database entries that fail this physics consistency check (with 0.2% absolute tolerance), while the PERLA pipeline enforces this filter as a validation requirement, ensuring only physically consistent entries are accepted into the database. </p> In\u00a0[1]: Copied! <pre>from plotly_theme import register_template, set_defaults # type: ignore\n\nregister_template()\nset_defaults()\n</pre> from plotly_theme import register_template, set_defaults # type: ignore  register_template() set_defaults() In\u00a0[\u00a0]: Copied! <pre># load the data from into a df from the parquet file\nimport pandas as pd\n\ndf = pd.read_parquet('perovskite_solar_cell_database.parquet')\n# df = pd.read_parquet('perovskite_solar_cell_database.parquet')\n# Set a source_database column: if name_of_person_entering_the_data is 'LLM Extraction', use 'LLM Extracted', else 'Manual Entry'\ndf['source_database'] = df['data.ref.name_of_person_entering_the_data'].apply(\n    lambda x: 'LLM Extracted' if x == 'LLM Extraction' else 'Manual Entry'\n)\n</pre> # load the data from into a df from the parquet file import pandas as pd  df = pd.read_parquet('perovskite_solar_cell_database.parquet') # df = pd.read_parquet('perovskite_solar_cell_database.parquet') # Set a source_database column: if name_of_person_entering_the_data is 'LLM Extraction', use 'LLM Extracted', else 'Manual Entry' df['source_database'] = df['data.ref.name_of_person_entering_the_data'].apply(     lambda x: 'LLM Extracted' if x == 'LLM Extraction' else 'Manual Entry' ) In\u00a0[3]: Copied! <pre># set in the df a source_database column. Is data.ref.person_entering_data is LLM Extracted else Manual Entry\n# df['source_database'] = df['data.ref.name_of_person_entering_the_data'].apply(\n#     lambda x: 'LLM Extracted' if x == 'LLM Extraction' else 'Manual Entry'\n# )\nfrom plotly_theme import DEFAULT_COLORWAY\n\nSOURCE_ORDER = ['Manual Entry', 'LLM Extracted']\n\nCOLOR_MAP = dict(zip(SOURCE_ORDER, DEFAULT_COLORWAY))\n</pre> # set in the df a source_database column. Is data.ref.person_entering_data is LLM Extracted else Manual Entry # df['source_database'] = df['data.ref.name_of_person_entering_the_data'].apply( #     lambda x: 'LLM Extracted' if x == 'LLM Extraction' else 'Manual Entry' # ) from plotly_theme import DEFAULT_COLORWAY  SOURCE_ORDER = ['Manual Entry', 'LLM Extracted']  COLOR_MAP = dict(zip(SOURCE_ORDER, DEFAULT_COLORWAY)) In\u00a0[4]: Copied! <pre># check in a histogram and print the distribution of results.properties.optoelectronic.solar_cell.illumination_intensity\n\n# print the overall distribution\nprint(\"=\"*80)\nprint(\"OVERALL DISTRIBUTION\")\nprint(\"=\"*80)\nprint(df['results.properties.optoelectronic.solar_cell.illumination_intensity'].describe())\nprint()\n\n# Check for None/NaN values\nnone_count = df['results.properties.optoelectronic.solar_cell.illumination_intensity'].isna().sum()\nprint(f\"Number of None/NaN entries: {none_count}\")\nprint(f\"Percentage: {none_count/len(df)*100:.2f}%\")\nprint()\n\n# Check for zero values\nzero_count = (df['results.properties.optoelectronic.solar_cell.illumination_intensity'] == 0).sum()\nprint(f\"Number of zero entries: {zero_count}\")\nprint(f\"Percentage: {zero_count/len(df)*100:.2f}%\")\nprint()\n\n# print how many are not 1000 W/m2 (excluding None/NaN and zeros)\nnot_1000 = df[\n    (df['results.properties.optoelectronic.solar_cell.illumination_intensity'] != 1000) &amp;\n    (df['results.properties.optoelectronic.solar_cell.illumination_intensity'].notna()) &amp;\n    (df['results.properties.optoelectronic.solar_cell.illumination_intensity'] != 0)\n]\nprint(f\"Number of entries not at 1000 W/m2 (excluding None/NaN and zeros): {len(not_1000)}\")\nprint(f\"Percentage: {len(not_1000)/len(df)*100:.2f}%\")\nprint()\n\n# print the distribution values for the illumination intensity for each source_database\nprint(\"=\"*80)\nprint(\"BREAKDOWN BY SOURCE DATABASE\")\nprint(\"=\"*80)\nfor source in SOURCE_ORDER:\n    subset = df[df['source_database'] == source]\n    print(f\"\\n{source}:\")\n    print(subset['results.properties.optoelectronic.solar_cell.illumination_intensity'].describe())\n    print()\n\n    # Check for None/NaN values\n    none_count = subset['results.properties.optoelectronic.solar_cell.illumination_intensity'].isna().sum()\n    print(f\"Number of None/NaN entries for {source}: {none_count}\")\n    print(f\"Percentage: {none_count/len(subset)*100:.2f}%\")\n    print()\n\n    # Check for zero values\n    zero_count = (subset['results.properties.optoelectronic.solar_cell.illumination_intensity'] == 0).sum()\n    print(f\"Number of zero entries for {source}: {zero_count}\")\n    print(f\"Percentage: {zero_count/len(subset)*100:.2f}%\")\n    print()\n\n    # print how many are not 1000 W/m2 (excluding None/NaN and zeros)\n    not_1000 = subset[\n        (subset['results.properties.optoelectronic.solar_cell.illumination_intensity'] != 1000) &amp;\n        (subset['results.properties.optoelectronic.solar_cell.illumination_intensity'].notna()) &amp;\n        (subset['results.properties.optoelectronic.solar_cell.illumination_intensity'] != 0)\n    ]\n    print(f\"Number of entries not at 1000 W/m2 for {source} (excluding None/NaN and zeros): {len(not_1000)}\")\n    print(f\"Percentage: {len(not_1000)/len(subset)*100:.2f}%\")\n    print()\n</pre> # check in a histogram and print the distribution of results.properties.optoelectronic.solar_cell.illumination_intensity  # print the overall distribution print(\"=\"*80) print(\"OVERALL DISTRIBUTION\") print(\"=\"*80) print(df['results.properties.optoelectronic.solar_cell.illumination_intensity'].describe()) print()  # Check for None/NaN values none_count = df['results.properties.optoelectronic.solar_cell.illumination_intensity'].isna().sum() print(f\"Number of None/NaN entries: {none_count}\") print(f\"Percentage: {none_count/len(df)*100:.2f}%\") print()  # Check for zero values zero_count = (df['results.properties.optoelectronic.solar_cell.illumination_intensity'] == 0).sum() print(f\"Number of zero entries: {zero_count}\") print(f\"Percentage: {zero_count/len(df)*100:.2f}%\") print()  # print how many are not 1000 W/m2 (excluding None/NaN and zeros) not_1000 = df[     (df['results.properties.optoelectronic.solar_cell.illumination_intensity'] != 1000) &amp;     (df['results.properties.optoelectronic.solar_cell.illumination_intensity'].notna()) &amp;     (df['results.properties.optoelectronic.solar_cell.illumination_intensity'] != 0) ] print(f\"Number of entries not at 1000 W/m2 (excluding None/NaN and zeros): {len(not_1000)}\") print(f\"Percentage: {len(not_1000)/len(df)*100:.2f}%\") print()  # print the distribution values for the illumination intensity for each source_database print(\"=\"*80) print(\"BREAKDOWN BY SOURCE DATABASE\") print(\"=\"*80) for source in SOURCE_ORDER:     subset = df[df['source_database'] == source]     print(f\"\\n{source}:\")     print(subset['results.properties.optoelectronic.solar_cell.illumination_intensity'].describe())     print()      # Check for None/NaN values     none_count = subset['results.properties.optoelectronic.solar_cell.illumination_intensity'].isna().sum()     print(f\"Number of None/NaN entries for {source}: {none_count}\")     print(f\"Percentage: {none_count/len(subset)*100:.2f}%\")     print()      # Check for zero values     zero_count = (subset['results.properties.optoelectronic.solar_cell.illumination_intensity'] == 0).sum()     print(f\"Number of zero entries for {source}: {zero_count}\")     print(f\"Percentage: {zero_count/len(subset)*100:.2f}%\")     print()      # print how many are not 1000 W/m2 (excluding None/NaN and zeros)     not_1000 = subset[         (subset['results.properties.optoelectronic.solar_cell.illumination_intensity'] != 1000) &amp;         (subset['results.properties.optoelectronic.solar_cell.illumination_intensity'].notna()) &amp;         (subset['results.properties.optoelectronic.solar_cell.illumination_intensity'] != 0)     ]     print(f\"Number of entries not at 1000 W/m2 for {source} (excluding None/NaN and zeros): {len(not_1000)}\")     print(f\"Percentage: {len(not_1000)/len(subset)*100:.2f}%\")     print() <pre>================================================================================\nOVERALL DISTRIBUTION\n================================================================================\ncount    48745.000000\nmean      1004.623427\nstd        278.436899\nmin          0.000000\n25%       1000.000000\n50%       1000.000000\n75%       1000.000000\nmax      18000.000000\nName: results.properties.optoelectronic.solar_cell.illumination_intensity, dtype: float64\n\nNumber of None/NaN entries: 2383\nPercentage: 4.66%\n\nNumber of zero entries: 8\nPercentage: 0.02%\n\nNumber of entries not at 1000 W/m2 (excluding None/NaN and zeros): 449\nPercentage: 0.88%\n\n================================================================================\nBREAKDOWN BY SOURCE DATABASE\n================================================================================\n\nManual Entry:\ncount    43032.000000\nmean       999.496165\nstd        191.401147\nmin          0.000000\n25%       1000.000000\n50%       1000.000000\n75%       1000.000000\nmax      18000.000000\nName: results.properties.optoelectronic.solar_cell.illumination_intensity, dtype: float64\n\nNumber of None/NaN entries for Manual Entry: 75\nPercentage: 0.17%\n\nNumber of zero entries for Manual Entry: 8\nPercentage: 0.02%\n\nNumber of entries not at 1000 W/m2 for Manual Entry (excluding None/NaN and zeros): 403\nPercentage: 0.93%\n\n\nLLM Extracted:\ncount     5713.000000\nmean      1043.243480\nstd        619.607578\nmin        100.000000\n25%       1000.000000\n50%       1000.000000\n75%       1000.000000\nmax      10000.000000\nName: results.properties.optoelectronic.solar_cell.illumination_intensity, dtype: float64\n\nNumber of None/NaN entries for LLM Extracted: 2308\nPercentage: 28.77%\n\nNumber of zero entries for LLM Extracted: 0\nPercentage: 0.00%\n\nNumber of entries not at 1000 W/m2 for LLM Extracted (excluding None/NaN and zeros): 46\nPercentage: 0.57%\n\n</pre> In\u00a0[5]: Copied! <pre># check in a histogram and print the distribution of results.properties.optoelectronic.solar_cell.illumination_intensity\n\n# print the distribution values for the illumination intensity for each source_database\nfor source in SOURCE_ORDER:\n    subset = df[df['source_database'] == source]\n    print(f\"Distribution for {source}:\")\n    print(subset['results.properties.optoelectronic.solar_cell.illumination_intensity'].describe())\n    print()\n\n    # print how many are not 1000 W/m2\n\n    not_1000 = subset[subset['results.properties.optoelectronic.solar_cell.illumination_intensity'] != 1000]\n    print(f\"Number of entries not at 1000 W/m2 for {source}: {len(not_1000)}\")\n    print()\n</pre> # check in a histogram and print the distribution of results.properties.optoelectronic.solar_cell.illumination_intensity  # print the distribution values for the illumination intensity for each source_database for source in SOURCE_ORDER:     subset = df[df['source_database'] == source]     print(f\"Distribution for {source}:\")     print(subset['results.properties.optoelectronic.solar_cell.illumination_intensity'].describe())     print()      # print how many are not 1000 W/m2      not_1000 = subset[subset['results.properties.optoelectronic.solar_cell.illumination_intensity'] != 1000]     print(f\"Number of entries not at 1000 W/m2 for {source}: {len(not_1000)}\")     print() <pre>Distribution for Manual Entry:\ncount    43032.000000\nmean       999.496165\nstd        191.401147\nmin          0.000000\n25%       1000.000000\n50%       1000.000000\n75%       1000.000000\nmax      18000.000000\nName: results.properties.optoelectronic.solar_cell.illumination_intensity, dtype: float64\n\nNumber of entries not at 1000 W/m2 for Manual Entry: 486\n\nDistribution for LLM Extracted:\ncount     5713.000000\nmean      1043.243480\nstd        619.607578\nmin        100.000000\n25%       1000.000000\n50%       1000.000000\n75%       1000.000000\nmax      10000.000000\nName: results.properties.optoelectronic.solar_cell.illumination_intensity, dtype: float64\n\nNumber of entries not at 1000 W/m2 for LLM Extracted: 2354\n\n</pre> In\u00a0[6]: Copied! <pre>import numpy as np\nimport plotly.graph_objects as go\n\n# columns we REQUIRE to be present\nrequired_cols = [\n    'results.properties.optoelectronic.solar_cell.fill_factor',\n    'results.properties.optoelectronic.solar_cell.short_circuit_current_density',\n    'results.properties.optoelectronic.solar_cell.open_circuit_voltage',\n    'results.properties.optoelectronic.solar_cell.efficiency',\n    'results.properties.optoelectronic.solar_cell.illumination_intensity',\n]\n\n# drop rows where ANY required value is missing\ndf_clean = df.dropna(subset=required_cols).copy()\n\n# alias for readability\n# Units from NOMAD results (all SI):\n# ff: dimensionless (0-1)\n# jsc: A/m\u00b2 (SI)\n# voc: V (SI)\n# pce: % (percentage)\n# illumination_intensity: W/m\u00b2 (SI)\nff = df_clean['results.properties.optoelectronic.solar_cell.fill_factor']\njsc = df_clean[\n    'results.properties.optoelectronic.solar_cell.short_circuit_current_density'\n]\nvoc = df_clean['results.properties.optoelectronic.solar_cell.open_circuit_voltage']\npce = df_clean['results.properties.optoelectronic.solar_cell.efficiency']\nillumination = df_clean['results.properties.optoelectronic.solar_cell.illumination_intensity']\n\n# compute expected PCE using correct formula\n# PCE (%) = (FF \u00d7 V_OC [V] \u00d7 J_SC [A/m\u00b2]) / P_in [W/m\u00b2] \u00d7 100\n# Units: (dimensionless \u00d7 V \u00d7 A/m\u00b2) / (W/m\u00b2) \u00d7 100 = (W/m\u00b2) / (W/m\u00b2) \u00d7 100 = %\ndf_clean['pce_calc'] = (ff * voc * jsc) / illumination * 100\n\n# isclose check (absolute tolerance only)\ndf_clean['pce_isclose'] = np.isclose(\n    pce,\n    df_clean['pce_calc'],\n    atol=0.2,\n)\n\nsummary = (\n    df_clean.groupby('source_database')['pce_isclose']\n    .agg(fraction='mean', n='size')\n    .reindex(['Manual Entry', 'LLM Extracted'])\n)\n\n# Calculate mismatch data for panel b\ndf_mismatch = df_clean[~df_clean['pce_isclose']].copy()\ndf_mismatch['pce_diff'] = abs(df_mismatch['pce_calc'] - pce[df_mismatch.index])\ndf_mismatch['pce_diff_percent'] = (df_mismatch['pce_diff'] / pce[df_mismatch.index]) * 100\n</pre> import numpy as np import plotly.graph_objects as go  # columns we REQUIRE to be present required_cols = [     'results.properties.optoelectronic.solar_cell.fill_factor',     'results.properties.optoelectronic.solar_cell.short_circuit_current_density',     'results.properties.optoelectronic.solar_cell.open_circuit_voltage',     'results.properties.optoelectronic.solar_cell.efficiency',     'results.properties.optoelectronic.solar_cell.illumination_intensity', ]  # drop rows where ANY required value is missing df_clean = df.dropna(subset=required_cols).copy()  # alias for readability # Units from NOMAD results (all SI): # ff: dimensionless (0-1) # jsc: A/m\u00b2 (SI) # voc: V (SI) # pce: % (percentage) # illumination_intensity: W/m\u00b2 (SI) ff = df_clean['results.properties.optoelectronic.solar_cell.fill_factor'] jsc = df_clean[     'results.properties.optoelectronic.solar_cell.short_circuit_current_density' ] voc = df_clean['results.properties.optoelectronic.solar_cell.open_circuit_voltage'] pce = df_clean['results.properties.optoelectronic.solar_cell.efficiency'] illumination = df_clean['results.properties.optoelectronic.solar_cell.illumination_intensity']  # compute expected PCE using correct formula # PCE (%) = (FF \u00d7 V_OC [V] \u00d7 J_SC [A/m\u00b2]) / P_in [W/m\u00b2] \u00d7 100 # Units: (dimensionless \u00d7 V \u00d7 A/m\u00b2) / (W/m\u00b2) \u00d7 100 = (W/m\u00b2) / (W/m\u00b2) \u00d7 100 = % df_clean['pce_calc'] = (ff * voc * jsc) / illumination * 100  # isclose check (absolute tolerance only) df_clean['pce_isclose'] = np.isclose(     pce,     df_clean['pce_calc'],     atol=0.2, )  summary = (     df_clean.groupby('source_database')['pce_isclose']     .agg(fraction='mean', n='size')     .reindex(['Manual Entry', 'LLM Extracted']) )  # Calculate mismatch data for panel b df_mismatch = df_clean[~df_clean['pce_isclose']].copy() df_mismatch['pce_diff'] = abs(df_mismatch['pce_calc'] - pce[df_mismatch.index]) df_mismatch['pce_diff_percent'] = (df_mismatch['pce_diff'] / pce[df_mismatch.index]) * 100 In\u00a0[7]: Copied! <pre>from plotly.subplots import make_subplots\n\n# Create figure with subplots\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=('', ''),\n    horizontal_spacing=0.15,\n    column_widths=[0.45, 0.55]\n)\n\n# PANEL A: Bar chart\nbar_colors = [COLOR_MAP[src] for src in summary.index]\n\nfig.add_trace(\n    go.Bar(\n        x=summary.index,\n        y=summary['fraction'],\n        text=[f'{frac:.1%}&lt;br&gt;n={n}' for frac, n in zip(summary['fraction'], summary['n'])],\n        textposition='inside',\n        textfont=dict(size=16, color='white', family='Arial'),\n        marker=dict(color=bar_colors),\n        showlegend=False,\n    ),\n    row=1, col=1\n)\n\n# PANEL B: Scatter plot with improved styling\nfor source in SOURCE_ORDER:\n    subset = df_mismatch[df_mismatch['source_database'] == source]\n\n    fig.add_trace(\n        go.Scatter(\n            x=subset['results.properties.optoelectronic.solar_cell.efficiency'],\n            y=subset['pce_calc'],\n            mode='markers',\n            name=source,\n            marker=dict(\n                color=COLOR_MAP[source],\n                size=6,\n                # opacity=0.9,\n                line=dict(color='white', width=1.0)\n            ),\n            showlegend=True,\n        ),\n        row=1, col=2\n    )\n\n# Add diagonal line to panel b\nfig.add_trace(\n    go.Scatter(\n        x=[0, 26],\n        y=[0, 26],\n        mode='lines',\n        line=dict(color='gray', dash='dash', width=1.5),\n        showlegend=False,\n        hoverinfo='skip'\n    ),\n    row=1, col=2\n)\n\n# Update axes for panel a\nfig.update_xaxes(\n    # title_text='Data Source',\n    title_font=dict(size=16, family='Arial'),\n    tickfont=dict(size=16, family='Arial'),\n    showgrid=False,\n    row=1, col=1\n)\n\nfig.update_yaxes(\n    title_text='Fraction Passing Consistency Check',\n    title_font=dict(size=16, family='Arial'),\n    tickformat='.0%',\n    tickfont=dict(size=16, family='Arial'),\n    range=[0, 1.05],\n    showgrid=True,\n    gridcolor='rgba(200, 200, 200, 0.3)',\n    griddash='dot',\n    row=1, col=1\n)\n\n# Update axes for panel b\nfig.update_xaxes(\n    title_text='Reported PCE (%)',\n    title_font=dict(size=16, family='Arial'),\n    tickfont=dict(size=16, family='Arial'),\n    range=[0, 26],\n    showgrid=True,\n    gridcolor='rgba(200, 200, 200, 0.3)',\n    griddash='dot',\n    row=1, col=2\n)\n\nfig.update_yaxes(\n    title_text='Calculated PCE (%)',\n    title_font=dict(size=16, family='Arial'),\n    tickfont=dict(size=16, family='Arial'),\n    range=[0, 26],\n    showgrid=True,\n    gridcolor='rgba(200, 200, 200, 0.3)',\n    griddash='dot',\n    row=1, col=2\n)\n\n# Add Nature-style panel labels\nfig.add_annotation(\n    text='&lt;b&gt;a&lt;/b&gt;',\n    xref='x domain', yref='y domain',\n    x=-0.15, y=1.05,\n    xanchor='left', yanchor='bottom',\n    font=dict(size=18, family='Arial', color='black'),\n    showarrow=False,\n    row=1, col=1\n)\nfig.add_annotation(\n    text='&lt;b&gt;b&lt;/b&gt;',\n    xref='x2 domain', yref='y2 domain',\n    x=-0.15, y=1.05,\n    xanchor='left', yanchor='bottom',\n    font=dict(size=18, family='Arial', color='black'),\n    showarrow=False,\n    row=1, col=2\n)\n\n# Update overall layout\nfig.update_layout(\n    plot_bgcolor='white',\n    paper_bgcolor='white',\n    font=dict(family='Arial', size=12),\n    legend=dict(\n        x=0.535,\n        y=0.98,\n        xanchor='left',\n        yanchor='top',\n        bgcolor='rgba(255, 255, 255, 0.8)',\n        font=dict(size=16, family='Arial')\n    ),\n    width=700,\n    height=400,\n    margin=dict(t=60, b=80, l=80, r=80)\n)\n\nfig.show(renderer=\"notebook\")\n</pre> from plotly.subplots import make_subplots  # Create figure with subplots fig = make_subplots(     rows=1, cols=2,     subplot_titles=('', ''),     horizontal_spacing=0.15,     column_widths=[0.45, 0.55] )  # PANEL A: Bar chart bar_colors = [COLOR_MAP[src] for src in summary.index]  fig.add_trace(     go.Bar(         x=summary.index,         y=summary['fraction'],         text=[f'{frac:.1%}n={n}' for frac, n in zip(summary['fraction'], summary['n'])],         textposition='inside',         textfont=dict(size=16, color='white', family='Arial'),         marker=dict(color=bar_colors),         showlegend=False,     ),     row=1, col=1 )  # PANEL B: Scatter plot with improved styling for source in SOURCE_ORDER:     subset = df_mismatch[df_mismatch['source_database'] == source]      fig.add_trace(         go.Scatter(             x=subset['results.properties.optoelectronic.solar_cell.efficiency'],             y=subset['pce_calc'],             mode='markers',             name=source,             marker=dict(                 color=COLOR_MAP[source],                 size=6,                 # opacity=0.9,                 line=dict(color='white', width=1.0)             ),             showlegend=True,         ),         row=1, col=2     )  # Add diagonal line to panel b fig.add_trace(     go.Scatter(         x=[0, 26],         y=[0, 26],         mode='lines',         line=dict(color='gray', dash='dash', width=1.5),         showlegend=False,         hoverinfo='skip'     ),     row=1, col=2 )  # Update axes for panel a fig.update_xaxes(     # title_text='Data Source',     title_font=dict(size=16, family='Arial'),     tickfont=dict(size=16, family='Arial'),     showgrid=False,     row=1, col=1 )  fig.update_yaxes(     title_text='Fraction Passing Consistency Check',     title_font=dict(size=16, family='Arial'),     tickformat='.0%',     tickfont=dict(size=16, family='Arial'),     range=[0, 1.05],     showgrid=True,     gridcolor='rgba(200, 200, 200, 0.3)',     griddash='dot',     row=1, col=1 )  # Update axes for panel b fig.update_xaxes(     title_text='Reported PCE (%)',     title_font=dict(size=16, family='Arial'),     tickfont=dict(size=16, family='Arial'),     range=[0, 26],     showgrid=True,     gridcolor='rgba(200, 200, 200, 0.3)',     griddash='dot',     row=1, col=2 )  fig.update_yaxes(     title_text='Calculated PCE (%)',     title_font=dict(size=16, family='Arial'),     tickfont=dict(size=16, family='Arial'),     range=[0, 26],     showgrid=True,     gridcolor='rgba(200, 200, 200, 0.3)',     griddash='dot',     row=1, col=2 )  # Add Nature-style panel labels fig.add_annotation(     text='a',     xref='x domain', yref='y domain',     x=-0.15, y=1.05,     xanchor='left', yanchor='bottom',     font=dict(size=18, family='Arial', color='black'),     showarrow=False,     row=1, col=1 ) fig.add_annotation(     text='b',     xref='x2 domain', yref='y2 domain',     x=-0.15, y=1.05,     xanchor='left', yanchor='bottom',     font=dict(size=18, family='Arial', color='black'),     showarrow=False,     row=1, col=2 )  # Update overall layout fig.update_layout(     plot_bgcolor='white',     paper_bgcolor='white',     font=dict(family='Arial', size=12),     legend=dict(         x=0.535,         y=0.98,         xanchor='left',         yanchor='top',         bgcolor='rgba(255, 255, 255, 0.8)',         font=dict(size=16, family='Arial')     ),     width=700,     height=400,     margin=dict(t=60, b=80, l=80, r=80) )  fig.show(renderer=\"notebook\") In\u00a0[\u00a0]: Copied! <pre># Export combined figure to PDF (Nature quality)\nfig.write_image('physics_filter_combined.pdf', scale=1, width=700, height=500)\nprint(\"Figure exported to: physics_filter_combined.pdf\")\n</pre> # Export combined figure to PDF (Nature quality) fig.write_image('physics_filter_combined.pdf', scale=1, width=700, height=500) print(\"Figure exported to: physics_filter_combined.pdf\") <pre>Figure exported to: physics_filter_combined.pdf\n</pre>"},{"location":"notebooks/perla_notebooks/physics_filter.html#implications","title":"Implications\u00b6","text":"<p>This comparison reveals important differences in data quality between the two datasets:</p> <ul> <li><p>Legacy Database: Entries that fail the physics consistency check may originate from errors in the manual data curtion, inconsistencies in the source publications themselves, such as reporting errors, calculation mistakes in the original papers, or unit mismatches. The fraction of failing entries reflects the challenges inherent in literature-reported data, regardless of the curation method.</p> </li> <li><p>PERLA Pipeline: By enforcing physics-based filters during the LLM extraction process, PERLA automatically excludes entries that fail consistency checks. This automated validation approach ensures physically coherent data enters the database, improving overall data reliability while maintaining scalability. This also helps to exclude extracted solar cells with mixed parameters from papers that report multiple solar cells.</p> </li> </ul> <p>The results demonstrate that we can dramatically reduce these inconsistencies by having this check in the extraction pipeline.</p>"},{"location":"notebooks/perla_notebooks/physics_filter.html#methodology","title":"Methodology\u00b6","text":"<p>The physics consistency check validates that the reported power conversion efficiency (PCE) matches the calculated efficiency from measured parameters:</p> <p>$$\\text{PCE} = \\frac{\\text{FF} \\times V_{\\text{OC}} \\times J_{\\text{SC}}}{P_{\\text{in}}}$$</p> <p>Where:</p> <ul> <li>FF = Fill Factor (dimensionless, 0-1)</li> <li>V<sub>OC</sub> = Open Circuit Voltage (V)</li> <li>J<sub>SC</sub> = Short Circuit Current Density (A/m\u00b2, SI units)</li> <li>P<sub>in</sub> = Incident Power Density (W/m\u00b2, SI units; typically 1000 W/m\u00b2 for standard test conditions)</li> </ul> <p>All quantities are in SI units as stored in NOMAD results. The calculation is straightforward: (FF \u00d7 V<sub>OC</sub> \u00d7 J<sub>SC</sub>) / P<sub>in</sub> \u00d7 100 gives PCE in percentage. Units: (V \u00d7 A/m\u00b2) / (W/m\u00b2) \u00d7 100 = (W/m\u00b2) / (W/m\u00b2) \u00d7 100 = %. We use an absolute tolerance of \u00b10.2% to account for rounding and measurement precision. Entries that fail this check may indicate data entry errors, unit mismatches, or measurement inconsistencies.</p>"},{"location":"notebooks/perla_notebooks/physics_filter.html#the-dataset-for-this-analysis","title":"The dataset for this analysis\u00b6","text":"<p>The query to create this parquet files includes onlysolar cells with registered illumination intensity around 1-sun conditions. It excludes entries where the illumination intendity could not be resgistered.</p>"},{"location":"notebooks/perla_notebooks/plotly_theme.html","title":"Plotly theme","text":"In\u00a0[\u00a0]: Copied! <pre>from __future__ import annotations\n</pre> from __future__ import annotations In\u00a0[\u00a0]: Copied! <pre>from collections.abc import Iterable, Sequence\nfrom typing import Optional\n</pre> from collections.abc import Iterable, Sequence from typing import Optional In\u00a0[\u00a0]: Copied! <pre>import plotly.graph_objects as go\nimport plotly.io as pio\n</pre> import plotly.graph_objects as go import plotly.io as pio In\u00a0[\u00a0]: Copied! <pre># ---------- Color Definitions (Single Source of Truth) ----------\n# Default colorway for all plots\nDEFAULT_COLORWAY = (\n    '#1f77b4',\n    '#ff0e5a',\n    '#e9c821',\n    '#86d9ea',\n    '#ff9408',\n    '#ba78d6',\n    '#4cd8a5',\n    '#7f7f7f',\n    '#bcbd22',\n    '#17becf',\n)\n</pre> # ---------- Color Definitions (Single Source of Truth) ---------- # Default colorway for all plots DEFAULT_COLORWAY = (     '#1f77b4',     '#ff0e5a',     '#e9c821',     '#86d9ea',     '#ff9408',     '#ba78d6',     '#4cd8a5',     '#7f7f7f',     '#bcbd22',     '#17becf', ) In\u00a0[\u00a0]: Copied! <pre>def get_model_colors(colorway: Sequence[str] | None = None) -&gt; dict[str, str]:\n    \"\"\"\n    Get model color mapping from colorway.\n\n    This is the single source of truth for model colors.\n    All model colors should be obtained from this function.\n\n    Args:\n        colorway: Optional colorway sequence. If None, uses DEFAULT_COLORWAY.\n\n    Returns:\n        Dictionary mapping model display names to hex colors.\n    \"\"\"\n    if colorway is None:\n        colorway = DEFAULT_COLORWAY\n\n    colorway_list = list(colorway)\n\n    return {\n        'GPT-5 Mini': colorway_list[0],\n        'GPT-5': colorway_list[1],\n        'GPT-4.1': colorway_list[2],\n        'GPT-4o': colorway_list[3],\n        'Claude Sonnet 4': colorway_list[4],\n        'Claude Opus 4': colorway_list[5],\n        'Claude Opus 4.1': colorway_list[6],\n        'Consensus': colorway_list[7],\n    }\n</pre> def get_model_colors(colorway: Sequence[str] | None = None) -&gt; dict[str, str]:     \"\"\"     Get model color mapping from colorway.      This is the single source of truth for model colors.     All model colors should be obtained from this function.      Args:         colorway: Optional colorway sequence. If None, uses DEFAULT_COLORWAY.      Returns:         Dictionary mapping model display names to hex colors.     \"\"\"     if colorway is None:         colorway = DEFAULT_COLORWAY      colorway_list = list(colorway)      return {         'GPT-5 Mini': colorway_list[0],         'GPT-5': colorway_list[1],         'GPT-4.1': colorway_list[2],         'GPT-4o': colorway_list[3],         'Claude Sonnet 4': colorway_list[4],         'Claude Opus 4': colorway_list[5],         'Claude Opus 4.1': colorway_list[6],         'Consensus': colorway_list[7],     } In\u00a0[\u00a0]: Copied! <pre># ---------- Template ----------\ndef register_template(  # noqa: PLR0913\n    *,\n    name: str = 'pepe',\n    font_family: str = 'Arial',\n    font_size: int = 18,\n    gridcolor: str = 'lightgray',\n    plot_bgcolor: str = 'rgba(0,0,0,0)',\n    paper_bgcolor: str = 'rgba(0,0,0,0)',\n    colorway: Sequence[str] | None = None,\n) -&gt; str:\n    \"\"\"\n    Register a Plotly template with Nature-compatible styling.\n\n    Args:\n        name: Template name\n        font_family: Font family for all text\n        font_size: Base font size\n        gridcolor: Grid line color\n        plot_bgcolor: Plot background color\n        paper_bgcolor: Paper/outer background color\n        colorway: Color sequence for traces. If None, uses DEFAULT_COLORWAY.\n\n    Returns:\n        Template name\n    \"\"\"\n    if colorway is None:\n        colorway = DEFAULT_COLORWAY\n    tmpl = go.layout.Template(\n        layout=go.Layout(\n            font=dict(family=font_family, size=font_size),\n            colorway=list(colorway),\n            plot_bgcolor=plot_bgcolor,\n            paper_bgcolor=paper_bgcolor,\n            margin=dict(l=50, r=50, t=60, b=60),\n            # Set width to ~7.2 inches at 100 dpi (Nature two-column width)\n            width=720,\n            legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.6)'),\n            xaxis=dict(\n                showgrid=True,\n                gridcolor=gridcolor,\n                zeroline=False,\n                showline=True,\n                linecolor='black',\n                linewidth=1,\n                mirror=True,\n                ticks='inside',\n                tickcolor='black',\n                automargin=True,\n                title_standoff=10,\n                exponentformat='power',\n            ),\n            yaxis=dict(\n                showgrid=True,\n                gridcolor=gridcolor,\n                zeroline=False,\n                showline=True,\n                linecolor='black',\n                linewidth=1,\n                mirror=True,\n                ticks='inside',\n                tickcolor='black',\n                automargin=True,\n                title_standoff=10,\n                exponentformat='power',\n            ),\n        )\n    )\n    # Default trace tweaks\n    tmpl.data.scatter = [\n        go.Scatter(\n            marker=dict(line=dict(color='black', width=1)),\n            line=dict(width=2),\n        )\n    ]\n    pio.templates[name] = tmpl\n    pio.templates.default = name\n    return name\n</pre> # ---------- Template ---------- def register_template(  # noqa: PLR0913     *,     name: str = 'pepe',     font_family: str = 'Arial',     font_size: int = 18,     gridcolor: str = 'lightgray',     plot_bgcolor: str = 'rgba(0,0,0,0)',     paper_bgcolor: str = 'rgba(0,0,0,0)',     colorway: Sequence[str] | None = None, ) -&gt; str:     \"\"\"     Register a Plotly template with Nature-compatible styling.      Args:         name: Template name         font_family: Font family for all text         font_size: Base font size         gridcolor: Grid line color         plot_bgcolor: Plot background color         paper_bgcolor: Paper/outer background color         colorway: Color sequence for traces. If None, uses DEFAULT_COLORWAY.      Returns:         Template name     \"\"\"     if colorway is None:         colorway = DEFAULT_COLORWAY     tmpl = go.layout.Template(         layout=go.Layout(             font=dict(family=font_family, size=font_size),             colorway=list(colorway),             plot_bgcolor=plot_bgcolor,             paper_bgcolor=paper_bgcolor,             margin=dict(l=50, r=50, t=60, b=60),             # Set width to ~7.2 inches at 100 dpi (Nature two-column width)             width=720,             legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.6)'),             xaxis=dict(                 showgrid=True,                 gridcolor=gridcolor,                 zeroline=False,                 showline=True,                 linecolor='black',                 linewidth=1,                 mirror=True,                 ticks='inside',                 tickcolor='black',                 automargin=True,                 title_standoff=10,                 exponentformat='power',             ),             yaxis=dict(                 showgrid=True,                 gridcolor=gridcolor,                 zeroline=False,                 showline=True,                 linecolor='black',                 linewidth=1,                 mirror=True,                 ticks='inside',                 tickcolor='black',                 automargin=True,                 title_standoff=10,                 exponentformat='power',             ),         )     )     # Default trace tweaks     tmpl.data.scatter = [         go.Scatter(             marker=dict(line=dict(color='black', width=1)),             line=dict(width=2),         )     ]     pio.templates[name] = tmpl     pio.templates.default = name     return name In\u00a0[\u00a0]: Copied! <pre># Export model colors for use in notebooks\nMODEL_COLORS = get_model_colors()\n</pre> # Export model colors for use in notebooks MODEL_COLORS = get_model_colors() In\u00a0[\u00a0]: Copied! <pre>def set_defaults(\n    format: str = 'svg',\n    filename: str = 'plot',\n    scale: int = 1,\n):\n    \"\"\"\n    Set global Plotly defaults for all .show() calls.\n    In particular, makes the modebar download button export SVG.\n    \"\"\"\n    pio.renderers.default = pio.renderers.default  # ensure current renderer stays\n    pio.renderers[pio.renderers.default].config = {\n        'toImageButtonOptions': {\n            'format': format,\n            'filename': filename,\n            'scale': scale,\n        }\n    }\n</pre> def set_defaults(     format: str = 'svg',     filename: str = 'plot',     scale: int = 1, ):     \"\"\"     Set global Plotly defaults for all .show() calls.     In particular, makes the modebar download button export SVG.     \"\"\"     pio.renderers.default = pio.renderers.default  # ensure current renderer stays     pio.renderers[pio.renderers.default].config = {         'toImageButtonOptions': {             'format': format,             'filename': filename,             'scale': scale,         }     }"},{"location":"notebooks/perla_notebooks/query-perovskite-database.html","title":"Query the Perovskite Database","text":"Query the Perovskite Database in NOMAD <p>     This notebook retrieves data from the Perovskite database in NOMAD and stores it as a parquet file. \u26a0\ufe0f Note: This query may take up to 1 hour to complete due to API rate limits. </p> In\u00a0[\u00a0]: Copied! <pre>from time import time\n\nimport nest_asyncio\nfrom nomad.client.archive import ArchiveQuery\n\nnest_asyncio.apply()\n\n\n# Define which sections to retrieve from the archive\nrequired = {\n    'results': '*',\n    'data': '*',\n}\n\n# Build the query to find perovskite solar cells measured under approximately one-sun conditions\nquery = ArchiveQuery(\n    query={\n        'and': [\n            {\n                'results.properties.optoelectronic.solar_cell.illumination_intensity': {\n                    'gte': 600.0,  # constrain to around one-sun illumination conditions\n                    'lte': 1200.0,\n                }\n            },\n            {\n                'section_defs.definition_qualified_name:all': [\n                    'perovskite_solar_cell_database.schema.PerovskiteSolarCell'\n                ]\n            },\n        ]\n    },\n    required=required,\n    page_size=50000,\n    results_max=60000,\n)\n</pre> from time import time  import nest_asyncio from nomad.client.archive import ArchiveQuery  nest_asyncio.apply()   # Define which sections to retrieve from the archive required = {     'results': '*',     'data': '*', }  # Build the query to find perovskite solar cells measured under approximately one-sun conditions query = ArchiveQuery(     query={         'and': [             {                 'results.properties.optoelectronic.solar_cell.illumination_intensity': {                     'gte': 600.0,  # constrain to around one-sun illumination conditions                     'lte': 1200.0,                 }             },             {                 'section_defs.definition_qualified_name:all': [                     'perovskite_solar_cell_database.schema.PerovskiteSolarCell'                 ]             },         ]     },     required=required,     page_size=50000,     results_max=60000, ) In\u00a0[\u00a0]: Copied! <pre>number_of_entries = (\n    await query.async_fetch()\n)  # indicative number n applies: async_fetch(n)\n</pre> number_of_entries = (     await query.async_fetch() )  # indicative number n applies: async_fetch(n) In\u00a0[\u00a0]: Copied! <pre>start_time = time()\nresults = await query.async_download(\n    number_of_entries\n)  # indicative number n applies: async_download(n)\nend_time = time()\n\nprint(f'Downloaded in {end_time - start_time} seconds.')\n</pre> start_time = time() results = await query.async_download(     number_of_entries )  # indicative number n applies: async_download(n) end_time = time()  print(f'Downloaded in {end_time - start_time} seconds.') In\u00a0[\u00a0]: Copied! <pre>query._entries_dict.append(results)  # this is only needed in async mode.\ndf = query.entries_to_dataframe()\ndf\n</pre> query._entries_dict.append(results)  # this is only needed in async mode. df = query.entries_to_dataframe() df In\u00a0[\u00a0]: Copied! <pre>df.to_parquet('perovskite_solar_cell_database.parquet', index=False)\n</pre> df.to_parquet('perovskite_solar_cell_database.parquet', index=False)"},{"location":"notebooks/perla_notebooks/query-perovskite-database.html#initialize-the-nomad-query","title":"Initialize the NOMAD query\u00b6","text":"<p>Import the async helpers, apply <code>nest_asyncio</code> so the event loop can run inside Jupyter, define the archive sections to retrieve, and build a filter that isolates perovskite solar cell entries measured near one-sun illumination.</p>"},{"location":"notebooks/perla_notebooks/query-perovskite-database.html#estimate-matching-entries","title":"Estimate matching entries\u00b6","text":"<p>Trigger <code>async_fetch</code> to learn how many archive entries satisfy the filter. This count guides the download size used in the following step.</p>"},{"location":"notebooks/perla_notebooks/query-perovskite-database.html#download-the-dataset","title":"Download the dataset\u00b6","text":"<p>Download the full set of matching archives asynchronously and record how long the transfer takes for future reference.</p>"},{"location":"notebooks/perla_notebooks/query-perovskite-database.html#convert-results-to-a-dataframe","title":"Convert results to a dataframe\u00b6","text":"<p>Append the downloaded batch to the query object's internal cache and materialize a pandas dataframe for inspection inside the notebook.</p>"},{"location":"notebooks/perla_notebooks/query-perovskite-database.html#persist-a-local-copy","title":"Persist a local copy\u00b6","text":"<p>Store the dataframe as <code>perovskite_solar_cell_database.parquet</code> so the curated dataset can be reused outside the NOMAD client session.</p>"},{"location":"reference/composition_and_ion_schema.html","title":"Composition and ion schema","text":""},{"location":"reference/composition_and_ion_schema.html#perovskitechemicalsection","title":"PerovskiteChemicalSection","text":"<p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type common_name <code>str</code> The common trade name molecular_formula <code>str</code> The molecular formula smiles <code>str</code> The canonical SMILE string iupac_name <code>str</code> The standard IUPAC name cas_number <code>str</code> The CAS number if available"},{"location":"reference/composition_and_ion_schema.html#perovskiteionsection","title":"PerovskiteIonSection","text":"<p>inherits from: <code>PerovskiteChemicalSection</code></p> <p>properties:</p> name type abbreviation <code>str</code> The standard abbreviation of the ion. If the abbreviation is in the archive, additional data is complemented automatically source_compound_molecular_formula <code>str</code> The molecular formula of the source compound source_compound_smiles <code>str</code> The canonical SMILE string of the source compound source_compound_iupac_name <code>str</code> The standard IUPAC name of the source compound source_compound_cas_number <code>str</code> The CAS number if available of the source compound"},{"location":"reference/composition_and_ion_schema.html#perovskiteion","title":"PerovskiteIon","text":"<p>description: Abstract class for describing a general perovskite ion.</p> <p>inherits from: <code>nomad.datamodel.metainfo.basesections.v1.PureSubstance</code>, <code>PerovskiteIonSection</code></p> <p>properties:</p> name type abbreviation <code>str</code> The standard abbreviation of the ion. If the abbreviation is in the archive, additional data is complemented automatically pure_substance <code>nomad.datamodel.metainfo.basesections.v1.PubChemPureSubstanceSection</code> Section with properties describing the substance.sub-section source_compound <code>nomad.datamodel.metainfo.basesections.v1.PubChemPureSubstanceSection</code> Section with properties describing the substance.sub-section <p>normalization: </p> <p>The normalizer for the <code>PerovskiteIon</code> class.</p> <p>Args:     archive (EntryArchive): The archive containing the section that is being     normalized.     logger (BoundLogger): A structlog logger.</p>"},{"location":"reference/composition_and_ion_schema.html#perovskiteaion","title":"PerovskiteAIon","text":"<p>inherits from: <code>PerovskiteIon</code>, <code>nomad.datamodel.data.EntryData</code></p> <p>normalization: </p> <p>The normalizer for the <code>PerovskiteIon</code> class.</p> <p>Args:     archive (EntryArchive): The archive containing the section that is being     normalized.     logger (BoundLogger): A structlog logger.</p>"},{"location":"reference/composition_and_ion_schema.html#perovskitebion","title":"PerovskiteBIon","text":"<p>inherits from: <code>PerovskiteIon</code>, <code>nomad.datamodel.data.EntryData</code></p> <p>normalization: </p> <p>The normalizer for the <code>PerovskiteIon</code> class.</p> <p>Args:     archive (EntryArchive): The archive containing the section that is being     normalized.     logger (BoundLogger): A structlog logger.</p>"},{"location":"reference/composition_and_ion_schema.html#perovskitexion","title":"PerovskiteXIon","text":"<p>inherits from: <code>PerovskiteIon</code>, <code>nomad.datamodel.data.EntryData</code></p> <p>normalization: </p> <p>The normalizer for the <code>PerovskiteIon</code> class.</p> <p>Args:     archive (EntryArchive): The archive containing the section that is being     normalized.     logger (BoundLogger): A structlog logger.</p>"},{"location":"reference/composition_and_ion_schema.html#perovskiteioncomponent","title":"PerovskiteIonComponent","text":"<p>inherits from: <code>nomad.datamodel.metainfo.basesections.v1.SystemComponent</code>, <code>PerovskiteIonSection</code></p> <p>properties:</p> name type coefficient <code>str</code> The stoichiometric coefficient system <code>PerovskiteIon</code> A reference to the component system. <p>normalization: </p> <p>The normalizer for the <code>IonComponent</code> class.</p> <p>Args:     archive (EntryArchive): The archive containing the section that is being     normalized.     logger (BoundLogger): A structlog logger.</p>"},{"location":"reference/composition_and_ion_schema.html#perovskiteaioncomponent","title":"PerovskiteAIonComponent","text":"<p>inherits from: <code>PerovskiteIonComponent</code></p> <p>properties:</p> name type system <code>PerovskiteAIon</code> A reference to the component system. <p>normalization: </p> <p>The normalizer for the <code>IonComponent</code> class.</p> <p>Args:     archive (EntryArchive): The archive containing the section that is being     normalized.     logger (BoundLogger): A structlog logger.</p>"},{"location":"reference/composition_and_ion_schema.html#perovskitebioncomponent","title":"PerovskiteBIonComponent","text":"<p>inherits from: <code>PerovskiteIonComponent</code></p> <p>properties:</p> name type system <code>PerovskiteBIon</code> A reference to the component system. <p>normalization: </p> <p>The normalizer for the <code>IonComponent</code> class.</p> <p>Args:     archive (EntryArchive): The archive containing the section that is being     normalized.     logger (BoundLogger): A structlog logger.</p>"},{"location":"reference/composition_and_ion_schema.html#perovskitexioncomponent","title":"PerovskiteXIonComponent","text":"<p>inherits from: <code>PerovskiteIonComponent</code></p> <p>properties:</p> name type system <code>PerovskiteXIon</code> A reference to the component system. <p>normalization: </p> <p>The normalizer for the <code>IonComponent</code> class.</p> <p>Args:     archive (EntryArchive): The archive containing the section that is being     normalized.     logger (BoundLogger): A structlog logger.</p>"},{"location":"reference/composition_and_ion_schema.html#impurity","title":"Impurity","text":"<p>inherits from: <code>nomad.datamodel.metainfo.basesections.v1.PureSubstanceComponent</code>, <code>PerovskiteChemicalSection</code></p> <p>properties:</p> name type abbreviation <code>str</code> The abbreviation used for the additive or impurity. concentration <code>float</code> The concentration of the additive or impurity.unit=<code>1 / centimeter ** 3</code> pure_substance <code>nomad.datamodel.metainfo.basesections.v1.PubChemPureSubstanceSection</code> Section describing the pure substance that is the component.sub-section <p>normalization: </p> <p>The normalizer for the <code>Impurity</code> class.</p> <p>Args:     archive (EntryArchive): The archive containing the section that is being     normalized.     logger (BoundLogger): A structlog logger.</p>"},{"location":"reference/composition_and_ion_schema.html#perovskitecompositionsection","title":"PerovskiteCompositionSection","text":"<p>inherits from: <code>nomad.datamodel.data.ArchiveSection</code></p> <p>properties:</p> name type short_form <code>str</code> long_form <code>str</code> formula <code>str</code> composition_estimate <code>['Estimated from XRD data', 'Estimated from precursor solutions', 'Estimated from spectroscopic data', 'Hypothetical compound', 'Literature value', 'Other', 'Theoretical simulation']</code> sample_type <code>['Amorphous', 'Colloidal solution', 'Nano rods', 'Other', 'Polycrystalline film', 'Quantum dots', 'Single crystal']</code> dimensionality <code>['0D', '1D', '2D', '2D/3D', '3D', 'Other']</code> The dimensionality of the perovskite, i.e. 3D, 2D, 1D (nanorods), quantum dots (0D), etc. band_gap <code>float</code> Band gap of photoabsorber in eV.unit=<code>electron_volt</code> ions_a_site <code>PerovskiteAIonComponent</code> sub-section, repeats ions_b_site <code>PerovskiteBIonComponent</code> sub-section, repeats ions_x_site <code>PerovskiteXIonComponent</code> sub-section, repeats impurities <code>Impurity</code> sub-section, repeats additives <code>Impurity</code> sub-section, repeats <p>normalization: </p> <p>The normalizer for the <code>PerovskiteCompositionSection</code> class.</p> <p>Args:     archive (EntryArchive): The archive containing the section that is being     normalized.     logger (BoundLogger): A structlog logger.</p>"},{"location":"reference/composition_and_ion_schema.html#perovskitecomposition","title":"PerovskiteComposition","text":"<p>description: Schema for describing a perovskite composition.</p> <p>inherits from: <code>PerovskiteCompositionSection</code>, <code>nomad.datamodel.metainfo.basesections.v1.CompositeSystem</code>, <code>nomad.datamodel.data.EntryData</code></p> <p>normalization: </p> <p>The normalizer for the <code>PerovskiteComposition</code> class.</p> <p>Args:     archive (EntryArchive): The archive containing the section that is being     normalized.     logger (BoundLogger): A structlog logger.</p>"},{"location":"reference/solar_cell_schema.html","title":"Solar cells schema","text":""},{"location":"reference/solar_cell_schema.html#perovskitesolarcell","title":"PerovskiteSolarCell","text":"<p>description: This schema is adapted to map the data in the Perovskite Solar Cell Database Project. The descriptions in the quantities represent the instructions given to the user who manually curated the data.</p> <p>inherits from: <code>nomad.datamodel.data.EntryData</code>, <code>nomad.datamodel.metainfo.plot.PlotSection</code></p> <p>properties:</p> name type ref <code>perovskite_solar_cell_database.schema_sections.ref.Ref</code> sub-section cell <code>perovskite_solar_cell_database.schema_sections.cell.Cell</code> sub-section module <code>perovskite_solar_cell_database.schema_sections.module.Module</code> sub-section substrate <code>perovskite_solar_cell_database.schema_sections.substrate.Substrate</code> sub-section etl <code>perovskite_solar_cell_database.schema_sections.etl.ETL</code> sub-section perovskite <code>perovskite_solar_cell_database.schema_sections.perovskite.Perovskite</code> sub-section perovskite_deposition <code>perovskite_solar_cell_database.schema_sections.perovskite_deposition.PerovskiteDeposition</code> sub-section htl <code>perovskite_solar_cell_database.schema_sections.htl.HTL</code> sub-section backcontact <code>perovskite_solar_cell_database.schema_sections.backcontact.Backcontact</code> sub-section add <code>perovskite_solar_cell_database.schema_sections.add.Add</code> sub-section encapsulation <code>perovskite_solar_cell_database.schema_sections.encapsulation.Encapsulation</code> sub-section jv <code>perovskite_solar_cell_database.schema_sections.jv.JV</code> sub-section stabilised <code>perovskite_solar_cell_database.schema_sections.stabilised.Stabilised</code> sub-section eqe <code>perovskite_solar_cell_database.schema_sections.eqe.EQE</code> sub-section stability <code>perovskite_solar_cell_database.schema_sections.stability.Stability</code> sub-section outdoor <code>perovskite_solar_cell_database.schema_sections.outdoor.Outdoor</code> sub-section <p>normalization without further documentation</p>"},{"location":"reference/tandem_schema.html","title":"Tandem schema","text":""},{"location":"reference/tandem_schema.html#macro-rendering-error","title":"Macro Rendering Error","text":"<p>File: <code>reference/tandem_schema.md</code></p> <p>ModuleNotFoundError: No module named 'perovskite_solar_cell_database.schema_packages.tandem.tandem'</p> <pre><code>Traceback (most recent call last):\n  File \"/home/runner/work/nomad-perovskite-solar-cells-database/nomad-perovskite-solar-cells-database/.venv/lib/python3.12/site-packages/mkdocs_macros/plugin.py\", line 703, in render\n    return md_template.render(**page_variables)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/runner/work/nomad-perovskite-solar-cells-database/nomad-perovskite-solar-cells-database/.venv/lib/python3.12/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/home/runner/work/nomad-perovskite-solar-cells-database/nomad-perovskite-solar-cells-database/.venv/lib/python3.12/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 3, in top-level template code\n  File \"/home/runner/work/nomad-perovskite-solar-cells-database/nomad-perovskite-solar-cells-database/.venv/lib/python3.12/site-packages/nomad_docs/__init__.py\", line 393, in metainfo_package\n    module = importlib.import_module(path)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1387, in _gcd_import\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1360, in _find_and_load\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1324, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'perovskite_solar_cell_database.schema_packages.tandem.tandem'\n</code></pre>"},{"location":"tutorial/index.html","title":"Tutorial","text":""},{"location":"tutorial/index.html#tutorial","title":"Tutorial","text":"<p>Welcome to the Perovskite Solar Cell Database tutorial section! This comprehensive guide will help you learn how to work with perovskite solar cell data and compositions in NOMAD.</p>"},{"location":"tutorial/index.html#getting-started","title":"Getting Started","text":"<p>If you're new to the perovskite solar cell database, we recommend starting with:</p> <ol> <li>Sharing a Perovskite Composition - Learn the basics of working with perovskite compositions in NOMAD</li> </ol>"},{"location":"tutorial/index.html#interactive-notebooks","title":"Interactive Notebooks","text":"<p>Our collection of Jupyter notebooks provides hands-on examples demonstrating various aspects of querying, analyzing, and working with perovskite solar cell data.</p>"},{"location":"tutorial/index.html#basic-queries","title":"Basic Queries","text":"<p>Start with these notebooks to learn how to retrieve data from the NOMAD databases:</p> <ul> <li>Query the Perovskite Database - Retrieve perovskite solar cell data from NOMAD</li> <li>Query the Ion Database - Access the perovskite ion database</li> <li>Query Perovskite Compositions - Search for specific perovskite compositions</li> </ul>"},{"location":"tutorial/index.html#structure-building","title":"Structure Building","text":"<p>Learn how to construct perovskite structures:</p> <ul> <li>Build Perovskite Structures from Ions - Build crystal structures from ionic components</li> </ul>"},{"location":"tutorial/index.html#data-analysis","title":"Data Analysis","text":"<p>Explore how the field has evolved over time:</p> <ul> <li>Architecture Evolution Analysis - Analyze device architecture trends</li> <li>Bandgap Evolution Analysis - Study bandgap distribution changes</li> <li>Performance Evolution Analysis - Track device performance improvements</li> <li>Diversity Evolution Analysis - Examine material composition diversity</li> </ul>"},{"location":"tutorial/index.html#specialized-tools","title":"Specialized Tools","text":"<p>Discover advanced analysis tools:</p> <ul> <li>PerovScribe Analysis - Use PerovScribe for automated data extraction</li> <li>Perovskite Paperbot Plotting - Visualize data from automated literature mining</li> </ul>"},{"location":"tutorial/index.html#machine-learning","title":"Machine Learning","text":"<p>Apply machine learning techniques to perovskite data:</p> <ul> <li>CrabNet Bandgap Prediction - Predict bandgaps using CrabNet</li> <li>ML Distribution Shift Case Study - Understand distribution shift in ML models</li> </ul>"},{"location":"tutorial/index.html#next-steps","title":"Next Steps","text":"<p>After completing the tutorials, explore the How-to guides for practical instructions on specific tasks, or dive into the Reference documentation for detailed schema information.</p>"},{"location":"tutorial/sharing_a_perovskite_composition.html","title":"Sharing a Perovskite Composition","text":""},{"location":"tutorial/sharing_a_perovskite_composition.html#sharing-a-perovskite-composition-in-nomad","title":"Sharing a perovskite composition in NOMAD","text":"<p>Attention</p> <p>This tutorial is work in progress. For now, please see the how-to guides and interactive notebooks.</p>"},{"location":"tutorial/sharing_a_perovskite_composition.html#how-to-guides","title":"How-to Guides","text":"<p>Learn the basics through our step-by-step guides:</p> <ul> <li>How to add a new ion</li> <li>How to create a perovskite composition</li> </ul>"},{"location":"tutorial/sharing_a_perovskite_composition.html#interactive-notebooks","title":"Interactive Notebooks","text":"<p>Explore practical examples with our collection of Jupyter notebooks:</p>"},{"location":"tutorial/sharing_a_perovskite_composition.html#working-with-ions-and-compositions","title":"Working with Ions and Compositions","text":"<ul> <li>Query the Ion Database - Learn how to search for and retrieve ion data</li> <li>Query Perovskite Compositions - Find specific perovskite compositions in NOMAD</li> <li>Build Perovskite Structures from Ions - Construct crystal structures using pyrovskite</li> </ul>"},{"location":"tutorial/sharing_a_perovskite_composition.html#querying-solar-cell-data","title":"Querying Solar Cell Data","text":"<ul> <li>Query the Perovskite Database - Retrieve complete solar cell device data</li> </ul>"},{"location":"tutorial/sharing_a_perovskite_composition.html#all-tutorials","title":"All Tutorials","text":"<p>For a complete overview of all available notebooks and tutorials, visit the Tutorial section.</p>"}]}